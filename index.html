<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="img/xmu.ico" />
<link rel="bookmark" href="img/xmu.ico" type="image/x-icon"ã€€/>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<title>Ziyang Chen (é™ˆå­æ‰¬)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=t64KgqAAAAAJ&hl=zh-CN">Google Scholar</a></div>
<div class="menu-item"><a href="https://www.researchgate.net/profile/Ziyang-Chen-25">ResearchGate</a></div>
<div class="menu-item"><a href="https://github.com/ZYangChen">GitHub</a></div>
<div class="menu-item"><a href="https://orcid.org/0000-0002-9361-0240">ORCiD</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Chen, Ziyang (é™ˆå­æ‰¬)</h1>
</div>
<table class="imgtable"><tr><td>
<!--<a href="https://xiuzezhou.github.io/">-->
<img src="photos/me.jpg" alt="alt text" width="160px" height="160px" />
<!--</a>-->
&nbsp;</td>
<td align="left"><p><b>Ph.D. Student, </b> 
Computer Science and Technology, <br />
Media Analytics & Computing (MAC) Laboratory, <b>Xiamen University</b> <br />
E-mail: <a href="mailto:ziyangchen2000@gmail.com">ziyangchen2000@gmail.com</a></p>
</td></tr></table>
<h2>Short Biography</h2>
<p>
Ziyang Chen is currently pursuing the Ph.D. degree with the Media Analytics & Computing (MAC) Laboratory, Xiamen University. 
He is under the supervision of Prof. <a href="https://mac.xmu.edu.cn/ljcao/">Liujuan Cao (æ›¹åˆ˜å¨Ÿ)</a>. 
He is also co-advised by Prof. <a href="https://chengxuan90.github.io">Xuan Cheng (ç¨‹è½©)</a>. 
He received the M.S. degree from Guizhou University under the guidance of Prof. <a href="http://cs.gzu.edu.cn/2021/1210/c17588a163831/page.htm">Yongjun Zhang (å¼ æ°¸å†›)</a>, 
and the B.S. degree from Huaqiao University mentored by Prof. <a href="https://www.scholat.com/hlin">Hailin Li (ææµ·æ—)</a>.
His current research interest lies in <strong>Stereo Vision</strong>. 
</p>

<h2>News</h2>
<details><summary>Click to expand</summary>
<div style="height: 200px; overflow: auto;">                       
<ul>
    <!--âœŒMaster
    <li><i>Oct. 1, 2024:</i> I am awarded <font color="red">National Scholarship</font>.</li>-->
    <li><i>Jun. 7, 2024:</i> <font color="red">MoCha-V2</font> ranks <strong>1st</strong> on <a href="img/benchmark/bad1-all.html">Middlebury benchmark</a>!ğŸš€ğŸš€ğŸš€ I am the designer of this algorithm. 
        <details id="closeDetails"><summary>Click to expand the screenshots of stereo matching leaderboards</summary>
            <ul>
                <li>ğŸš©Middlebury leaderboard (Jun. 7, 2024) ğŸ‘‡<br>
                <img style="width:80%;" src="img/benchmark/middlebury.png"></li>
                <li>ğŸš©<a href="img/benchmark/ref/eth3d-benchmark/Low-res two-view results - ETH3D.html">ETH3D leaderboard</a> (Jun. 27, 2024) ğŸ‘‡<br>
                <img style="width:80%;" src="img/benchmark/ref/eth3d-benchmark/eth3d.png"></li>
            </ul>
        </details>  
    </li>
    <li><i>Feb. 27, 2024:</i> <font color="red">MoCha-Stereo (æŠ¹èŒ¶ç®—æ³•)</font> is accepted by <a href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers">CVPR2024</a>!!! [<a href="http://gs.gzu.edu.cn/2024/0315/c13269a213613/page.htm">Report</a>]ğŸ”¥ğŸ”¥ğŸ”¥ 
        <!--This is the <strong>first time</strong> that Guizhou University publishes its work in CVPR as the <strong>only corresponding institution</strong>. -->
        It once held <strong>1st</strong> on <a href="img/benchmark/ref/KITTI2015_11_18.html">
        KITTI 2015
        </a> and 
        <a href="img/benchmark/ref/KITTI2012Reflective.html">
        KITTI 2012 Reflective
        </a> benchmarks for 245 days. ğŸš€ğŸš€ğŸš€ I am the designer of this algorithm. 
        
        <details id="closeDetails"><summary>Click to expand the screenshots of these leaderboards (Nov. 3, 2023)</summary>
            <ul>
                <li>ğŸš©KITTI 2015 leaderboard ğŸ‘‡<br>
                <img style="width:80%;" src="img/benchmark/KITTI_20151.png"></li>
                <li>ğŸš©KITTI 2012 Reflective leaderboard ğŸ‘‡<br>
                <img style="width:80%;" src="img/benchmark/KITTI_20121.png"></li>
            </ul>
        </details>      

    </li>

    <!--<li><i>Feb. 1, 2024:</i> <font color="red">HART (èµ¤é¹¿ç®—æ³•)</font> proposed by us ranks <strong>1st</strong> on 
        <a href="img/benchmark/ref/HART/KITTI2012.html">KITTI 2012 Reflective</a>
        benchmark! ğŸš€ğŸš€ğŸš€ I am the designer of this algorithm. 
    </li>-->
    
</ul>
</div>
</details>
<h2>Selected Publications</h2>

<div class="paper-box"><div class="paper-box-image"><div><img src="img/MoChaV2.jpg" alt="sym" width="100%"></div></div>
<div class="paper-box-text">
    <p><strong>Motif Channel Opened in a White-Box: Stereo Matching via Motif Correlation Graph</strong></p>
<p>
<strong>Ziyang Chen</strong>,
<a href="http://cs.gzu.edu.cn/2021/1210/c17588a163831/page.htm">Yongjun Zhang<sup>âœ‰</sup></a>,
<a href="https://www.gzcc.edu.cn/jsjyxxgcxy/contents/3205/3569.html">Wenting Li</a>,
<a href="https://teacher.nwpu.edu.cn/wangbingshu.html">Bingshu Wang</a>,
Yong Zhao,
<a href="http://dsail.vip/PersonInCharge.html">C. L. Philip Chen</a></p>
<p>
<a href="./project_page/mocha_project/index.html" class="paper-btn proj-btn">
    <span class="btn-text">Project Page</span>
</a>
<a href="https://arxiv.org/abs/2411.12426" class="paper-btn proj-btn">
    <span class="btn-text">arXiv</span>
</a>
<a href="https://github.com/ZYangChen/MoCha-Stereo" class="gh-btn">
    <span class="gh-text">Code</span>
    <i class="fab fa-fw fa-github"></i>
    <span class="gh-star-count" id="MoCha-2-star-count-idarb">--</span>
</a>
<a href="https://kns.cnki.net/kcms2/article/abstract?v=Ap1IM-J3Ck22GOVY6AizuOIHQLbWLW89aC5u-Iks_8go9KZy8VlRXIG5uUICz761CostupCWytumH6TBYxOBceGMCgS24W6TyobawnuvTaKPAm5U9kup6eJDpCsgfM_LrOpAmd8hkzVvZ2bUTjQ39vwwfrDP2ILNEVheZchKRdG6yiGFJEsCCJ11xVOjXYlntVQeDDoTpsc&uniplatform=NZKPT&captchaId=57330074-4582-43d0-989b-e55f07f5afa8" class="paper-btn proj-btn">
    <span class="btn-text">ä¸­æ–‡ç‰ˆ</span>
</a>
</p>
</div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><img src="img/FSA.png" alt="sym" width="100%"></div></div>
<div class="paper-box-text">
    <p><strong>Leveraging negative correlation for full-range self-attention in vision transformers</strong></p>
<p>
<a href="https://scholar.google.com/citations?user=CsVTBJoAAAAJ&hl=en">Wei Long<sup>â€ </sup></a>,
<strong>Ziyang Chen</strong><sup>â€ </sup>,
<a href="https://www.gzcc.edu.cn/jsjyxxgcxy/contents/3205/3569.html">Wenting Li</a>,
<a href="http://cs.gzu.edu.cn/2021/1210/c17588a163831/page.htm">Yongjun Zhang<sup>âœ‰</sup></a>,
<a href="https://orcid.org/0009-0002-4212-5023">He Yao</a>,
Jiaxin Peng,
Zhongwei Cui
</p>
<p class="venue">
ğŸ“˜ Pattern Recognition<br>
  ğŸ’¡JCR Q1 ğŸ’¡SCI Q1 <font color="red">TOP</font> ğŸ’¡CCF-B ğŸ’¡CAAI-A ğŸ’¡IF: 8.0</a>
</p>
<p>
<a href="https://www.sciencedirect.com/science/article/pii/S003132032500559X" class="paper-btn proj-btn">
    <span class="btn-text">Paper</span>
</a>
<a href="https://github.com/gitlonglong/Full-Range-Self-Attention" class="gh-btn">
    <span class="gh-text">Code</span>
    <i class="fab fa-fw fa-github"></i>
    <span class="gh-star-count" id="FSA-star-count-idarb">--</span>
</a>
</p>
</div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">CVPR 2024</div><img src="img/MoCha.gif" alt="sym" width="100%"></div></div>
<div class="paper-box-text">
<p><strong>MoCha-Stereo: Motif Channel Attention Network for Stereo Matching</strong></p>
<p><strong>Ziyang Chen</strong><sup>â€ </sup>,
<a href="https://scholar.google.com/citations?user=CsVTBJoAAAAJ&hl=en">Wei Long<sup>â€ </sup></a>,
<a href="https://orcid.org/0009-0002-4212-5023">He Yao<sup>â€ </sup></a>,
<a href="http://cs.gzu.edu.cn/2021/1210/c17588a163831/page.htm">Yongjun Zhang<sup>âœ‰</sup></a>,
<a href="https://teacher.nwpu.edu.cn/wangbingshu.html">Bingshu Wang</a>,
<a href="http://cs.gzu.edu.cn/2021/1210/c17588a163794/page.htm">Yongbin Qin</a>,
<a href="https://faculty.csu.edu.cn/jiawu/zh_CN/index.htm">Jia Wu</a>
</p>
<p class="venue">
ğŸ“•<a href="https://cvpr.thecvf.com/Conferences/2024">IEEE Conference on Computer Vision and Pattern Recognition (<font color="red">CVPR</font>) 2024</a><br>
  ğŸ’¡CCF-A ğŸ’¡CAAI-A ğŸ’¡H5-index: 440 (<font color="red">1st</font> in CS, <font color="red">1st</font> in CV)
</p>
<p>
<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_MoCha-Stereo_Motif_Channel_Attention_Network_for_Stereo_Matching_CVPR_2024_paper.html" class="paper-btn proj-btn">
    <span class="btn-text">Project Page</span>
</a>
<a href="https://arxiv.org/pdf/2404.06842" class="paper-btn proj-btn">
    <span class="btn-text">Paper</span>
</a>
<a href="https://github.com/ZYangChen/MoCha-Stereo" class="gh-btn">
    <span class="gh-text">Code</span>
    <i class="fab fa-fw fa-github"></i>
    <span class="gh-star-count" id="MoCha-star-count-idarb">--</span>
</a>
</p>

</div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><img src="img/dc_sat.png" alt="sym" width="100%"></div></div>
<div class="paper-box-text">
    <p><strong>Surface Depth Estimation from Multi-view Stereo Satellite Images with Distribution Contrast Network</strong></p>
<p>
<strong>Ziyang Chen</strong>,
<a href="https://www.gzcc.edu.cn/jsjyxxgcxy/contents/3205/3569.html">Wenting Li</a>, Zhongwei Cui, 
<a href="http://cs.gzu.edu.cn/2021/1210/c17588a163831/page.htm">Yongjun Zhang<sup>âœ‰</sup></a>
</p>
<p class="venue">
ğŸ“˜ IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing<br>
  ğŸ’¡JCR Q1 ğŸ’¡SCI Q2 <font color="red">TOP</font> ğŸ’¡CAAI-B ğŸ’¡IF: 4.7
</p>
<p>
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10689488" class="paper-btn proj-btn">
    <span class="btn-text">Paper</span>
</a>
<a href="https://github.com/ZYangChen/DC-SatMVS" class="gh-btn">
    <span class="gh-text">Code</span>
    <i class="fab fa-fw fa-github"></i>
    <span class="gh-star-count" id="DC-star-count-idarb">--</span>
</a>
</p>
</div>
</div>

<p><b>Note</b>: âœ‰ indicates the corresponding author, â€  indicates co-first author. ğŸ“•: Conference paper, ğŸ“˜: Journal paper.</p>
<p><a href="https://scholar.google.com/citations?user=t64KgqAAAAAJ&hl=zh-CN">Full list of publications in Google Scholar</a>.</p>

<h2>Academic Service</h2>
<p><b>Member</b></p>
<ul>
<li><p>IEEE Student Member</p>
</li>  
<li><p>CCF Student Member</p>
</li>
</ul>

<p><b>Conference Reviewer</b></p>
<ul>
<li><p>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2026</p>
</li>  
<li><p>Neural Information Processing Systems Conference (NeurIPS) 2025</p>
</li>
</ul>

<p><b>Journal Reviewer</b></p>
<ul>
<li>IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI)</li>
<li>IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT)</li>       
<li>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (IEEE JSTARS)</li>
<li>Pattern Recognition (PR)</li>
<li>Knowledge-Based Systems (KBS)</li>
<li>Neurocomputing</li>
<li>Information Sciences</li>
<li>Photogrammetric Engineering & Remote Sensing</li>
<li>International Journal of Machine Learning and Cybernetics</li>
<li>Signal, Image and Video Processing</li>
<li>Electronics Letters</li>
</ul>   

<h2>Industry Experience</h2>
<div class="intern-grid">
<div class="intern-item">
    <div class="intern-logo"><img src="img/starnet.jpeg" alt="StarNet"></div>
    <div class="intern-info">
        <p><strong>Fujian Star-Net Communication Co.,Ltd.</strong> (æ˜Ÿç½‘é”æ·)</p>
        <!--<p class="intern-date">Mar. 2024 - Jun. 2024</p>-->
        <p>AI Algorithm Engineer (Intern)</p>
        <!--<p>Mentor: .</p>-->
    </div>
</div>

<div class="intern-item">
    <div class="intern-logo"><img src="img/zettlab.jpg" alt="Zettlab"></div>
    <div class="intern-info">
        <p><strong>Zettlab AI</strong> (å¾äº‘åˆ›æ–°)</p>
        <!--<p class="intern-date">Dec. 2023 - Mar. 2024</p>-->
        <p>AI Algorithm Engineer (Intern)</p>
        <!--<p>Mentor: .</p>-->
    </div>
</div>

</div>

<script> 
async function fetchStars(repo, elementId) { 
     try { 
         const response = await fetch(`https://api.github.com/repos/${repo}`); 
         const data = await response.json(); 
         if (data.stargazers_count !== undefined) { 
             document.getElementById(elementId).innerText = data.stargazers_count; 
         } 
     } catch (error) { 
         console.error("Failed to fetch stars", error); 
     } 
 } 
 
 // é¡µé¢åŠ è½½åæ‰§è¡Œï¼šå¡«å…¥ ä»“åº“è·¯å¾„ å’Œ å¯¹åº”çš„ HTML ID 
 fetchStars('ZYangChen/MoCha-Stereo', 'MoCha-2-star-count-idarb'); 
 fetchStars('ZYangChen/MoCha-Stereo', 'MoCha-star-count-idarb'); 
 fetchStars('gitlonglong/Full-Range-Self-Attention', 'FSA-star-count-idarb');
 fetchStars('ZYangChen/DC-SatMVS', 'DC-star-count-idarb');
</script>

</body>
</html>


