<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Project Page of MoCha-Stereo</title>
  <link rel="icon" type="image/x-icon" href="static/images/gzu.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Motif Channel Opened in a White-Box: Stereo Matching via Motif Correlation Graph</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zyangchen.github.io/" target="_blank">Ziyang Chen</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="http://cs.gzu.edu.cn/2021/1210/c17588a163831/page.htm" target="_blank">Yongjun Zhang</a><sup>1*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=a3nIKnsAAAAJ&hl=en" target="_blank">Wenting Li</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://teacher.nwpu.edu.cn/wangbingshu.html" target="_blank">Bingshu Wang</a><sup>3</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://www.ece.pku.edu.cn/info/1045/2131.htm" target="_blank">Yong Zhao</a><sup>4</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://www.ieeeiciea.org/2023/Prof.PhilipChen.html" target="_blank">C. L. Philip Chen</a><sup>5</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Guizhou University <sup>2</sup> Guizhou University of Commerce <sup>3</sup> Northwestern Polytechnical
                      University <br><sup>4</sup> Peking University <sup>5</sup> South China University of Technology
                      <br>CVPR 2024 Expand Version</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2411.12426" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ZYangChen/MoCha-Stereo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.12426" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here
      </video> -->
      <img src="static/images/motivation.jpg">
      <h2 class="subtitle has-text-centered">
        In the deep learning process, feature channels are extended to higher dimensions. During this process, the geometric details learned in individual feature channels are inevitably lost. 
        We propose that these details can be recovered by identifying recurrent geometric structures across the channels. MoCha-V2 constructs a directed graph, i.e., motif correlation graph (MCG), 
        to capture frequent patterns in feature channels. Specifically, MCG leverages node weights to detect recurring geometric structures within the channels. MCG establishes a robust and stable framework for identifying repetitive geometric structures. 
        Due to its interpretability, MoCha-V2 also demonstrates significant potential for safety-critical real-world applications.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Real-world applications of stereo matching, such as autonomous driving, place stringent demands on both safety and accuracy. However, learning-based stereo matching methods inherently suffer from the loss of geometric structures in certain feature channels, creating a bottleneck in achieving precise detail matching. Additionally, these methods lack interpretability due to the black-box nature of deep learning. In this paper, we propose MoCha-V2, a novel learning-based paradigm for stereo matching. MoCha-V2 introduces the Motif Correlation Graph (MCG) to capture recurring textures, which are referred to as ``motifs" within feature channels. These motifs reconstruct geometric structures and are learned in a more interpretable way. Subsequently, we integrate features from multiple frequency domains through wavelet inverse transformation. The resulting motif features are utilized to restore geometric structures in the stereo matching process. Experimental results demonstrate the effectiveness of MoCha-V2. MoCha-V2 achieved <a href="https://zyangchen.github.io/img/bad1-all.html">1st place on the Middlebury benchmark at the time of its release</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/MoCha1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Pipeline of our previous work, MoCha-Stereo. 
          MoCha-Stereo first constructs the Motif Channel Correlation Volume (MCCV). 
          MCCV is build by projecting the relationship between motif channels and normal channels into the basic group correlation volume. 
          Subsequently, we employ a iterative update operator to caculate a disparity, accroding to the correlation value. 
          Finally, the Reconstruction Error Motif Penalty (REMP) module is applied to penalize the generation of the full-resolution disparity map. 
          In REMP, $LFE$ refers to the Low-frequency Error branch, $LMC$ denotes to the Latent Motif Channel branch, and $HFE$ means the High-frequency Error branch.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/MoCha2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Architecture overview of MoCha-V2. We improved the method of obtaining Motif Channels in MoCha-Stereo by introducing the Motif Correlation Graph Attention (MCGA).
        </h2>
      </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/eth3d.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Visual comparisons with SOTA stereo methods on the ETH3D test set. In the first row, GMStereo and GANet+ADL fail to capture the fine-level geometry of sluices and pipes. 
        In the second row, reflection effects make it challenging for existing methods to accurately identify the contours of the sculpture. Furthermore, among the three methods, only MoCha-V2 successfully detects the outline of the thin object adjacent to the sculpture. In the third row, lighting effects mislead existing methods, resulting in inaccuracies when identifying lamps, walls, and thin objects. In contrast, MoCha-V2 accurately identifies these objects and preserves their geometric contours.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/MCG.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Visualization of feature channels. We utilize Principal Component Analysis (PCA) to reduce the channel features to a one-dimensional feature map and visualize it.
      </h2>
   </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Paper poster
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{chen2024mocha,
          title={MoCha-Stereo: Motif Channel Attention Network for Stereo Matching},
          author={Chen, Ziyang and Long, Wei and Yao, He and Zhang, Yongjun and Wang, Bingshu and Qin, Yongbin and Wu, Jia},
          booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
          pages={27768--27777},
          year={2024}
        }
        
        @article{chen2024motif,
          title={Motif Channel Opened in a White-Box: Stereo Matching via Motif Correlation Graph},
          author={Chen, Ziyang and Zhang, Yongjun and Li, Wenting and Wang, Bingshu and Zhao, Yong and Chen, CL},
          journal={arXiv preprint arXiv:2411.12426},
          year={2024}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
