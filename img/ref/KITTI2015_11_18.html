<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!-- saved from url=(0074)https://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>The KITTI Vision Benchmark Suite</title>
<link rel="stylesheet" type="text/css" href="./KITTI2015_files/style.css" media="screen">
<style type="text/css" data-fbcssmodules="css:fb.css.base css:fb.css.dialog css:fb.css.iframewidget css:fb.css.customer_chat_plugin_iframe">.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_reposition{overflow:hidden;position:relative}.fb_invisible{display:none}.fb_reset{background:none;border:0;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:'lucida grande', tahoma, verdana, arial, sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}@keyframes fb_transform{from{opacity:0;transform:scale(.95)}to{opacity:1;transform:scale(1)}}.fb_animate{animation:fb_transform .3s forwards}
.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_reposition{overflow:hidden;position:relative}.fb_invisible{display:none}.fb_reset{background:none;border:0;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:'lucida grande', tahoma, verdana, arial, sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}@keyframes fb_transform{from{opacity:0;transform:scale(.95)}to{opacity:1;transform:scale(1)}}.fb_animate{animation:fb_transform .3s forwards}
.fb_dialog{background:rgba(82, 82, 82, .7);position:absolute;top:-10000px;z-index:10001}.fb_dialog_advanced{border-radius:8px;padding:10px}.fb_dialog_content{background:#fff;color:#373737}.fb_dialog_close_icon{background:url(https://connect.facebook.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 0 transparent;cursor:pointer;display:block;height:15px;position:absolute;right:18px;top:17px;width:15px}.fb_dialog_mobile .fb_dialog_close_icon{left:5px;right:auto;top:5px}.fb_dialog_padding{background-color:transparent;position:absolute;width:1px;z-index:-1}.fb_dialog_close_icon:hover{background:url(https://connect.facebook.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -15px transparent}.fb_dialog_close_icon:active{background:url(https://connect.facebook.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -30px transparent}.fb_dialog_iframe{line-height:0}.fb_dialog_content .dialog_title{background:#6d84b4;border:1px solid #365899;color:#fff;font-size:14px;font-weight:bold;margin:0}.fb_dialog_content .dialog_title>span{background:url(https://connect.facebook.net/rsrc.php/v3/yd/r/Cou7n-nqK52.gif) no-repeat 5px 50%;float:left;padding:5px 0 7px 26px}body.fb_hidden{height:100%;left:0;margin:0;overflow:visible;position:absolute;top:-10000px;transform:none;width:100%}.fb_dialog.fb_dialog_mobile.loading{background:url(https://connect.facebook.net/rsrc.php/v3/ya/r/3rhSv5V8j3o.gif) white no-repeat 50% 50%;min-height:100%;min-width:100%;overflow:hidden;position:absolute;top:0;z-index:10001}.fb_dialog.fb_dialog_mobile.loading.centered{background:none;height:auto;min-height:initial;min-width:initial;width:auto}.fb_dialog.fb_dialog_mobile.loading.centered #fb_dialog_loader_spinner{width:100%}.fb_dialog.fb_dialog_mobile.loading.centered .fb_dialog_content{background:none}.loading.centered #fb_dialog_loader_close{clear:both;color:#fff;display:block;font-size:18px;padding-top:20px}#fb-root #fb_dialog_ipad_overlay{background:rgba(0, 0, 0, .4);bottom:0;left:0;min-height:100%;position:absolute;right:0;top:0;width:100%;z-index:10000}#fb-root #fb_dialog_ipad_overlay.hidden{display:none}.fb_dialog.fb_dialog_mobile.loading iframe{visibility:hidden}.fb_dialog_mobile .fb_dialog_iframe{position:sticky;top:0}.fb_dialog_content .dialog_header{background:linear-gradient(from(#738aba), to(#2c4987));border-bottom:1px solid;border-color:#043b87;box-shadow:white 0 1px 1px -1px inset;color:#fff;font:bold 14px Helvetica, sans-serif;text-overflow:ellipsis;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0;vertical-align:middle;white-space:nowrap}.fb_dialog_content .dialog_header table{height:43px;width:100%}.fb_dialog_content .dialog_header td.header_left{font-size:12px;padding-left:5px;vertical-align:middle;width:60px}.fb_dialog_content .dialog_header td.header_right{font-size:12px;padding-right:5px;vertical-align:middle;width:60px}.fb_dialog_content .touchable_button{background:linear-gradient(from(#4267B2), to(#2a4887));background-clip:padding-box;border:1px solid #29487d;border-radius:3px;display:inline-block;line-height:18px;margin-top:3px;max-width:85px;padding:4px 12px;position:relative}.fb_dialog_content .dialog_header .touchable_button input{background:none;border:none;color:#fff;font:bold 12px Helvetica, sans-serif;margin:2px -12px;padding:2px 6px 3px 6px;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog_content .dialog_header .header_center{color:#fff;font-size:16px;font-weight:bold;line-height:18px;text-align:center;vertical-align:middle}.fb_dialog_content .dialog_content{background:url(https://connect.facebook.net/rsrc.php/v3/y9/r/jKEcVPZFk-2.gif) no-repeat 50% 50%;border:1px solid #4a4a4a;border-bottom:0;border-top:0;height:150px}.fb_dialog_content .dialog_footer{background:#f5f6f7;border:1px solid #4a4a4a;border-top-color:#ccc;height:40px}#fb_dialog_loader_close{float:left}.fb_dialog.fb_dialog_mobile .fb_dialog_close_icon{visibility:hidden}#fb_dialog_loader_spinner{animation:rotateSpinner 1.2s linear infinite;background-color:transparent;background-image:url(https://connect.facebook.net/rsrc.php/v3/yD/r/t-wz8gw1xG1.png);background-position:50% 50%;background-repeat:no-repeat;height:24px;width:24px}@keyframes rotateSpinner{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}
.fb_iframe_widget{display:inline-block;position:relative}.fb_iframe_widget span{display:inline-block;position:relative;text-align:justify}.fb_iframe_widget iframe{position:absolute}.fb_iframe_widget_fluid_desktop,.fb_iframe_widget_fluid_desktop span,.fb_iframe_widget_fluid_desktop iframe{max-width:100%}.fb_iframe_widget_fluid_desktop iframe{min-width:220px;position:relative}.fb_iframe_widget_lift{z-index:1}.fb_iframe_widget_fluid{display:inline}.fb_iframe_widget_fluid span{width:100%}
.fb_mpn_mobile_landing_page_slide_out{animation-duration:200ms;animation-name:fb_mpn_landing_page_slide_out;transition-timing-function:ease-in}.fb_mpn_mobile_landing_page_slide_out_from_left{animation-duration:200ms;animation-name:fb_mpn_landing_page_slide_out_from_left;transition-timing-function:ease-in}.fb_mpn_mobile_landing_page_slide_up{animation-duration:500ms;animation-name:fb_mpn_landing_page_slide_up;transition-timing-function:ease-in}.fb_mpn_mobile_bounce_in{animation-duration:300ms;animation-name:fb_mpn_bounce_in;transition-timing-function:ease-in}.fb_mpn_mobile_bounce_out{animation-duration:300ms;animation-name:fb_mpn_bounce_out;transition-timing-function:ease-in}.fb_mpn_mobile_bounce_out_v2{animation-duration:300ms;animation-name:fb_mpn_fade_out;transition-timing-function:ease-in}.fb_customer_chat_bounce_in_v2{animation-duration:300ms;animation-name:fb_bounce_in_v2;transition-timing-function:ease-in}.fb_customer_chat_bounce_in_from_left{animation-duration:300ms;animation-name:fb_bounce_in_from_left;transition-timing-function:ease-in}.fb_customer_chat_bounce_out_v2{animation-duration:300ms;animation-name:fb_bounce_out_v2;transition-timing-function:ease-in}.fb_customer_chat_bounce_out_from_left{animation-duration:300ms;animation-name:fb_bounce_out_from_left;transition-timing-function:ease-in}.fb_invisible_flow{display:inherit;height:0;overflow-x:hidden;width:0}@keyframes fb_mpn_landing_page_slide_out{0%{margin:0 12px;width:100% - 24px}60%{border-radius:18px}100%{border-radius:50%;margin:0 24px;width:60px}}@keyframes fb_mpn_landing_page_slide_out_from_left{0%{left:12px;width:100% - 24px}60%{border-radius:18px}100%{border-radius:50%;left:12px;width:60px}}@keyframes fb_mpn_landing_page_slide_up{0%{bottom:0;opacity:0}100%{bottom:24px;opacity:1}}@keyframes fb_mpn_bounce_in{0%{opacity:.5;top:100%}100%{opacity:1;top:0}}@keyframes fb_mpn_fade_out{0%{bottom:30px;opacity:1}100%{bottom:0;opacity:0}}@keyframes fb_mpn_bounce_out{0%{opacity:1;top:0}100%{opacity:.5;top:100%}}@keyframes fb_bounce_in_v2{0%{opacity:0;transform:scale(0, 0);transform-origin:bottom right}50%{transform:scale(1.03, 1.03);transform-origin:bottom right}100%{opacity:1;transform:scale(1, 1);transform-origin:bottom right}}@keyframes fb_bounce_in_from_left{0%{opacity:0;transform:scale(0, 0);transform-origin:bottom left}50%{transform:scale(1.03, 1.03);transform-origin:bottom left}100%{opacity:1;transform:scale(1, 1);transform-origin:bottom left}}@keyframes fb_bounce_out_v2{0%{opacity:1;transform:scale(1, 1);transform-origin:bottom right}100%{opacity:0;transform:scale(0, 0);transform-origin:bottom right}}@keyframes fb_bounce_out_from_left{0%{opacity:1;transform:scale(1, 1);transform-origin:bottom left}100%{opacity:0;transform:scale(0, 0);transform-origin:bottom left}}@keyframes slideInFromBottom{0%{opacity:.1;transform:translateY(100%)}100%{opacity:1;transform:translateY(0)}}@keyframes slideInFromBottomDelay{0%{opacity:0;transform:translateY(100%)}97%{opacity:0;transform:translateY(100%)}100%{opacity:1;transform:translateY(0)}}</style></head>
<body>

<div id="fb-root" class=" fb_reset"><div style="position: absolute; top: -10000px; width: 0px; height: 0px;"><div></div></div></div>
<script src="./KITTI2015_files/sdk.js.下载" async="" crossorigin="anonymous"></script><script id="facebook-jssdk" src="./KITTI2015_files/sdk(1).js.下载"></script><script>(function(d, s, id) {
var js, fjs = d.getElementsByTagName(s)[0];
if (d.getElementById(id)) return;
js = d.createElement(s); js.id = id;
js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.6";
fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<div id="wrapper">
<div id="header" class="container">
<div id="logo">
<h1><a href="https://www.cvlibs.net/datasets/kitti">The KITTI Vision Benchmark Suite</a></h1>
<h2>A project of <a href="http://www.kit.edu/english" target="_blank">Karlsruhe Institute of Technology</a><br>and
<a href="http://www.ttic.edu/" target="_blank">Toyota Technological Institute at Chicago</a></h2>
</div>
<div id="banner">
<a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/home/" target="_blank"><img src="./KITTI2015_files/unitue.jpg"></a>
<a href="http://www.ttic.edu/" target="_blank"><img src="./KITTI2015_files/ttic.jpg"></a>
<a href="http://www.kit.edu/english" target="_blank"><img src="./KITTI2015_files/kit.jpg"></a>
</div>
</div>
<div id="menu" class="container">
<ul id="navigation">
<li><a href="https://www.cvlibs.net/datasets/kitti/index.php">home</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/setup.php">setup</a></li>
<li class="active"><a href="https://www.cvlibs.net/datasets/kitti/eval_stereo.php">stereo</a>
<ul>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo" "="">Stereo 2012</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo">Stereo 2015</a></li>
</ul>
</li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_flow.php">flow</a>
<ul>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=flow" "="">Flow 2012</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow">Flow 2015</a></li>
</ul>
</li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow.php">sceneflow</a></li>

<li><a href="https://www.cvlibs.net/datasets/kitti/eval_depth_all.php">depth</a>
<ul>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion">Depth Completion</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction">Depth Prediction</a></li>
</ul>
</li>

<li><a href="https://www.cvlibs.net/datasets/kitti/eval_odometry.php">odometry</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_3dobject.php">object</a>
<ul>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d" "="">2d object</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" "="">3d object</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=bev">bird's eye view</a></li>
</ul>
</li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_tracking_overview.php">tracking</a>
<ul>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_tracking.php" "="">multi-object tracking</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_mots.php" "="">multi-object tracking and segmentation</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_step.php" "="">segmenting and tracking every pixel</a></li>
</ul>
</li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_road.php">road</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_semantics.php">semantics</a>
<ul>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_semseg.php?benchmark=semantics2015" "="">pixel-level</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/eval_instance_seg.php?benchmark=instanceSeg2015" "="">instance-level</a></li>
</ul>
</li>
<li><a href="https://www.cvlibs.net/datasets/kitti/raw_data.php">raw data</a></li>
<li><a href="https://www.cvlibs.net/datasets/kitti/user_submit.php">submit results</a></li>
</ul>
</div>
<div id="top-bar" class="container">
<div class="bar"><div class="text">
  <div style="width:900px">
<div style="float:left;">
  <a href="https://www.cvlibs.net/" target="_blank">A. Geiger</a> |
  <a href="http://www.mrt.kit.edu/mitarbeiter_lenz.php" target="_blank">P. Lenz</a> |
  <a href="http://www.mrt.kit.edu/mitarbeiter_stiller.php" target="_blank">C. Stiller</a> |
  <a href="http://ttic.uchicago.edu/~rurtasun/" target="_blank">R. Urtasun</a>
</div>
<div style="float:right;">
ziyang chen | <a href="https://www.cvlibs.net/datasets/kitti/user_logout.php">Log out</a>
</div>
</div>
</div></div>
</div>
<div id="page" class="container">
<div id="content">


<div class="section">
<h2 class="title">Stereo Evaluation 2015</h2>
<div class="entry">

<p>
<img src="./KITTI2015_files/header_scene_flow.jpg"><br>
The <b>stereo 2015</b> / <b>flow 2015</b> / <b>scene flow 2015</b> benchmark consists of 200 training scenes and 200 test scenes (4 color images per scene, saved in loss less png format). Compared to the stereo 2012 and flow 2012 benchmarks, it comprises dynamic scenes for which the ground truth has been established in a semi-automatic process. Our evaluation server computes the percentage of bad pixels averaged over all ground truth pixels of all 200 test images. For this benchmark, we consider a pixel to be correctly estimated if the disparity or flow end-point error is <b>&lt;3px or &lt;5%</b> (for scene flow this criterion needs to be fulfilled for both disparity maps and the flow map). We require that all methods use the same parameter set for all test pairs. Our development kit provides details about the data format as well as MATLAB / C++ utility functions for reading and writing disparity maps and flow fields. More details can be found in <a href="https://www.cvlibs.net/publications/Menze2015CVPR.pdf" target="_blank">Object Scene Flow for Autonomous Vehicles (CVPR 2015)</a>.
</p><ul>
<li><a href="https://s3.eu-central-1.amazonaws.com/avg-kitti/data_scene_flow.zip" target="_blank">Download stereo 2015/flow 2015/scene flow 2015 data set (2 GB)</a></li>
<li><a href="https://s3.eu-central-1.amazonaws.com/avg-kitti/data_scene_flow_calib.zip" target="_blank">Download calibration files (1 MB)</a></li>
<li><a href="https://s3.eu-central-1.amazonaws.com/avg-kitti/data_scene_flow_multiview.zip" target="_blank">Download multi-view extension (20 frames per scene) (14 GB)</a></li>
<li><a href="https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_scene_flow.zip" target="_blank">Download development kit (3 MB)</a></li>
</ul>
<p></p>
<p>
Our evaluation table ranks all methods according to the number of erroneous pixels. All methods providing less than 100 % density have been interpolated using simple background interpolation as explained in the corresponding header file in the development kit. Legend:
</p>
<ul>
<li><b>D1:</b> Percentage of stereo disparity outliers in first frame</li>
<li><b>D2:</b> Percentage of stereo disparity outliers in second frame</li>
<li><b>Fl:</b> Percentage of optical flow outliers</li>
<li><b>SF:</b> Percentage of scene flow outliers (=outliers in either D0, D1 or Fl)</li>
<li><b>bg:</b> Percentage of outliers averaged only over background regions</li>
<li><b>fg:</b> Percentage of outliers averaged only over foreground regions</li>
<li><b>all:</b> Percentage of outliers averaged over all ground truth pixels</li>
</ul>
<p>
</p>

<br>
<b>Note:</b> On 13.03.2017 we have fixed several small errors in the flow (noc+occ) ground truth of the dynamic foreground objects and manually verified all images for correctness by warping them according to the ground truth. As a consequence, all error numbers have decreased slightly. Please download the devkit and the annotations with the improved ground truth for the training set again if you have downloaded the files prior to 13.03.2017 and consider reporting these new number in all future publications. The last leaderboards before these corrections can be found <a href="https://www.cvlibs.net/datasets/kitti/backups/2017_03_13_15_52_09_flow_2015.html" target="blank">here (optical flow 2015)</a> and <a href="https://www.cvlibs.net/datasets/kitti/backups/2017_03_13_15_52_09_scene_flow_2015.html" target="blank">here (scene flow 2015)</a>. The leaderboards for the KITTI 2015 stereo benchmarks did not change.<br><br>

<div class="alertbox_600"><b>Important Policy Update:</b> As more and more non-published work and re-implementations of existing work is submitted to KITTI, we have established a new policy: from now on, only submissions with significant novelty that are leading to a peer-reviewed paper in a conference or journal are allowed. Minor modifications of existing algorithms or student research projects are not allowed. Such work must be evaluated on a split of the training set. To ensure that our policy is adopted, new users must detail their status, describe their work and specify the targeted venue during registration. Furthermore, we will regularly delete all entries that are 6 months old but are still anonymous or do not have a paper associated with them. For conferences, 6 month is enough to determine if a paper has been accepted and to add the bibliography information. For longer review cycles, you need to resubmit your results.</div><div class="alertbox_600"><b><u><center>Additional information used by the methods</center></u></b><ul><li><img src="./KITTI2015_files/icon_fl.png"> Flow: Method uses optical flow (2 temporally adjacent images)</li><li><img src="./KITTI2015_files/icon_mv.png"> Multiview: Method uses more than 2 temporally adjacent images</li><li><img src="./KITTI2015_files/icon_ms.png"> Motion stereo: Method uses epipolar geometry for computing optical flow</li><li><img src="./KITTI2015_files/icon_at.png"> Additional training data: Use of additional data sources for training (see details)</li></ul></div>
<!--
<br>
<a href="http://www.robustvision.net"><center>
<img src="http://www.robustvision.net/images/banner.png" /></center></a>
-->
<br>

<form method="get" name="form_options" action="https://www.cvlibs.net/datasets/kitti/eval_scene_flow.php">
<input type="hidden" name="benchmark" value="stereo">
<b>Evaluation ground truth</b>
<select name="eval_gt" onchange="document.forms[&#39;form_options&#39;].submit()"> 
<option value="all" selected="">All pixels</option><option value="noc">Non-Occluded pixels</option></select>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<b>Evaluation area</b>
<select name="eval_area" onchange="document.forms[&#39;form_options&#39;].submit()"> 
<option value="all" selected="">All pixels </option><option value="est">Estimated pixels </option></select> 
</form>
<br>




<form method="post" action="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo"><table class="results">
  <tbody><tr class="heading">
    <td class="results"></td>
     <td class="results">Method</td>
     <td class="results">Setting</td>
     <td class="results">Code</td>
     <td class="results">D1-bg</td>
     <td class="results">D1-fg</td>
     <td class="results"><span style="color:red"><u>D1-all</u></span></td>
     <td class="results">Density</td>
     <td class="results">Runtime</td>
     <td class="results">Environment</td>
     <td class="results"><input type="submit" value="Compare" name="compare"></td>
   </tr>
   <tr>
    <td class="results">1</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=8ad7a3fbb8e4bd9964afabac7e5a3babed26c0df">MoCha-Stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.36 %</td>
     <td class="results"> 2.43 %</td>
     <td class="results"><b> 1.53 %</b></td>
     <td class="results">100.00 %</td>
     <td class="results">0.27 s</td>
     <td class="results">NVIDIA Tesla A6000 (PyTorch)</td>
     <td class="results"><input type="checkbox" value="8ad7a3fbb8e4bd9964afabac7e5a3babed26c0df" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">2</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b639f3612cea170d21e9f06231a9c3d769a8a9aa">DiffuVolume</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.35 %</td>
     <td class="results"> 2.51 %</td>
     <td class="results"> 1.54 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.36 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="b639f3612cea170d21e9f06231a9c3d769a8a9aa" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">3</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f5a4785b2479e8d9888b12ad94fa8e637ffd815d">GANet+ADL</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.38 %</td>
     <td class="results"> 2.38 %</td>
     <td class="results"> 1.55 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.67s</td>
     <td class="results">NVIDIA RTX 3090 (PyTorch)	</td>
     <td class="results"><input type="checkbox" value="f5a4785b2479e8d9888b12ad94fa8e637ffd815d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">4</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=cbf9e1cd5c807d35798d28dce58e3762062fd3d8">MC-Stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.36 %</td>
     <td class="results"> 2.51 %</td>
     <td class="results"> 1.55 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.40 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="cbf9e1cd5c807d35798d28dce58e3762062fd3d8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">5</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=e90dcc903c0177bc00a16727e0b656eef508304e">IGEV-ICGNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.38 %</td>
     <td class="results"> 2.55 %</td>
     <td class="results"> 1.57 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.18 s</td>
     <td class="results">NVIDIA Tesla A5000 (Pytorch)</td>
     <td class="results"><input type="checkbox" value="e90dcc903c0177bc00a16727e0b656eef508304e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">6</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=909f3a72f63cfb28b1aadd1d64fa99d17cd1fc1f">yjlig</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.37 %</td>
     <td class="results"> 2.62 %</td>
     <td class="results"> 1.58 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.35 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="909f3a72f63cfb28b1aadd1d64fa99d17cd1fc1f" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">7</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=72351c69795fa0f21df4612b8dce9e33a497e808">UGNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.34 %</td>
     <td class="results"> 2.77 %</td>
     <td class="results"> 1.58 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">GPU @ 3.0 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="72351c69795fa0f21df4612b8dce9e33a497e808" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">8</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=3df5f417e3b22ff5e254ddb4b9c8d62d205da8ed">Any-IGEV</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.43 %</td>
     <td class="results"> 2.35 %</td>
     <td class="results"> 1.58 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.32 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="3df5f417e3b22ff5e254ddb4b9c8d62d205da8ed" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">9</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=671afd2f1f9a52934629319942ffffe400f5c4a3">OpenStereo-IGEV</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/XiandaGuo/OpenStereo" target="blank">code</a></td>
     <td class="results"> 1.44 %</td>
     <td class="results"><b> 2.31 %</b></td>
     <td class="results"> 1.59 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.18 s</td>
     <td class="results">NVIDIA-3090</td>
     <td class="results"><input type="checkbox" value="671afd2f1f9a52934629319942ffffe400f5c4a3" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">ERROR: Wrong syntax in BIBTEX file.<br></td>
   </tr>
   <tr>
    <td class="results">10</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=eebfd7f2c168b0ce93d3c5f86b9d01a1b79de9d1">GSSNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.31 %</td>
     <td class="results"> 2.96 %</td>
     <td class="results"> 1.59 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.78 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="eebfd7f2c168b0ce93d3c5f86b9d01a1b79de9d1" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">11</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=9333da33677132e4bf3d4d2883fd4e2b0e833f69">CWA-stereo-v1</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.38 %</td>
     <td class="results"> 2.66 %</td>
     <td class="results"> 1.59 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.23 s</td>
     <td class="results">2080</td>
     <td class="results"><input type="checkbox" value="9333da33677132e4bf3d4d2883fd4e2b0e833f69" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">12</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=99ce4b75496a9c36494154f56886decea264a639">MDM-Stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"><b> 1.28 %</b></td>
     <td class="results"> 3.13 %</td>
     <td class="results"> 1.59 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.09 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="99ce4b75496a9c36494154f56886decea264a639" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">13</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b9603a567a1d3553196867d145ffba0da25069a8">CroCo-Stereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/naver/croco" target="blank">code</a></td>
     <td class="results"> 1.38 %</td>
     <td class="results"> 2.65 %</td>
     <td class="results"> 1.59 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.93s</td>
     <td class="results">NVIDIA A100</td>
     <td class="results"><input type="checkbox" value="b9603a567a1d3553196867d145ffba0da25069a8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">P. Weinzaepfel, T. Lucas, V. Leroy, Y. Cabon, V. Arora, R. Br\'egier, G. Csurka, L. Antsfeld, B. Chidlovskii and J. Revaud: <a href="http://scholar.google.de/scholar?q=CroCo%20v2:%20Improved%20Cross-view%20Completion%20Pre-training%20for%20Stereo%20Matching%20and%20Optical%20Flow"> CroCo v2: Improved Cross-view Completion 
Pre-training for Stereo Matching and Optical Flow</a>. ICCV 2023.<br></td>
   </tr>
   <tr>
    <td class="results">14</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=2d4914d352df5d8b12c3dda08ef85f47e687f236">IGEV-Stereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/gangweiX/IGEV" target="blank">code</a></td>
     <td class="results"> 1.38 %</td>
     <td class="results"> 2.67 %</td>
     <td class="results"> 1.59 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.18 s</td>
     <td class="results">NVIDIA RTX 3090 (PyTorch)</td>
     <td class="results"><input type="checkbox" value="2d4914d352df5d8b12c3dda08ef85f47e687f236" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">G. Xu, X. Wang, X. Ding and X. Yang: <a href="http://scholar.google.de/scholar?q=Iterative%20Geometry%20Encoding%20Volume%20for%20Stereo%20Matching"> Iterative Geometry Encoding Volume for 
Stereo Matching</a>. CVPR 2023.<br></td>
   </tr>
   <tr>
    <td class="results">15</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=7eb37216cf5b83ac5cf1098604526d637bf3e50a">AMSCF-Net</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.32 %</td>
     <td class="results"> 2.98 %</td>
     <td class="results"> 1.60 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="7eb37216cf5b83ac5cf1098604526d637bf3e50a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">16</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=3ff20cb6b04238e129a8b0f4a52a1f6aef4a9064">CGF-ACV</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/gangweiX/CGI-Stereo" target="blank">code</a></td>
     <td class="results"> 1.32 %</td>
     <td class="results"> 3.08 %</td>
     <td class="results"> 1.61 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.24 s</td>
     <td class="results">NVIDIA RTX 3090 (PyTorch)</td>
     <td class="results"><input type="checkbox" value="3ff20cb6b04238e129a8b0f4a52a1f6aef4a9064" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">17</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=d1ad0483ee3591614dd66918c76ef342f1ffe2ab">UPFNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.38 %</td>
     <td class="results"> 2.85 %</td>
     <td class="results"> 1.62 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.25 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="d1ad0483ee3591614dd66918c76ef342f1ffe2ab" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Q. Chen, B. Ge and J. Quan: <a href="http://scholar.google.de/scholar?q=Unambiguous%20Pyramid%20Cost%20Volumes%20Fusion%20for%20Stereo%20Matching"> Unambiguous Pyramid Cost Volumes Fusion 
for Stereo Matching</a>. IEEE Transactions on Circuits and 
Systems for Video Technology 2023.<br></td>
   </tr>
   <tr>
    <td class="results">18</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=d5b2b8e074841a2c348409307aaa2b910b7234cf">yjlig</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.42 %</td>
     <td class="results"> 2.66 %</td>
     <td class="results"> 1.62 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.35 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="d5b2b8e074841a2c348409307aaa2b910b7234cf" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">19</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=1ea457825cb28b79b0785a54da036f50196683ba">SSMF</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.37 %</td>
     <td class="results"> 2.91 %</td>
     <td class="results"> 1.63 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.20 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="1ea457825cb28b79b0785a54da036f50196683ba" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">20</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=7b63f139fa9edb242103c98c75050e1cafc837ed">UDG</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.39 %</td>
     <td class="results"> 2.88 %</td>
     <td class="results"> 1.64 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.4 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="7b63f139fa9edb242103c98c75050e1cafc837ed" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">21</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=28f1f46774fe5e9a80b2b734ad91ff225cef4b2a">SCVFormer</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.31 %</td>
     <td class="results"> 3.26 %</td>
     <td class="results"> 1.64 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.09 s</td>
     <td class="results">NVIDIA RTX 3090 (PyTorch)</td>
     <td class="results"><input type="checkbox" value="28f1f46774fe5e9a80b2b734ad91ff225cef4b2a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">22</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=fb0ff69c720042202f474c26986c9c726ec5a6d1">DiffuVolume</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.39 %</td>
     <td class="results"> 2.93 %</td>
     <td class="results"> 1.65 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.36 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="fb0ff69c720042202f474c26986c9c726ec5a6d1" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">23</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=729684b58e306281be582f9f4284f923be4be1d6">M-FUSE</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div><div class="icon"><img src="./KITTI2015_files/icon_mv.png" alt="This method makes use of multiple (&gt;2) views."></div></td>
     <td class="results"><a href="https://github.com/cv-stuttgart/M-FUSE" target="blank">code</a></td>
     <td class="results"> 1.40 %</td>
     <td class="results"> 2.91 %</td>
     <td class="results"> 1.65 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.3 s</td>
     <td class="results">GPU</td>
     <td class="results"><input type="checkbox" value="729684b58e306281be582f9f4284f923be4be1d6" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">L. Mehl, A. Jahedi, J. Schmalfuss and A. Bruhn: <a href="http://scholar.google.de/scholar?q=M-FUSE:%20Multi-frame%20Fusion%20for%20Scene%20Flow%20Estimation"> M-FUSE: Multi-frame Fusion for Scene Flow Estimation</a>. Proc. Winter Conference on Applications of Computer Vision (WACV) 2023.<br></td>
   </tr>
   <tr>
    <td class="results">24</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=150c8554f90564428f79e481c3dd9e97ba1b36d8">SF2SE3</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="https://github.com/lmb-freiburg/sf2se3" target="blank">code</a></td>
     <td class="results"> 1.40 %</td>
     <td class="results"> 2.91 %</td>
     <td class="results"> 1.65 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2.7 s</td>
     <td class="results">GPU @ &gt;3.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="150c8554f90564428f79e481c3dd9e97ba1b36d8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">L. Sommer, P. Schröppel and T. Brox: <a href="http://scholar.google.de/scholar?q=SF2SE3:%20Clustering%20Scene%20Flow%20into%20SE%20(3)-Motions%20via%20Proposal%20and%20Selection"> SF2SE3: Clustering Scene Flow into SE (3)-Motions via Proposal and Selection</a>. DAGM German Conference on Pattern Recognition 2022.<br></td>
   </tr>
   <tr>
    <td class="results">25</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=34355e9ba7dff3c353e422271aa8f2abbc8a5c47">LEAStereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/XuelianCheng/LEAStereo" target="blank">code</a></td>
     <td class="results"> 1.40 %</td>
     <td class="results"> 2.91 %</td>
     <td class="results"> 1.65 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.30 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="34355e9ba7dff3c353e422271aa8f2abbc8a5c47" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">X. Cheng, Y. Zhong, M. Harandi, Y. Dai, X. Chang, H. Li, T. Drummond and Z. Ge: <a href="http://scholar.google.de/scholar?q=Hierarchical%20Neural%20Architecture%20Search%20for%20Deep%20Stereo%20Matching"> Hierarchical Neural Architecture Search 
for Deep Stereo Matching</a>. Advances in Neural Information 
Processing Systems 2020.<br></td>
   </tr>
   <tr>
    <td class="results">26</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=759415c94ab7669c96af6f96d7b799c1f1e7cf16">EFLOW</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.40 %</td>
     <td class="results"> 2.91 %</td>
     <td class="results"> 1.65 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.06 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="759415c94ab7669c96af6f96d7b799c1f1e7cf16" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">27</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b424b7db7488b7acdae14c1d7055793f6ceeda27">SplatFlow3D</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="https://github.com/wwsource/SplatFlow3D" target="blank">code</a></td>
     <td class="results"> 1.40 %</td>
     <td class="results"> 2.91 %</td>
     <td class="results"> 1.65 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="b424b7db7488b7acdae14c1d7055793f6ceeda27" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">28</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=60d52743ada82512f75bef0805c57267247e0d74">LoS</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.42 %</td>
     <td class="results"> 2.81 %</td>
     <td class="results"> 1.65 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.19 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="60d52743ada82512f75bef0805c57267247e0d74" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">29</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=be440a5d507459a31ea6b3afc2018b9c0f352538">ACVNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/gangweiX/ACVNet" target="blank">code</a></td>
     <td class="results"> 1.37 %</td>
     <td class="results"> 3.07 %</td>
     <td class="results"> 1.65 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">NVIDIA RTX 3090 (PyTorch)</td>
     <td class="results"><input type="checkbox" value="be440a5d507459a31ea6b3afc2018b9c0f352538" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">G. Xu, J. Cheng, P. Guo and X. Yang: <a href="http://scholar.google.de/scholar?q=Attention%20Concatenation%20Volume%20for%20Accurate%20and%20Efficient%20Stereo%20Matching"> Attention Concatenation Volume for 
Accurate and Efficient Stereo Matching</a>. CVPR 2022.<br></td>
   </tr>
   <tr>
    <td class="results">30</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ae750f0650ba90474cfd981e03033292dc109982">SPRNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.37 %</td>
     <td class="results"> 3.12 %</td>
     <td class="results"> 1.66 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">GPU @ &gt;3.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="ae750f0650ba90474cfd981e03033292dc109982" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">31</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=d1419e3616212f17bfd6abfec270c48b0640e0a8">DCANet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.42 %</td>
     <td class="results"> 2.91 %</td>
     <td class="results"> 1.66 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.19 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="d1419e3616212f17bfd6abfec270c48b0640e0a8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">32</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=e2e12c79187cc1d325c6c3a2aa2f7e80af2a4f40">PCWNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/gallenszl/PCW-Net" target="blank">code</a></td>
     <td class="results"> 1.37 %</td>
     <td class="results"> 3.16 %</td>
     <td class="results"> 1.67 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.44 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="e2e12c79187cc1d325c6c3a2aa2f7e80af2a4f40" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Shen, Y. Dai, X. Song, Z. Rao, D. Zhou and L. Zhang: <a href="http://scholar.google.de/scholar?q=PCW-Net:%20Pyramid%20Combination%20and%20Warping%20Cost%20Volume%20for%20Stereo%20Matching"> PCW-Net: Pyramid Combination and Warping 
Cost Volume for Stereo Matching</a>. European Conference on Computer 
Vision(ECCV) 2022.<br></td>
   </tr>
   <tr>
    <td class="results">33</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=bdea1199e31049d7b6ba2d0aa5427fdd113bda47">LaC+GANet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/SpadeLiu/Lac-GwcNet" target="blank">code</a></td>
     <td class="results"> 1.44 %</td>
     <td class="results"> 2.83 %</td>
     <td class="results"> 1.67 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.8 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="bdea1199e31049d7b6ba2d0aa5427fdd113bda47" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">B. Liu, H. Yu and Y. Long: <a href="http://scholar.google.de/scholar?q=Local%20Similarity%20Pattern%20and%20Cost%20Self-Reassembling%20for%20Deep%20Stereo%20Matching%20Networks"> Local Similarity Pattern and Cost Self-
Reassembling for Deep Stereo Matching Networks</a>. Proceedings of the AAAI Conference on 
Artificial Intelligence 2022.<br></td>
   </tr>
   <tr>
    <td class="results">34</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=cfce8433e2add80ec2db134a9b8da0c023a7d254">CREStereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/megvii-research/CREStereo" target="blank">code</a></td>
     <td class="results"> 1.45 %</td>
     <td class="results"> 2.86 %</td>
     <td class="results"> 1.69 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.41 s</td>
     <td class="results">GPU @ &gt;3.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="cfce8433e2add80ec2db134a9b8da0c023a7d254" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Li, P. Wang, P. Xiong, T. Cai, Z. Yan, L. Yang, J. Liu, H. Fan and S. Liu: <a href="http://scholar.google.de/scholar?q=Practical%20Stereo%20Matching%20via%20Cascaded%20Recurrent%20Network%20with%20Adaptive%20Correlation"> Practical Stereo Matching via 
Cascaded Recurrent Network with Adaptive 
Correlation</a>. 2022.<br></td>
   </tr>
   <tr>
    <td class="results">35</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=4c94a971d27af93dfc7c1a6846dbaedbcc5bcd79">GU</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.42 %</td>
     <td class="results"> 3.05 %</td>
     <td class="results"> 1.69 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="4c94a971d27af93dfc7c1a6846dbaedbcc5bcd79" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">36</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=65cdef10134f98c2ba4bc3a8082c6e594b614bae">SCVFormer</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.34 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.70 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.09 s</td>
     <td class="results">NVIDIA RTX 3090 (PyTorch)</td>
     <td class="results"><input type="checkbox" value="65cdef10134f98c2ba4bc3a8082c6e594b614bae" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">37</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=be603dbbadc1cff30a83bdc7531dcd8ed444abef">DuMa-Net</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.40 %</td>
     <td class="results"> 3.18 %</td>
     <td class="results"> 1.70 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.38 s</td>
     <td class="results">PyTorch GPU</td>
     <td class="results"><input type="checkbox" value="be603dbbadc1cff30a83bdc7531dcd8ed444abef" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">S. Sun, R. liu and S. Sun: <a href="http://scholar.google.de/scholar?q=Range-free%20disparity%20estimation%20with%20self-adaptive%20dual-matching"> Range-free disparity estimation with self-
adaptive dual-matching</a>. IET Computer Vision .<br></td>
   </tr>
   <tr>
    <td class="results">38</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=eba38d5eac452940042e9acf4aa5a7c55bcaf865">EGA-Stereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/yuankang1234/EGA-Stereo" target="blank">code</a></td>
     <td class="results"> 1.42 %</td>
     <td class="results"> 3.12 %</td>
     <td class="results"> 1.70 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.41 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="eba38d5eac452940042e9acf4aa5a7c55bcaf865" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">39</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=0f7815ce43e0bd388dc6f6bb655db4e701f02e22">Any-RAFT</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.44 %</td>
     <td class="results"> 3.04 %</td>
     <td class="results"> 1.70 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.34 s</td>
     <td class="results">GPU @ Nvidia A40 (Python)</td>
     <td class="results"><input type="checkbox" value="0f7815ce43e0bd388dc6f6bb655db4e701f02e22" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">40</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b6ed93535e59a684e1a4563132757bcdf07373a1">SAGIF-GMM</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.52 %</td>
     <td class="results"> 2.66 %</td>
     <td class="results"> 1.71 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.37 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="b6ed93535e59a684e1a4563132757bcdf07373a1" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">41</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=675e81943dd3433b917c1638eacd3df43548da92">IEG-Net</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.39 %</td>
     <td class="results"> 3.31 %</td>
     <td class="results"> 1.71 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.40 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="675e81943dd3433b917c1638eacd3df43548da92" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">42</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ced8281b30cfe69baa3fd682c4b6f058b5c8cc39">DANet-Stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.41 %</td>
     <td class="results"> 3.26 %</td>
     <td class="results"> 1.72 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2.7 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="ced8281b30cfe69baa3fd682c4b6f058b5c8cc39" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">43</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=87d5d6fb5e776b2584c50c1ef91d5c660f89f793">DKT-IGEV</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.46 %</td>
     <td class="results"> 3.05 %</td>
     <td class="results"> 1.72 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.18 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="87d5d6fb5e776b2584c50c1ef91d5c660f89f793" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">44</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=6796eb657f1221d5977ef0a5fc8b17825cd5b01d">AFNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.36 %</td>
     <td class="results"> 3.61 %</td>
     <td class="results"> 1.73 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.25 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="6796eb657f1221d5977ef0a5fc8b17825cd5b01d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">45</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=db760db37819e75b1a181016524372e970c762cf">Patchmatch Stereo++</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://drive.google.com/drive/folders/18kgpR5BfEVjuxYmjwpL6QAccCefuKdAf?usp=drive_link" target="blank">code</a></td>
     <td class="results"> 1.55 %</td>
     <td class="results"> 2.71 %</td>
     <td class="results"> 1.74 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results"></td>
     <td class="results"><input type="checkbox" value="db760db37819e75b1a181016524372e970c762cf" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">W. Ren, Q. Liao, Z. Shao, X. Lin, X. Yue, Y. Zhang and Z. Lu: <a href="http://scholar.google.de/scholar?q=Patchmatch%20Stereo++:%20Patchmatch%20Binocular%20Stereo%20with%20Continuous%20Disparity%20Optimization"> Patchmatch Stereo++: Patchmatch Binocular 
Stereo with Continuous Disparity Optimization</a>. Proceedings of the 31st ACM 
International Conference on Multimedia 2023.<br></td>
   </tr>
   <tr>
    <td class="results">46</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=bdb26a3c3664697a7b3c7534ccebf31055462bb8">OnestageStereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/none" target="blank">code</a></td>
     <td class="results"> 1.56 %</td>
     <td class="results"> 2.62 %</td>
     <td class="results"> 1.74 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.02 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="bdb26a3c3664697a7b3c7534ccebf31055462bb8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">47</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=52326df253daabebaa26496097d5852f24e6f95d">CSPN</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.51 %</td>
     <td class="results"> 2.88 %</td>
     <td class="results"> 1.74 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.0 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="52326df253daabebaa26496097d5852f24e6f95d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">X. Cheng, P. Wang and R. Yang: <a href="http://scholar.google.de/scholar?q=Learning%20Depth%20with%20Convolutional%20Spatial%20Propagation%20Network"> Learning Depth with Convolutional Spatial 
Propagation Network</a>. IEEE Transactions on Pattern Analysis 
and Machine Intelligence(T-PAMI) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">48</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=6b1aafff68c8cb98106a1c1fdbfa08099dda62e1">NeXt-Stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.51 %</td>
     <td class="results"> 2.93 %</td>
     <td class="results"> 1.75 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.06 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="6b1aafff68c8cb98106a1c1fdbfa08099dda62e1" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">49</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5c89237f97f5048e06c70d8aa89a495964fa35c6">ProNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.11 %</td>
     <td class="results"> 1.75 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.33 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="5c89237f97f5048e06c70d8aa89a495964fa35c6" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">50</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=198d69e8eafd89f01ade1ee0e11a16dc2d91b7fd">ASNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.45 %</td>
     <td class="results"> 3.27 %</td>
     <td class="results"> 1.75 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.17 s</td>
     <td class="results">GPU @ &gt;3.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="198d69e8eafd89f01ade1ee0e11a16dc2d91b7fd" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">51</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=dd6984c34b9c371c402dae70e42bce35cf22f6e1">GMOStereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.60 %</td>
     <td class="results"> 2.54 %</td>
     <td class="results"> 1.76 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.30 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="dd6984c34b9c371c402dae70e42bce35cf22f6e1" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">52</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=13b3f8a2871c1aae1778ae5499b3388fc9a03a3d">IGEV_TEST2</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.50 %</td>
     <td class="results"> 3.06 %</td>
     <td class="results"> 1.76 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.06 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="13b3f8a2871c1aae1778ae5499b3388fc9a03a3d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">53</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=4e2734894652c8df714b66e6d5f3304761b9c811">IGEV_15</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.50 %</td>
     <td class="results"> 3.06 %</td>
     <td class="results"> 1.76 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.07 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="4e2734894652c8df714b66e6d5f3304761b9c811" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">54</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=71551f29f11514353a594cc6fa3073a4ea7aadee">IGE_Corr</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.50 %</td>
     <td class="results"> 3.06 %</td>
     <td class="results"> 1.76 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="71551f29f11514353a594cc6fa3073a4ea7aadee" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">55</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=993712156e1da6604ac1d84df44d076536918581">DLNR</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.60 %</td>
     <td class="results"> 2.59 %</td>
     <td class="results"> 1.76 %</td>
     <td class="results">100.00 %</td>
     <td class="results">~0.3 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="993712156e1da6604ac1d84df44d076536918581" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">56</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=09b222ad9e39b56ca2e16cccbbf07cbd38409b9b">LaC+GwcNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/SpadeLiu/Lac-GwcNet" target="blank">code</a></td>
     <td class="results"> 1.43 %</td>
     <td class="results"> 3.44 %</td>
     <td class="results"> 1.77 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0. 65 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="09b222ad9e39b56ca2e16cccbbf07cbd38409b9b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">B. Liu, H. Yu and Y. Long: <a href="http://scholar.google.de/scholar?q=Local%20Similarity%20Pattern%20and%20Cost%20Self-Reassembling%20for%20Deep%20Stereo%20Matching%20Networks"> Local Similarity Pattern and Cost Self-
Reassembling for Deep Stereo Matching Networks</a>. Proceedings of the AAAI Conference on 
Artificial Intelligence 2022.<br></td>
   </tr>
   <tr>
    <td class="results">57</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5c3731e7a920d043f3f10fb852dae3c7e7358541">GMStereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/autonomousvision/unimatch" target="blank">code</a></td>
     <td class="results"> 1.49 %</td>
     <td class="results"> 3.14 %</td>
     <td class="results"> 1.77 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.17 s</td>
     <td class="results">GPU (Python)</td>
     <td class="results"><input type="checkbox" value="5c3731e7a920d043f3f10fb852dae3c7e7358541" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Xu, J. Zhang, J. Cai, H. Rezatofighi, F. Yu, D. Tao and A. Geiger: <a href="http://scholar.google.de/scholar?q=Unifying%20Flow,%20Stereo%20and%20Depth%20Estimation"> Unifying Flow, Stereo and Depth 
Estimation</a>. arXiv preprint arXiv:2211.05783 2022.<br></td>
   </tr>
   <tr>
    <td class="results">58</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=d8ef1fd255b1d8dbc3209902947e8b89dc3952e6">CFNet+PPF</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.47 %</td>
     <td class="results"> 3.26 %</td>
     <td class="results"> 1.77 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.22 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="d8ef1fd255b1d8dbc3209902947e8b89dc3952e6" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">59</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=28036d6d160a9f3e37f8c80ebd74d83872cceb2b">UNI</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/none" target="blank">code</a></td>
     <td class="results"> 1.51 %</td>
     <td class="results"> 3.06 %</td>
     <td class="results"> 1.77 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="28036d6d160a9f3e37f8c80ebd74d83872cceb2b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">60</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f4feec6b2f4ef50445604eb82ebc533da6db44ed">D2Stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.58 %</td>
     <td class="results"> 2.70 %</td>
     <td class="results"> 1.77 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.25 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="f4feec6b2f4ef50445604eb82ebc533da6db44ed" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">61</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=909b92ee337cc8d4c7eeda57eb0678684191070d">NLCA-Net v2</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/Archaic-Atom/NLCA-Net_v2" target="blank">code</a></td>
     <td class="results"> 1.41 %</td>
     <td class="results"> 3.56 %</td>
     <td class="results"> 1.77 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.67 s</td>
     <td class="results">GPU @ &gt;3.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="909b92ee337cc8d4c7eeda57eb0678684191070d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Rao, D. Yuchao, S. Zhelun and H. Renjie: <a href="http://scholar.google.de/scholar?q=Rethinking%20Training%20Strategy%20in%20Stereo%20Matching"> Rethinking Training Strategy in 
Stereo Matching</a>. IEEE TRANSACTIONS ON NEURAL 
NETWORKS AND LEARNING SYSTEMS .<br></td>
   </tr>
   <tr>
    <td class="results">62</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b433fb9383fa0bb96ce9548ab4a340314469967d">GANet+DSMNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.23 %</td>
     <td class="results"> 1.77 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2.0 s</td>
     <td class="results">GPU @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="b433fb9383fa0bb96ce9548ab4a340314469967d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Zhang, X. Qi, R. Yang, V. Prisacariu, B. Wah and P. Torr: <a href="http://scholar.google.de/scholar?q=Domain-invariant%20Stereo%20Matching%20Networks"> Domain-invariant Stereo Matching 
Networks</a>. Europe Conference on Computer Vision 
(ECCV) 2020.<br></td>
   </tr>
   <tr>
    <td class="results">63</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=36c5ccceaef9170096eb80e26713d4f41f4c01c8">DVANet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.47 %</td>
     <td class="results"> 3.32 %</td>
     <td class="results"> 1.78 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.03 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="36c5ccceaef9170096eb80e26713d4f41f4c01c8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">64</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=cbb8a5069a1f6c2a1767738abc02dd27901f24d5">PFSMNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/N.A" target="blank">code</a></td>
     <td class="results"> 1.54 %</td>
     <td class="results"> 3.02 %</td>
     <td class="results"> 1.79 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.31 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="cbb8a5069a1f6c2a1767738abc02dd27901f24d5" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">K. Zeng, Y. Wang, Q. Zhu, J. Mao and H. Zhang: <a href="http://scholar.google.de/scholar?q=Deep%20Progressive%20Fusion%20Stereo%20Network"> Deep Progressive Fusion Stereo Network</a>. IEEE Transactions on Intelligent 
Transportation Systems 2021.<br></td>
   </tr>
   <tr>
    <td class="results">65</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=fd80957aef2378e0ce86d69b1bfddc0d60f84598">DIGEV</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.66 %</td>
     <td class="results"> 2.47 %</td>
     <td class="results"> 1.80 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.18 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="fd80957aef2378e0ce86d69b1bfddc0d60f84598" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">66</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=de85cdaab8bc848ce67998beba15ed55afc9dc44">SUW-Stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.47 %</td>
     <td class="results"> 3.45 %</td>
     <td class="results"> 1.80 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.8 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="de85cdaab8bc848ce67998beba15ed55afc9dc44" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Ren, A. Raj, M. El-Khamy and J. Lee: <a href="http://scholar.google.de/scholar?q=SUW-Learn:%20Joint%20Supervised,%20Unsupervised,%20Weakly%20Supervised%20Deep%20Learning%20for%20Monocular%20Depth%20Estimation"> SUW-Learn: Joint Supervised, 
Unsupervised, Weakly Supervised Deep Learning for 
Monocular Depth Estimation</a>. Proceedings of the IEEE/CVF 
Conference on Computer Vision and Pattern 
Recognition Workshops 2020.<br></td>
   </tr>
   <tr>
    <td class="results">67</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=13c6ec1235e2091f86a33de8065107df4e6f4924">TemporalStereo</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_mv.png" alt="This method makes use of multiple (&gt;2) views."></div></td>
     <td class="results"><a href="https://github.com/youmi-zym/TemporalStereo" target="blank">code</a></td>
     <td class="results"> 1.61 %</td>
     <td class="results"> 2.78 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.04 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="13c6ec1235e2091f86a33de8065107df4e6f4924" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Y. Zhang, M. Poggi and S. Mattoccia: <a href="http://scholar.google.de/scholar?q=TemporalStereo:%20Efficient%20Spatial-Temporal%20Stereo%20Matching%20Network"> TemporalStereo: Efficient 
Spatial-Temporal Stereo Matching Network</a>. IROS 2023.<br></td>
   </tr>
   <tr>
    <td class="results">68</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=02bf5ea740c5df7aa6af75194b872f2910944c09">Binary TTC</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2 s</td>
     <td class="results">GPU @ 1.0 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="02bf5ea740c5df7aa6af75194b872f2910944c09" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Badki, O. Gallo, J. Kautz and P. Sen: <a href="http://scholar.google.de/scholar?q=Binary%20TTC:%20A%20Temporal%20Geofence%20for%20Autonomous%20Navigation"> Binary TTC: A Temporal Geofence for Autonomous 
Navigation</a>. The IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR) 2021.<br></td>
   </tr>
   <tr>
    <td class="results">69</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=1294c74820e9bc5e76fb4b9b507afc904200728c">ScaleRAFT+RBO</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="1294c74820e9bc5e76fb4b9b507afc904200728c" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">70</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=53b4b18d818baa8dbef2699d507925c3481be009">CamLiRAFT</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="https://github.com/MCG-NJU/CamLiFlow" target="blank">code</a></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="53b4b18d818baa8dbef2699d507925c3481be009" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Liu, T. Lu, Y. Xu, J. Liu and L. Wang: <a href="http://scholar.google.de/scholar?q=Learning%20Optical%20Flow%20and%20Scene%20Flow%20with%20Bidirectional%20Camera-LiDAR%20Fusion"> Learning Optical Flow and Scene Flow 
with Bidirectional Camera-LiDAR Fusion</a>. arXiv preprint arXiv:2303.12017 2023.<br></td>
   </tr>
   <tr>
    <td class="results">71</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=546770b3768f416eeb0555a19504e1c0cca78b1e">ScaleRAFT</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="546770b3768f416eeb0555a19504e1c0cca78b1e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">72</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=59237fe5726a10f1ba7b31e6a799161875c30a67">Scale-flow-Nerf</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.1 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="59237fe5726a10f1ba7b31e6a799161875c30a67" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">73</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5ceb65f26b8667452a838d5949fdaa1ae9cb8748">SFG</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="5ceb65f26b8667452a838d5949fdaa1ae9cb8748" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">74</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=79f169714d0be5ea6e8bb86d99f186a6d26a5e55">Scale-flow</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.8 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="79f169714d0be5ea6e8bb86d99f186a6d26a5e55" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Ling, Q. Sun, Z. Ren, Y. Liu, H. Wang and Z. Wang: <a href="http://scholar.google.de/scholar?q=Scale-flow:%20Estimating%203D%20Motion%20from%20Video"> Scale-flow: Estimating 3D Motion from 
Video</a>. Proceedings of the 30th ACM 
International Conference on Multimedia 2022.<br></td>
   </tr>
   <tr>
    <td class="results">75</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=a3f60dbcc9e065abec2cac92428c9b9365be39a8">RAFT3D+mscv</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="a3f60dbcc9e065abec2cac92428c9b9365be39a8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">76</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b5a33a83f55fb8bf6fa60f51a900346426e0db26">CamLiRAFT-NR</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="https://github.com/MCG-NJU/CamLiFlow" target="blank">code</a></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="b5a33a83f55fb8bf6fa60f51a900346426e0db26" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Liu, T. Lu, Y. Xu, J. Liu and L. Wang: <a href="http://scholar.google.de/scholar?q=Learning%20Optical%20Flow%20and%20Scene%20Flow%20with%20Bidirectional%20Camera-LiDAR%20Fusion"> Learning Optical Flow and Scene Flow 
with Bidirectional Camera-LiDAR Fusion</a>. arXiv preprint arXiv:2303.12017 2023.<br></td>
   </tr>
   <tr>
    <td class="results">77</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b8e200e2e19e0a2fd44ab5360ed8ebd3d3c729db">RAFT3D+MSCV+ROB</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="b8e200e2e19e0a2fd44ab5360ed8ebd3d3c729db" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">78</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=bfe8dcef9ca56bfb8a0c67a5abcdae9a34e2c04e">RAFT-3D</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="bfe8dcef9ca56bfb8a0c67a5abcdae9a34e2c04e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Teed and J. Deng: <a href="http://scholar.google.de/scholar?q=RAFT-3D:%20Scene%20Flow%20using%20Rigid-Motion%20Embeddings"> RAFT-3D: Scene Flow using Rigid-Motion 
Embeddings</a>. arXiv preprint arXiv:2012.00726 2020.<br></td>
   </tr>
   <tr>
    <td class="results">79</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c58053ef4ae9717f938e87c3c8715a28994a6eb9">ScaleRAFT3D</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="c58053ef4ae9717f938e87c3c8715a28994a6eb9" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">80</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ccb2b24d3e08ec968368f85a4eeab8b668e70b8c">GANet-deep</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/feihuzhang/GANet" target="blank">code</a></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.8 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="ccb2b24d3e08ec968368f85a4eeab8b668e70b8c" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Zhang, V. Prisacariu, R. Yang and P. Torr: <a href="http://scholar.google.de/scholar?q=GA-Net:%20Guided%20Aggregation%20Net%20for%20End-to-end%20Stereo%20Matching"> GA-Net: Guided Aggregation Net for 
End-to-end Stereo Matching</a>. Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition 
(CVPR) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">81</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=cebd33cb163e9617915fe59c3d57447c6a1850cd">CamLiFlow</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="https://github.com/MCG-NJU/CamLiFlow" target="blank">code</a></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.2 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="cebd33cb163e9617915fe59c3d57447c6a1850cd" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Liu, T. Lu, Y. Xu, J. Liu, W. Li and L. Chen: <a href="http://scholar.google.de/scholar?q=CamLiFlow:%20Bidirectional%20Camera-LiDAR%20Fusion%20for%20Joint%20Optical%20Flow%20and%20Scene%20Flow%20Estimation"> CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint 
Optical Flow and Scene Flow Estimation</a>. CVPR 2022.<br></td>
   </tr>
   <tr>
    <td class="results">82</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=dbded696217f132e5722db1dafc6f2cfcc3760f0">Stereo expansion</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="http://www.contrib.andrew.cmu.edu/~gengshay/cvpr20expansion.html" target="blank">code</a></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="dbded696217f132e5722db1dafc6f2cfcc3760f0" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">G. Yang and D. Ramanan: <a href="http://scholar.google.de/scholar?q=Upgrading%20Optical%20Flow%20to%203D%20Scene%20Flow%20through%20Optical%20Expansion"> Upgrading Optical Flow to 3D Scene Flow 
through Optical Expansion</a>. CVPR 2020.<br></td>
   </tr>
   <tr>
    <td class="results">83</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=fa49d38988417846cdf4c0bf27831ddf9cd893db">RAFT-3D++</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 1.48 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.5 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="fa49d38988417846cdf4c0bf27831ddf9cd893db" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">84</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=017b19d8b1d6d5da3e53a7b597109dab6b57a15a">S3GNet-C</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.54 %</td>
     <td class="results"> 3.16 %</td>
     <td class="results"> 1.81 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.13 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="017b19d8b1d6d5da3e53a7b597109dab6b57a15a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">85</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=60693d5a91d01db6d8a6df1b8612fa4d0a488e84">FGDS-Net</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.49 %</td>
     <td class="results"> 3.46 %</td>
     <td class="results"> 1.82 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.3 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="60693d5a91d01db6d8a6df1b8612fa4d0a488e84" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">86</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=8f784a7f880a90fd5a42e4ac4fe884765e48bca7">ADStereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.53 %</td>
     <td class="results"> 3.27 %</td>
     <td class="results"> 1.82 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.05 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="8f784a7f880a90fd5a42e4ac4fe884765e48bca7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">87</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=23fa7b4d7379436560ef36601d1865ede8f16e3a">GwcNet+PPF</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.53 %</td>
     <td class="results"> 3.27 %</td>
     <td class="results"> 1.82 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.3 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="23fa7b4d7379436560ef36601d1865ede8f16e3a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">88</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=73a70bf5f9daad8ef4d84a33f3e6dab2b6595f42">TBFE-Net</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.52 %</td>
     <td class="results"> 3.36 %</td>
     <td class="results"> 1.82 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.3 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="73a70bf5f9daad8ef4d84a33f3e6dab2b6595f42" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">89</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=a364ca08cc9c0210ad035504b47d6b8614a24ab6">OptStereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.50 %</td>
     <td class="results"> 3.43 %</td>
     <td class="results"> 1.82 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.10 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="a364ca08cc9c0210ad035504b47d6b8614a24ab6" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Wang, R. Fan, P. Cai and M. Liu: <a href="http://scholar.google.de/scholar?q=PVStereo:%20Pyramid%20voting%20module%20for%20end-to-end%20self-supervised%20stereo%20matching"> PVStereo: Pyramid voting module for 
end-to-end self-supervised stereo matching</a>. IEEE Robotics and Automation Letters 2021.<br></td>
   </tr>
   <tr>
    <td class="results">90</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=3dda3a8d1c7eda1d0746ef49d0ef6ad269a007b8">RAFT-Stereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/princeton-vl/RAFT-Stereo" target="blank">code</a></td>
     <td class="results"> 1.58 %</td>
     <td class="results"> 3.05 %</td>
     <td class="results"> 1.82 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.38 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="3dda3a8d1c7eda1d0746ef49d0ef6ad269a007b8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">91</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=12a40c1a4a15520f0ea76936f8f9b7d348c3e9ef">RCGSNP</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.56 %</td>
     <td class="results"> 3.17 %</td>
     <td class="results"> 1.83 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.12 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="12a40c1a4a15520f0ea76936f8f9b7d348c3e9ef" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">92</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=e0af9e25357bb24d06c9551ebc48cfd52887c185">URDAD_1</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.53 %</td>
     <td class="results"> 3.34 %</td>
     <td class="results"> 1.83 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.35 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="e0af9e25357bb24d06c9551ebc48cfd52887c185" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">93</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=8723722d0b5d248524c5a99730822f0b2bf4ab6e">LoS_RVC</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.58 %</td>
     <td class="results"> 3.08 %</td>
     <td class="results"> 1.83 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.19 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="8723722d0b5d248524c5a99730822f0b2bf4ab6e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">94</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=aa3594629ad3816549b109d5c5dc91dc4c644297">NLCA-Net-3</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/NPU-IAP/NLCA-Net" target="blank">code</a></td>
     <td class="results"> 1.45 %</td>
     <td class="results"> 3.78 %</td>
     <td class="results"> 1.83 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.44 s</td>
     <td class="results">&gt;8 cores @ 3.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="aa3594629ad3816549b109d5c5dc91dc4c644297" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Rao, M. He, Y. Dai, Z. Zhu, B. Li and R. He: <a href="http://scholar.google.de/scholar?q=NLCA-Net:%20a%20non-local%20context%20attention%20network%20for%20stereo%20matching"> NLCA-Net: a non-local context attention 
network for stereo matching</a>. APSIPA Transactions on Signal and 
Information Processing 2020.<br></td>
   </tr>
   <tr>
    <td class="results">95</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=887df8a05c541ece9acf3f922b7556575fcb5a81">GOAT</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.71 %</td>
     <td class="results"> 2.51 %</td>
     <td class="results"> 1.84 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.29 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="887df8a05c541ece9acf3f922b7556575fcb5a81" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">96</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=a0e8ba8b4dfbfa92f6208aa5eb4938cf2954ec13">OAGNet18</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.71 %</td>
     <td class="results"> 2.51 %</td>
     <td class="results"> 1.84 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.4 s</td>
     <td class="results">GPU @ 3.0 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="a0e8ba8b4dfbfa92f6208aa5eb4938cf2954ec13" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">97</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=13f8ed9d79560e26bfe7e71f1a744ce563b9d944">AMNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.53 %</td>
     <td class="results"> 3.43 %</td>
     <td class="results"> 1.84 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.9 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="13f8ed9d79560e26bfe7e71f1a744ce563b9d944" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">X. Du, M. El-Khamy and J. Lee: <a href="http://scholar.google.de/scholar?q=AMNet:%20Deep%20Atrous%20Multiscale%20Stereo%20Disparity%20Estimation%20Networks"> AMNet: Deep Atrous Multiscale Stereo 
Disparity Estimation Networks</a>. 2019.<br></td>
   </tr>
   <tr>
    <td class="results">98</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=221a6e12f019ebf9a0323b5f5d8384f5943f9c16">FAPEEM</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.61 %</td>
     <td class="results"> 3.08 %</td>
     <td class="results"> 1.85 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.35 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="221a6e12f019ebf9a0323b5f5d8384f5943f9c16" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">99</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=6f5e4fa68bde3a1a435fca9e683c84028c88f148">UCFNet_RVC</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/gallenszl/UCFNet" target="blank">code</a></td>
     <td class="results"> 1.57 %</td>
     <td class="results"> 3.33 %</td>
     <td class="results"> 1.86 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.21 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="6f5e4fa68bde3a1a435fca9e683c84028c88f148" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Shen, X. Song, Y. Dai, D. Zhou, Z. Rao and L. Zhang: <a href="http://scholar.google.de/scholar?q=Digging%20Into%20Uncertainty-Based%20Pseudo-Label%20for%20Robust%20Stereo%20Matching"> Digging Into Uncertainty-Based Pseudo-
Label for Robust Stereo Matching</a>. IEEE Transactions on Pattern Analysis 
and Machine Intelligence 2023.<br></td>
   </tr>
   <tr>
    <td class="results">100</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=76c2d68b0bc701667a7532523d0edf8092c4ffc3">PSMNet+</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.51 %</td>
     <td class="results"> 3.60 %</td>
     <td class="results"> 1.86 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.41 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="76c2d68b0bc701667a7532523d0edf8092c4ffc3" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">101</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=bc2617e4c3257e906291617e0008b00178e35337">URDAD</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.54 %</td>
     <td class="results"> 3.54 %</td>
     <td class="results"> 1.87 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.35 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="bc2617e4c3257e906291617e0008b00178e35337" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">102</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=cc940ad106d6758ed5e72fde5fb19e0bf3952026">MAF-Stereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/LeiJ-USTB/MAF-Stereo/tree/main" target="blank">code</a></td>
     <td class="results"> 1.62 %</td>
     <td class="results"> 3.15 %</td>
     <td class="results"> 1.87 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.07 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="cc940ad106d6758ed5e72fde5fb19e0bf3952026" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">103</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b4503cdad7286e2123f831f70b93cc6c92278aa7">EAC-Stereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/yuankang1234/eac_stereo" target="blank">code</a></td>
     <td class="results"> 1.52 %</td>
     <td class="results"> 3.68 %</td>
     <td class="results"> 1.88 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.38 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="b4503cdad7286e2123f831f70b93cc6c92278aa7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">104</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=a725098dcab07bdb9f9fea5aced4bafa4e570bc5">CFNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/gallenszl/CFNet" target="blank">code</a></td>
     <td class="results"> 1.54 %</td>
     <td class="results"> 3.56 %</td>
     <td class="results"> 1.88 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.18 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="a725098dcab07bdb9f9fea5aced4bafa4e570bc5" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Shen, Y. Dai and Z. Rao: <a href="http://scholar.google.de/scholar?q=CFNet:%20Cascade%20and%20Fused%20Cost%20Volume%20for%20Robust%20Stereo%20Matching"> CFNet: Cascade and Fused Cost Volume for 
Robust Stereo Matching</a>. IEEE Conference on Computer Vision 
and 
Pattern Recognition (CVPR) 2021.<br>Z. Shen, X. Song, Y. Dai, D. Zhou, Z. Rao and L. Zhang: <a href="http://scholar.google.de/scholar?q=Digging%20Into%20Uncertainty-Based%20Pseudo-Label%20for%20Robust%20Stereo%20Matching"> Digging Into Uncertainty-Based Pseudo-
Label for Robust Stereo Matching</a>. IEEE Transactions on Pattern Analysis 
and Machine Intelligence 2023.<br></td>
   </tr>
   <tr>
    <td class="results">105</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=606f0b9e284e8a3b3147aefc3ef5f3144a81bf66">non-parametric</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.56 %</td>
     <td class="results"> 3.49 %</td>
     <td class="results"> 1.88 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.34 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="606f0b9e284e8a3b3147aefc3ef5f3144a81bf66" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">106</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=662315b70a95c62b67a6e625eb727a646b06c375">RigidMask+ISF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="https://github.com/gengshan-y/rigidmask" target="blank">code</a></td>
     <td class="results"> 1.53 %</td>
     <td class="results"> 3.65 %</td>
     <td class="results"> 1.89 %</td>
     <td class="results">100.00 %</td>
     <td class="results">3.3 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="662315b70a95c62b67a6e625eb727a646b06c375" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">G. Yang and D. Ramanan: <a href="http://scholar.google.de/scholar?q=Learning%20to%20Segment%20Rigid%20Motions%20from%20Two%20Frames"> Learning to Segment Rigid Motions from Two 
Frames</a>. CVPR 2021.<br></td>
   </tr>
   <tr>
    <td class="results">107</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5cd458364e5aa4bd17d754cee0f7d34734807a64">AcfNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/DeepMotionAIResearch/DenseMatchingBenchmark" target="blank">code</a></td>
     <td class="results"> 1.51 %</td>
     <td class="results"> 3.80 %</td>
     <td class="results"> 1.89 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.48 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="5cd458364e5aa4bd17d754cee0f7d34734807a64" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Y. Zhang, Y. Chen, X. Bai, S. Yu, K. Yu, Z. Li and K. Yang: <a href="http://scholar.google.de/scholar?q=Adaptive%20Unimodal%20Cost%20Volume%20Filtering%20for%20Deep%20Stereo%20Matching"> Adaptive Unimodal Cost Volume Filtering for Deep 
Stereo Matching</a>. AAAI 2020.<br></td>
   </tr>
   <tr>
    <td class="results">108</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=49086613fa6b09d03096d813254d83ee3264ccf3">TemporalCoEx</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.71 %</td>
     <td class="results"> 2.78 %</td>
     <td class="results"> 1.89 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.04 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="49086613fa6b09d03096d813254d83ee3264ccf3" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">109</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=0a865a56577435057ab4cdc224407b9c49bd8ce0">PSMNet+PPF</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.55 %</td>
     <td class="results"> 3.63 %</td>
     <td class="results"> 1.89 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.35 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="0a865a56577435057ab4cdc224407b9c49bd8ce0" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">110</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ea576199f9403d6e1b8cb137f855f14f033b3e62">EAC-Stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.52 %</td>
     <td class="results"> 3.87 %</td>
     <td class="results"> 1.91 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.38 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="ea576199f9403d6e1b8cb137f855f14f033b3e62" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">111</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=51dd994f9a96d58175dad07999a7bed287ce345b">Fast-ACV+</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/gangweiX/Fast-ACVNet" target="blank">code</a></td>
     <td class="results"> 1.68 %</td>
     <td class="results"> 3.06 %</td>
     <td class="results"> 1.91 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.04s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="51dd994f9a96d58175dad07999a7bed287ce345b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">112</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=d1d3d2c553caa6d01e864a833d8d3b7cd755ffbb">NLCA_NET_v2_RVC</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.51 %</td>
     <td class="results"> 3.97 %</td>
     <td class="results"> 1.92 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.67 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="d1d3d2c553caa6d01e864a833d8d3b7cd755ffbb" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Rao, M. He, Y. Dai, Z. Zhu, B. Li and R. He: <a href="http://scholar.google.de/scholar?q=NLCA-Net:%20a%20non-local%20context%20attention%20network%20for%20stereo%20matching"> NLCA-Net: a non-local context attention 
network for stereo matching</a>. APSIPA Transactions on Signal and 
Information Processing 2020.<br></td>
   </tr>
   <tr>
    <td class="results">113</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=e0d0707f93bd0d59f5dc3dd8f838560e54ff12a5">CDN</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/CDN" target="blank">code</a></td>
     <td class="results"> 1.66 %</td>
     <td class="results"> 3.20 %</td>
     <td class="results"> 1.92 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.4 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="e0d0707f93bd0d59f5dc3dd8f838560e54ff12a5" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">D. Garg, Y. Wang, B. Hariharan, M. Campbell, K. Weinberger and W. Chao: <a href="http://scholar.google.de/scholar?q=Wasserstein%20Distances%20for%20Stereo%20Disparity%20Estimation"> Wasserstein Distances for Stereo Disparity 
Estimation</a>. Advances in Neural Information 
Processing Systems 2020.<br></td>
   </tr>
   <tr>
    <td class="results">114</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f51ca7c8cf96931e1482168434e188fa928961e3">Abc-Net</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.47 %</td>
     <td class="results"> 4.20 %</td>
     <td class="results"> 1.92 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.83 s</td>
     <td class="results">4 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="f51ca7c8cf96931e1482168434e188fa928961e3" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">X. Li, Y. Fan, G. Lv and H. Ma: <a href="http://scholar.google.de/scholar?q=Area-based%20correlation%20and%20non-local%20attention%20network%20for%20stereo%20matching"> Area-based correlation and non-local 
attention network for stereo matching</a>. The Visual Computer 2021.<br></td>
   </tr>
   <tr>
    <td class="results">115</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=59cfbc4149e979b63b961f9daa3aa2bae021eff3">GANet-15</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/feihuzhang/GANet" target="blank">code</a></td>
     <td class="results"> 1.55 %</td>
     <td class="results"> 3.82 %</td>
     <td class="results"> 1.93 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.36 s</td>
     <td class="results">GPU (Pytorch)</td>
     <td class="results"><input type="checkbox" value="59cfbc4149e979b63b961f9daa3aa2bae021eff3" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Zhang, V. Prisacariu, R. Yang and P. Torr: <a href="http://scholar.google.de/scholar?q=GA-Net:%20Guided%20Aggregation%20Net%20for%20End-to-end%20Stereo%20Matching"> GA-Net: Guided Aggregation Net for 
End-to-end Stereo Matching</a>. Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition 
(CVPR) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">116</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=2ce07e3fe7cc32da7d8ae2fd8dcd878e0340549d">PCVNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.68 %</td>
     <td class="results"> 3.19 %</td>
     <td class="results"> 1.93 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.05 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="2ce07e3fe7cc32da7d8ae2fd8dcd878e0340549d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">117</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=9adb123c0300b315088f3c48100c2a108c32c95a">MDCTest</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/MDCTest" target="blank">code</a></td>
     <td class="results"> 1.65 %</td>
     <td class="results"> 3.37 %</td>
     <td class="results"> 1.94 %</td>
     <td class="results">100.00 %</td>
     <td class="results">MDCT s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="9adb123c0300b315088f3c48100c2a108c32c95a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">118</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=747355a786da295d6997afa70c67f890c391afec">CAL-Net</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.59 %</td>
     <td class="results"> 3.76 %</td>
     <td class="results"> 1.95 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.44 s</td>
     <td class="results">2 cores @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="747355a786da295d6997afa70c67f890c391afec" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">S. Chen, B. Li, W. Wang, H. Zhang, H. Li and Z. Wang: <a href="http://scholar.google.de/scholar?q=Cost%20Affinity%20Learning%20Network%20for%20Stereo%20Matching"> Cost Affinity Learning Network for 
Stereo Matching</a>. IEEE International Conference on 
Acoustics, Speech and Signal Processing,
               ICASSP 2021, Toronto, ON, Canada, 
June 6-11, 2021 2021.<br></td>
   </tr>
   <tr>
    <td class="results">119</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=1439711046db1242e2fa422ad99706a7ef9602d7">SASNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.61 %</td>
     <td class="results"> 3.65 %</td>
     <td class="results"> 1.95 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.21 s</td>
     <td class="results">GPU @ &gt;3.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="1439711046db1242e2fa422ad99706a7ef9602d7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">120</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=8760719f300ecb31c23ec4c41ff1b586bbb02eb9">NLCA-Net</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/NPU-IAP/NLCA-Net" target="blank">code</a></td>
     <td class="results"> 1.53 %</td>
     <td class="results"> 4.09 %</td>
     <td class="results"> 1.96 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.6 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="8760719f300ecb31c23ec4c41ff1b586bbb02eb9" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Rao, M. He, Y. Dai, Z. Zhu, B. Li and R. He: <a href="http://scholar.google.de/scholar?q=NLCA-Net:%20a%20non-local%20context%20attention%20network%20for%20stereo%20matching"> NLCA-Net: a non-local context attention 
network for stereo matching</a>. APSIPA Transactions on Signal and 
Information Processing 2020.<br></td>
   </tr>
   <tr>
    <td class="results">121</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=37b5d5950834203ec80e4f6628b96f43fee38dff">CFNet_RVC</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/gallenszl/CFNet" target="blank">code</a></td>
     <td class="results"> 1.65 %</td>
     <td class="results"> 3.53 %</td>
     <td class="results"> 1.96 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.22 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="37b5d5950834203ec80e4f6628b96f43fee38dff" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Shen, Y. Dai and Z. Rao: <a href="http://scholar.google.de/scholar?q=CFNet:%20Cascade%20and%20Fused%20Cost%20Volume%20for%20Robust%20Stereo%20Matching"> CFNet: Cascade and Fused Cost Volume for 
Robust Stereo Matching</a>. IEEE Conference on Computer Vision 
and 
Pattern Recognition (CVPR) 2021.<br>Z. Shen, X. Song, Y. Dai, D. Zhou, Z. Rao and L. Zhang: <a href="http://scholar.google.de/scholar?q=Digging%20Into%20Uncertainty-Based%20Pseudo-Label%20for%20Robust%20Stereo%20Matching"> Digging Into Uncertainty-Based Pseudo-
Label for Robust Stereo Matching</a>. IEEE Transactions on Pattern Analysis 
and Machine Intelligence 2023.<br></td>
   </tr>
   <tr>
    <td class="results">122</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c44979e3f922333f5cb2f3ce13cf83e2e780b727">High_U+A_coex</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.63 %</td>
     <td class="results"> 3.60 %</td>
     <td class="results"> 1.96 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.35 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="c44979e3f922333f5cb2f3ce13cf83e2e780b727" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">ERROR: Wrong syntax in BIBTEX file.<br></td>
   </tr>
   <tr>
    <td class="results">123</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f8a8beb7937949f4191b211a0ee449c8d2d63434">PGNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.64 %</td>
     <td class="results"> 3.60 %</td>
     <td class="results"> 1.96 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.7 s</td>
     <td class="results">1 core @ 2.5 Ghz (python)</td>
     <td class="results"><input type="checkbox" value="f8a8beb7937949f4191b211a0ee449c8d2d63434" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">S. Chen, Z. Xiang, C. Qiao, Y. Chen and T. Bai: <a href="http://scholar.google.de/scholar?q=PGNet:%20Panoptic%20parsing%20guided%20deep%20stereo%20matching"> PGNet: Panoptic parsing guided deep stereo 
matching</a>. Neurocomputing 2021.<br></td>
   </tr>
   <tr>
    <td class="results">124</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=e97b714d7ba96999dbeeb476aca05681cd986b37">GCGANet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.65 %</td>
     <td class="results"> 3.59 %</td>
     <td class="results"> 1.97 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.15 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="e97b714d7ba96999dbeeb476aca05681cd986b37" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">125</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=9e50caab0c3bd053791edb0043d3f7fbd698977f">DMCNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.49 %</td>
     <td class="results"> 4.40 %</td>
     <td class="results"> 1.97 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.27 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="9e50caab0c3bd053791edb0043d3f7fbd698977f" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">126</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=a4dd1420247d32cc2ffad69474b77986a2cfcc5e">HITNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://arxiv.org/abs/2007.12140" target="blank">code</a></td>
     <td class="results"> 1.74 %</td>
     <td class="results"> 3.20 %</td>
     <td class="results"> 1.98 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.02 s</td>
     <td class="results">GPU @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="a4dd1420247d32cc2ffad69474b77986a2cfcc5e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">V. Tankovich, C. Häne, Y. Zhang, A. Kowdle, S. Fanello and S. Bouaziz: <a href="http://scholar.google.de/scholar?q=HITNet:%20Hierarchical%20Iterative%20Tile%20Refinement%20Network%20for%20Real-time%20Stereo%20Matching"> HITNet: Hierarchical Iterative Tile 
Refinement Network for Real-time Stereo 
Matching</a>. CVPR 2021.<br></td>
   </tr>
   <tr>
    <td class="results">127</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=33e051fbb25c29dbe841d4604a820c72158f854f">SGNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.63 %</td>
     <td class="results"> 3.76 %</td>
     <td class="results"> 1.99 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.6 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="33e051fbb25c29dbe841d4604a820c72158f854f" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">S. Chen, Z. Xiang, C. Qiao, Y. Chen and T. Bai: <a href="http://scholar.google.de/scholar?q=SGNet:%20Semantics%20Guided%20Deep%20Stereo%20Matching"> SGNet: Semantics Guided Deep Stereo 
Matching</a>. Proceedings of the Asian Conference 
on Computer Vision (ACCV) 2020.<br></td>
   </tr>
   <tr>
    <td class="results">128</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c5af3a1639523ee318d2d783f98babe2318ec471">GEMA-Stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.66 %</td>
     <td class="results"> 3.65 %</td>
     <td class="results"> 1.99 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.03 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="c5af3a1639523ee318d2d783f98babe2318ec471" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">129</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ee16414592166b940061e2bf9594e11b08046751">ICGNet-gwc</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.62 %</td>
     <td class="results"> 3.90 %</td>
     <td class="results"> 2.00 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.15 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="ee16414592166b940061e2bf9594e11b08046751" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">130</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=de67e0bd2cedf34e2b5090bb0bb565bfc3c6919d">CSN</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/alibaba/cascade-stereo" target="blank">code</a></td>
     <td class="results"> 1.59 %</td>
     <td class="results"> 4.03 %</td>
     <td class="results"> 2.00 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.6 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="de67e0bd2cedf34e2b5090bb0bb565bfc3c6919d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">X. Gu, Z. Fan, S. Zhu, Z. Dai, F. Tan and P. Tan: <a href="http://scholar.google.de/scholar?q=Cascade%20cost%20volume%20for%20high-resolution%20multi-view%20stereo%20and%20stereo%20matching"> Cascade cost volume for high-resolution 
multi-view stereo and stereo matching</a>. Proceedings of the IEEE/CVF 
Conference on Computer Vision and Pattern 
Recognition 2020.<br></td>
   </tr>
   <tr>
    <td class="results">131</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f2962fb1b92b333f58169fd311cf28a52996e380">PCMAnet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/Anonymous" target="blank">code</a></td>
     <td class="results"> 1.71 %</td>
     <td class="results"> 3.55 %</td>
     <td class="results"> 2.02 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.27 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="f2962fb1b92b333f58169fd311cf28a52996e380" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">132</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c20ac5ce060f549320da9e6b87e54f31a02369d2">CoEx</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/antabangun/coex" target="blank">code</a></td>
     <td class="results"> 1.74 %</td>
     <td class="results"> 3.41 %</td>
     <td class="results"> 2.02 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.027 s</td>
     <td class="results">GPU RTX 2080Ti (Python)</td>
     <td class="results"><input type="checkbox" value="c20ac5ce060f549320da9e6b87e54f31a02369d2" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Bangunharcana, J. Cho, S. Lee, I. Kweon, K. Kim and S. Kim: <a href="http://scholar.google.de/scholar?q=Correlate-and-Excite:%20Real-Time%20Stereo%20Matching%20via%20Guided%20Cost%20Volume%20Excitation"> Correlate-and-Excite: Real-Time Stereo Matching via Guided Cost Volume Excitation</a>. 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2021.<br></td>
   </tr>
   <tr>
    <td class="results">133</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=32f4ff0fc8c4b2d9ca5eabb556cfa92b978a7b18">HD^3-Stereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/ucbdrive/hd3" target="blank">code</a></td>
     <td class="results"> 1.70 %</td>
     <td class="results"> 3.63 %</td>
     <td class="results"> 2.02 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.14 s</td>
     <td class="results">NVIDIA Pascal Titan XP</td>
     <td class="results"><input type="checkbox" value="32f4ff0fc8c4b2d9ca5eabb556cfa92b978a7b18" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Yin, T. Darrell and F. Yu: <a href="http://scholar.google.de/scholar?q=Hierarchical%20Discrete%20Distribution%20Decomposition%20for%20Match%20Density%20Estimation"> Hierarchical Discrete Distribution Decomposition 
for Match Density Estimation</a>. CVPR 2019.<br></td>
   </tr>
   <tr>
    <td class="results">134</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=14ffb891b891055751da855ac0360d53b2458d49">GFGANet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.62 %</td>
     <td class="results"> 4.04 %</td>
     <td class="results"> 2.02 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.39 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="14ffb891b891055751da855ac0360d53b2458d49" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">135</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f815524bff7b3b6e02676a8e2b26f3a46fd8e41a">SCV-Stereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://sites.google.com/view/scv-stereo" target="blank">code</a></td>
     <td class="results"> 1.67 %</td>
     <td class="results"> 3.78 %</td>
     <td class="results"> 2.02 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.08 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="f815524bff7b3b6e02676a8e2b26f3a46fd8e41a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Wang, R. Fan and M. Liu: <a href="http://scholar.google.de/scholar?q=SCV-Stereo:%20Learning%20stereo%20matching%20from%20a%20sparse%20cost%20volume"> SCV-Stereo: Learning stereo 
matching from a sparse cost volume</a>. 2021 IEEE International Conference 
on Image Processing (ICIP) 2021.<br></td>
   </tr>
   <tr>
    <td class="results">136</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=0d2f5efd0133e6fcf0fdd8efbeeac8edb6f79c0e">GDANet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.61 %</td>
     <td class="results"> 4.08 %</td>
     <td class="results"> 2.02 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.04 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="0d2f5efd0133e6fcf0fdd8efbeeac8edb6f79c0e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">137</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=25b0a1a20a5791ba19ff3412acc0d49aa48a120b">AANet+</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/haofeixu/aanet" target="blank">code</a></td>
     <td class="results"> 1.65 %</td>
     <td class="results"> 3.96 %</td>
     <td class="results"> 2.03 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.06 s</td>
     <td class="results">NVIDIA V100 GPU</td>
     <td class="results"><input type="checkbox" value="25b0a1a20a5791ba19ff3412acc0d49aa48a120b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Xu and J. Zhang: <a href="http://scholar.google.de/scholar?q=AANet:%20Adaptive%20Aggregation%20Networkfor%20Efficient%20Stereo%20Matching"> AANet: Adaptive Aggregation Network
for Efficient Stereo Matching</a>. CVPR 2020.<br></td>
   </tr>
   <tr>
    <td class="results">138</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=948e2837c0c0dd02be058a3ad7ccc1e107a7ebb1">OB_GWC</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.59 %</td>
     <td class="results"> 4.32 %</td>
     <td class="results"> 2.04 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.35 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="948e2837c0c0dd02be058a3ad7ccc1e107a7ebb1" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">139</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f073f7702985b5e0a8ebc391deaeae3148d8684e">ED-Net</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.71 %</td>
     <td class="results"> 3.80 %</td>
     <td class="results"> 2.05 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="f073f7702985b5e0a8ebc391deaeae3148d8684e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">140</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=84ebc811a273ff45c06d93082769dbcb142e896c">OGMNet_WO_GP_SA</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.76 %</td>
     <td class="results"> 3.55 %</td>
     <td class="results"> 2.06 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.4 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="84ebc811a273ff45c06d93082769dbcb142e896c" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">141</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f758ae2180d7aa16c919c0e4e0edbcebc554bf13">LR-PSMNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/WeiQin-C/LongRange-Stereo" target="blank">code</a></td>
     <td class="results"> 1.65 %</td>
     <td class="results"> 4.13 %</td>
     <td class="results"> 2.06 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.5 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="f758ae2180d7aa16c919c0e4e0edbcebc554bf13" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">W. Chuah, R. Tennakoon, R. Hoseinnezhad, A. Bab-Hadiashar and D. Suter: <a href="http://scholar.google.de/scholar?q=Adjusting%20Bias%20in%20Long%20Range%20Stereo%20Matching:%20A%20semantics%20guided%20approach"> Adjusting Bias in Long Range Stereo 
Matching: A semantics guided approach</a>. 2020.<br></td>
   </tr>
   <tr>
    <td class="results">142</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=1c68974184c2f900fa0c56b356f5757648579c0a">OB_COEX_1</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.77 %</td>
     <td class="results"> 3.56 %</td>
     <td class="results"> 2.07 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.35 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="1c68974184c2f900fa0c56b356f5757648579c0a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">143</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c83a766c9068b67b32fe671ec47bda68aa88e19b">iRaftStereo_RVC</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.88 %</td>
     <td class="results"> 3.03 %</td>
     <td class="results"> 2.07 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.5 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="c83a766c9068b67b32fe671ec47bda68aa88e19b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Jiang, R. Xu and W. Jiang: <a href="http://scholar.google.de/scholar?q=An%20Improved%20RaftStereo%20Trained%20with%20A%20Mixed%20Dataset%20for%20the%20Robust%20Vision%20Challenge%202022"> An Improved RaftStereo Trained with A 
Mixed Dataset for the Robust Vision Challenge 
2022</a>. arXiv preprint arXiv:2210.12785 2022.<br></td>
   </tr>
   <tr>
    <td class="results">144</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=703ff102a4b7b7ae251c92709cb78c4e332a77ea">PSM + SMD-Nets</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/fabiotosi92/SMD-Nets" target="blank">code</a></td>
     <td class="results"> 1.69 %</td>
     <td class="results"> 4.01 %</td>
     <td class="results"> 2.08 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.41 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="703ff102a4b7b7ae251c92709cb78c4e332a77ea" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Tosi, Y. Liao, C. Schmitt and A. Geiger: <a href="http://scholar.google.de/scholar?q=SMD-Nets:%20Stereo%20Mixture%20Density%20Networks"> SMD-Nets: Stereo Mixture Density Networks</a>. Conference on Computer Vision and Pattern Recognition (CVPR) 2021.<br></td>
   </tr>
   <tr>
    <td class="results">145</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=2666756f13541f3cff2b6f8da3a5de7117f3ec3c">MDCNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.76 %</td>
     <td class="results"> 3.68 %</td>
     <td class="results"> 2.08 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.05 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="2666756f13541f3cff2b6f8da3a5de7117f3ec3c" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">W. Chen, X. Jia, M. Wu and Z. Liang: <a href="http://scholar.google.de/scholar?q=Multi-Dimensional%20Cooperative%20Network%20for%20Stereo%20Matching"> Multi-Dimensional Cooperative Network for 
Stereo Matching</a>. IEEE Robotics and Automation Letters 2022.<br></td>
   </tr>
   <tr>
    <td class="results">146</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=01128c4c1575230e203e4e788d23df91933d7960">EdgeStereo-V2</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.84 %</td>
     <td class="results"> 3.30 %</td>
     <td class="results"> 2.08 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.32s</td>
     <td class="results">Nvidia GTX Titan Xp</td>
     <td class="results"><input type="checkbox" value="01128c4c1575230e203e4e788d23df91933d7960" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">X. Song, X. Zhao, L. Fang, H. Hu and Y. Yu: <a href="http://scholar.google.de/scholar?q=Edgestereo:%20An%20effective%20multi-task%20learning%20network%20for%20stereo%20matching%20and%20edge%20detection"> Edgestereo: An effective multi-task 
learning network for stereo matching and edge 
detection</a>. International Journal of Computer 
Vision (IJCV) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">147</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=a1fd814aa8b2353df689233fb00bdaa227f380a8">3D-MSNet / MSNet3D </a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/cogsys-tuebingen/mobilestereonet" target="blank">code</a></td>
     <td class="results"> 1.75 %</td>
     <td class="results"> 3.87 %</td>
     <td class="results"> 2.10 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.5s</td>
     <td class="results">Python,1080Ti</td>
     <td class="results"><input type="checkbox" value="a1fd814aa8b2353df689233fb00bdaa227f380a8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Shamsafar, S. Woerz, R. Rahim and A. Zell: <a href="http://scholar.google.de/scholar?q=MobileStereoNet:%20Towards%20Lightweight%20Deep%20Networks%20for%20Stereo%20Matching"> MobileStereoNet: Towards Lightweight Deep 
Networks for Stereo Matching</a>. Proceedings of the IEEE/CVF Winter 
Conference on Applications of Computer Vision 2022.<br></td>
   </tr>
   <tr>
    <td class="results">148</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=7153dee92b5f6b7e60f2153dcd7794577c7a8b9e">GwcNet-g</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/xy-guo/GwcNet" target="blank">code</a></td>
     <td class="results"> 1.74 %</td>
     <td class="results"> 3.93 %</td>
     <td class="results"> 2.11 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.32 s</td>
     <td class="results">GPU @ 2.0 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="7153dee92b5f6b7e60f2153dcd7794577c7a8b9e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">X. Guo, K. Yang, W. Yang, X. Wang and H. Li: <a href="http://scholar.google.de/scholar?q=Group-wise%20correlation%20stereo%20network"> Group-wise correlation stereo network</a>. CVPR 2019.<br></td>
   </tr>
   <tr>
    <td class="results">149</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=71fd024923ba6974691b53435313094dbde5600b">SSPCVNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.75 %</td>
     <td class="results"> 3.89 %</td>
     <td class="results"> 2.11 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.9 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="71fd024923ba6974691b53435313094dbde5600b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Wu, X. Wu, X. Zhang, S. Wang and L. Ju: <a href="http://scholar.google.de/scholar?q=Semantic%20Stereo%20Matching%20With%20Pyramid%20CostVolumes"> Semantic Stereo Matching With Pyramid Cost
Volumes</a>. The IEEE International Conference on
Computer Vision (ICCV) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">150</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=99fbafad9abc1c2f2cb34398c722264fd5941e14">WSMCnet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/wyf2017/WSMCnet" target="blank">code</a></td>
     <td class="results"> 1.72 %</td>
     <td class="results"> 4.19 %</td>
     <td class="results"> 2.13 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.39s</td>
     <td class="results">Nvidia GTX 1070  (Pytorch)</td>
     <td class="results"><input type="checkbox" value="99fbafad9abc1c2f2cb34398c722264fd5941e14" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Y. Wang, H. Wang, G. Yu, M. Yang, Y. Yuan and J. Quan: <a href="http://scholar.google.de/scholar?q=Stereo%20Matching%20Algorithm%20Based%20on%20Three-Dimensional%20Convolutional%20Neural%20Network"> Stereo Matching Algorithm Based on Three-Dimensional Convolutional Neural Network</a>. Acta Optica Sinica 2019.<br></td>
   </tr>
   <tr>
    <td class="results">151</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=9440aed8288267366a719616af48c042019f97c7">HSM-1.8x</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/gengshan-y/high-res-stereo" target="blank">code</a></td>
     <td class="results"> 1.80 %</td>
     <td class="results"> 3.85 %</td>
     <td class="results"> 2.14 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.14 s</td>
     <td class="results">Titan X Pascal</td>
     <td class="results"><input type="checkbox" value="9440aed8288267366a719616af48c042019f97c7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">G. Yang, J. Manela, M. Happold and D. Ramanan: <a href="http://scholar.google.de/scholar?q=Hierarchical%20Deep%20Stereo%20Matching%20on%20High-Resolution%20Images"> Hierarchical Deep Stereo Matching on High-
Resolution Images</a>. The IEEE Conference on Computer Vision 
and Pattern Recognition (CVPR) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">152</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c8972e144fae42dd1071091b39e5437f43753269">DeepPruner (best)</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/uber-research/DeepPruner/" target="blank">code</a></td>
     <td class="results"> 1.87 %</td>
     <td class="results"> 3.56 %</td>
     <td class="results"> 2.15 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.18 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="c8972e144fae42dd1071091b39e5437f43753269" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">S. Duggal, S. Wang, W. Ma, R. Hu and R. Urtasun: <a href="http://scholar.google.de/scholar?q=DeepPruner:%20Learning%20Efficient%20Stereo%20Matching%20via%20Differentiable%20PatchMatch"> DeepPruner: Learning Efficient Stereo Matching 
via Differentiable PatchMatch</a>. ICCV 2019.<br></td>
   </tr>
   <tr>
    <td class="results">153</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=866d2307c14dc9a04399f1dfa8b1ecd4dc3e36d6">W-Stereo-a-r</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.70 %</td>
     <td class="results"> 4.48 %</td>
     <td class="results"> 2.16 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.07 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="866d2307c14dc9a04399f1dfa8b1ecd4dc3e36d6" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">154</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ff553f32b19afc42a84726bd9840aa4dfdb90f80">CGF-F-B</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.75 %</td>
     <td class="results"> 4.20 %</td>
     <td class="results"> 2.16 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.26 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="ff553f32b19afc42a84726bd9840aa4dfdb90f80" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">155</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=d461cb56be546ee4707aaf25cbff19488af96b14">PMS++_Fast</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.93 %</td>
     <td class="results"> 3.30 %</td>
     <td class="results"> 2.16 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.40 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="d461cb56be546ee4707aaf25cbff19488af96b14" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">156</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=eaff6bea11d84e75996e8fdb983bbe6cc51f5139">Stereo-fusion-SJTU</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.87 %</td>
     <td class="results"> 3.61 %</td>
     <td class="results"> 2.16 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.7 s</td>
     <td class="results">Nvidia GTX Titan Xp</td>
     <td class="results"><input type="checkbox" value="eaff6bea11d84e75996e8fdb983bbe6cc51f5139" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">X. Song, X. Zhao, H. Hu and L. Fang: <a href="http://scholar.google.de/scholar?q=EdgeStereo:%20A%20Context%20Integrated%20ResidualPyramid%20Network%20for%20Stereo%20Matching"> EdgeStereo: A Context Integrated Residual
Pyramid Network for Stereo Matching</a>. Asian Conference on Computer Vision 2018.<br></td>
   </tr>
   <tr>
    <td class="results">157</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b1542eea5af269e69992eb8076803e87b2242c9c">OGMNet18</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.97 %</td>
     <td class="results"> 3.16 %</td>
     <td class="results"> 2.17 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="b1542eea5af269e69992eb8076803e87b2242c9c" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">158</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=6171ed533e0ef422c1c3bc0bb9de4999fa68ca48">AutoDispNet-CSS</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/lmb-freiburg/autodispnet" target="blank">code</a></td>
     <td class="results"> 1.94 %</td>
     <td class="results"> 3.37 %</td>
     <td class="results"> 2.18 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.9 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="6171ed533e0ef422c1c3bc0bb9de4999fa68ca48" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">T. Saikia, Y. Marrakchi, A. Zela, F. Hutter and T. Brox: <a href="http://scholar.google.de/scholar?q=AutoDispNet:%20Improving%20Disparity%20Estimation%20with%20AutoML"> AutoDispNet: Improving Disparity 
Estimation with AutoML</a>. The IEEE International Conference 
on Computer Vision (ICCV) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">159</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c7b7aecb9eea3b8b0588ed71c4549282ade39c9b">BGNet+</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.81 %</td>
     <td class="results"> 4.09 %</td>
     <td class="results"> 2.19 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.03 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="c7b7aecb9eea3b8b0588ed71c4549282ade39c9b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">B. Xu, Y. Xu, X. Yang, W. Jia and Y. Guo: <a href="http://scholar.google.de/scholar?q=Bilateral%20Grid%20Learning%20for%20Stereo%20Matching%20Network"> Bilateral Grid Learning for Stereo Matching 
Network</a>. Proceedings of the IEEE/CVF Conference on 
Computer Vision and Pattern Recognition (CVPR) 2021.<br></td>
   </tr>
   <tr>
    <td class="results">160</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b9c7c0794b497c73e839a478952afcd2bac238c8">OA_COEX</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.85 %</td>
     <td class="results"> 3.95 %</td>
     <td class="results"> 2.20 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.35 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="b9c7c0794b497c73e839a478952afcd2bac238c8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">161</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=eacb1f02dfdc376368bab5bc6cc45d6b9c79a03f">Bi3D</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/NVlabs/Bi3D" target="blank">code</a></td>
     <td class="results"> 1.95 %</td>
     <td class="results"> 3.48 %</td>
     <td class="results"> 2.21 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.48 s</td>
     <td class="results">GPU @ 1.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="eacb1f02dfdc376368bab5bc6cc45d6b9c79a03f" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Badki, A. Troccoli, K. Kim, J. Kautz, P. Sen and O. Gallo: <a href="http://scholar.google.de/scholar?q=Bi3D:%20Stereo%20Depth%20Estimation%20via%20Binary%20Classifications"> Bi3D: Stereo Depth Estimation via Binary Classifications</a>. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020.<br></td>
   </tr>
   <tr>
    <td class="results">162</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=980848d1c727a33235d12bad225ddf69f9bdec7e">NDR</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.88 %</td>
     <td class="results"> 3.87 %</td>
     <td class="results"> 2.21 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.05 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="980848d1c727a33235d12bad225ddf69f9bdec7e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">163</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=4818d008024aa27566c05a3e6109f1b88f44d421">pcanet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/Anonymous" target="blank">code</a></td>
     <td class="results"> 1.94 %</td>
     <td class="results"> 3.57 %</td>
     <td class="results"> 2.21 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.27 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="4818d008024aa27566c05a3e6109f1b88f44d421" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">164</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=1e90b8e2e91aaf17565419f84149841bc6ad6b91">dh</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.86 %</td>
     <td class="results"> 4.01 %</td>
     <td class="results"> 2.22 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.9 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="1e90b8e2e91aaf17565419f84149841bc6ad6b91" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Zhang, V. Prisacariu, R. Yang and P. Torr: <a href="http://scholar.google.de/scholar?q=GA-Net:%20Guided%20Aggregation%20Net%20for%20End-to-end%20Stereo%20Matching"> GA-Net: Guided Aggregation Net for 
End-to-end Stereo Matching</a>. Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition 
(CVPR) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">165</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=8d1e9b4257ae79ab7979775eb2c065773eff9f1b">AGDNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.77 %</td>
     <td class="results"> 4.44 %</td>
     <td class="results"> 2.22 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.08 s</td>
     <td class="results">2 cores @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="8d1e9b4257ae79ab7979775eb2c065773eff9f1b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">166</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ceeec0d986f248d85d740d57ef051115d741aeaf">PSMNet+CBAM</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.78 %</td>
     <td class="results"> 4.42 %</td>
     <td class="results"> 2.22 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.36 s</td>
     <td class="results">NVIDIA RTX 3090 (Python)</td>
     <td class="results"><input type="checkbox" value="ceeec0d986f248d85d740d57ef051115d741aeaf" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">167</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=18e726136d732d4ab2e12cae710d3a73a9dccb1f">SENSE</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="https://github.com/NVlabs/SENSE" target="blank">code</a></td>
     <td class="results"> 2.07 %</td>
     <td class="results"> 3.01 %</td>
     <td class="results"> 2.22 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.32s</td>
     <td class="results">GPU, GTX 2080Ti</td>
     <td class="results"><input type="checkbox" value="18e726136d732d4ab2e12cae710d3a73a9dccb1f" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Jiang, D. Sun, V. Jampani, Z. Lv, E. Learned-Miller and J. Kautz: <a href="http://scholar.google.de/scholar?q=SENSE:%20A%20Shared%20Encoder%20Network%20for%20Scene-Flow%20Estimation"> SENSE: A Shared Encoder Network for Scene-Flow 
Estimation</a>. The IEEE International Conference on Computer 
Vision (ICCV) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">168</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ffe2d4bc9e9cbb6df585f2a265ca2b971edb4951">GASN</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.88 %</td>
     <td class="results"> 3.96 %</td>
     <td class="results"> 2.23 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.09 s</td>
     <td class="results">NVIDIA RTX 3090 (PyTorch)</td>
     <td class="results"><input type="checkbox" value="ffe2d4bc9e9cbb6df585f2a265ca2b971edb4951" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">169</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=08c31d70563916c6b01d22f6518bf2257e0d5759">PSMNet+Pre</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.79 %</td>
     <td class="results"> 4.44 %</td>
     <td class="results"> 2.23 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.36 s</td>
     <td class="results">NVIDIA RTX 3090 (Python)</td>
     <td class="results"><input type="checkbox" value="08c31d70563916c6b01d22f6518bf2257e0d5759" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">170</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=045e0248493921e3078108570b22bdd53191a0ff">SegStereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/yangguorun/SegStereo" target="blank">code</a></td>
     <td class="results"> 1.88 %</td>
     <td class="results"> 4.07 %</td>
     <td class="results"> 2.25 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.6 s</td>
     <td class="results">Nvidia GTX Titan Xp</td>
     <td class="results"><input type="checkbox" value="045e0248493921e3078108570b22bdd53191a0ff" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">G. Yang, H. Zhao, J. Shi, Z. Deng and J. Jia: <a href="http://scholar.google.de/scholar?q=SegStereo:%20Exploiting%20Semantic%20Information%20for%20Disparity%20Estimation"> SegStereo: Exploiting Semantic 
Information for Disparity Estimation</a>. ECCV 2018.<br></td>
   </tr>
   <tr>
    <td class="results">171</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=90bc2d62aa376bbb58350a1ba0b2064b4e54a1a8">DTF_SENSE</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div><div class="icon"><img src="./KITTI2015_files/icon_mv.png" alt="This method makes use of multiple (&gt;2) views."></div></td>
     <td class="results"></td>
     <td class="results"> 2.08 %</td>
     <td class="results"> 3.13 %</td>
     <td class="results"> 2.25 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.76 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="90bc2d62aa376bbb58350a1ba0b2064b4e54a1a8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Schuster, C. Unger and D. Stricker: <a href="http://scholar.google.de/scholar?q=A%20Deep%20Temporal%20Fusion%20Framework%20for%20Scene%20Flow%20Using%20a%20Learnable%20Motion%20Model%20and%20Occlusions"> A Deep Temporal Fusion Framework for Scene Flow Using a Learnable Motion Model and Occlusions</a>. IEEE Winter Conference on Applications of Computer Vision (WACV) 2021.<br></td>
   </tr>
   <tr>
    <td class="results">172</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=8a5395094d5c890f8d11fa4c4ea1c0c09f7a565a">OpenStereo-PSMNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/XiandaGuo/OpenStereo" target="blank">code</a></td>
     <td class="results"> 1.80 %</td>
     <td class="results"> 4.58 %</td>
     <td class="results"> 2.26 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.21 s</td>
     <td class="results">GPU @ 2.0 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="8a5395094d5c890f8d11fa4c4ea1c0c09f7a565a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">ERROR: Wrong syntax in BIBTEX file.<br></td>
   </tr>
   <tr>
    <td class="results">173</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=802cb88c3e06612a571982044c7c6a8a13b9b8c1">MCV-MFC</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.95 %</td>
     <td class="results"> 3.84 %</td>
     <td class="results"> 2.27 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.35 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="802cb88c3e06612a571982044c7c6a8a13b9b8c1" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Liang, Y. Guo, Y. Feng, W. Chen, L. Qiao, L. Zhou, J. Zhang and H. Liu: <a href="http://scholar.google.de/scholar?q=Stereo%20Matching%20Using%20Multi-level%20Cost%20Volume%20and%20Multi-scale%20Feature%20Constancy"> Stereo Matching Using Multi-level Cost Volume and Multi-scale Feature Constancy</a>. IEEE transactions on pattern analysis and machine intelligence 2019.<br></td>
   </tr>
   <tr>
    <td class="results">174</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=95f0c228f008e40e1e030a96203f8f724d9c7c46">HSM-1.5x</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/gengshan-y/high-res-stereo" target="blank">code</a></td>
     <td class="results"> 1.95 %</td>
     <td class="results"> 3.93 %</td>
     <td class="results"> 2.28 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.085 s</td>
     <td class="results">Titan X Pascal</td>
     <td class="results"><input type="checkbox" value="95f0c228f008e40e1e030a96203f8f724d9c7c46" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">G. Yang, J. Manela, M. Happold and D. Ramanan: <a href="http://scholar.google.de/scholar?q=Hierarchical%20Deep%20Stereo%20Matching%20on%20High-%20Resolution%20Images"> Hierarchical Deep Stereo Matching on 
High- 
Resolution Images</a>. The IEEE Conference on Computer 
Vision 
and Pattern Recognition (CVPR) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">175</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=111b16cb83d17a60b78688e1dd14d37a56dfa7ec">GAANet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.91 %</td>
     <td class="results"> 4.25 %</td>
     <td class="results"> 2.30 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.08</td>
     <td class="results">2080tiGPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="111b16cb83d17a60b78688e1dd14d37a56dfa7ec" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">176</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=8979c4449712b915bd869c89daa7120203e9dbbc">Separable Convs</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/cogsys-tuebingen/separable-3D-convs-for-stereo-matching" target="blank">code</a></td>
     <td class="results"> 1.90 %</td>
     <td class="results"> 4.36 %</td>
     <td class="results"> 2.31 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="8979c4449712b915bd869c89daa7120203e9dbbc" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Rahim, F. Shamsafar and A. Zell: <a href="http://scholar.google.de/scholar?q=Separable%20Convolutions%20for%20Optimizing%203D%20Stereo%20Networks"> Separable Convolutions for Optimizing 3D Stereo Networks</a>. 2021 IEEE International Conference on Image Processing (ICIP) 2021.<br></td>
   </tr>
   <tr>
    <td class="results">177</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=8a3097837fea56c11eb309005a612281ceca43ab">Separable Convs</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/cogsys-tuebingen/separable-3D-convs-for-stereo-matching" target="blank">code</a></td>
     <td class="results"> 1.90 %</td>
     <td class="results"> 4.36 %</td>
     <td class="results"> 2.31 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="8a3097837fea56c11eb309005a612281ceca43ab" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Rahim, F. Shamsafar and A. Zell: <a href="http://scholar.google.de/scholar?q=Separable%20Convolutions%20for%20Optimizing%203D%20Stereo%20Networks"> Separable Convolutions for Optimizing 3D Stereo Networks</a>. 2021 IEEE International Conference on Image Processing (ICIP) 2021.<br></td>
   </tr>
   <tr>
    <td class="results">178</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b2625abde3809778f44b18152c8cd3822bf552f0">CFP-Net</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/progressforever/Cross-form-Pyramid-Network-for-Stereo-Matching-CFPNet-.git" target="blank">code</a></td>
     <td class="results"> 1.90 %</td>
     <td class="results"> 4.39 %</td>
     <td class="results"> 2.31 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.9 s</td>
     <td class="results">8 cores @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="b2625abde3809778f44b18152c8cd3822bf552f0" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Zhu, M. He, Y. Dai, Z. Rao and B. Li: <a href="http://scholar.google.de/scholar?q=Multi-scale%20Cross-form%20Pyramid%20Network%20for%20Stereo%20Matching"> Multi-scale Cross-form Pyramid Network for Stereo Matching</a>. arXiv preprint 2019.<br></td>
   </tr>
   <tr>
    <td class="results">179</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=efb9db97938e12a20b9c95ce593f633dd63a2744">PSMNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/JiaRenChang/PSMNet" target="blank">code</a></td>
     <td class="results"> 1.86 %</td>
     <td class="results"> 4.62 %</td>
     <td class="results"> 2.32 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.41 s</td>
     <td class="results">Nvidia GTX Titan Xp</td>
     <td class="results"><input type="checkbox" value="efb9db97938e12a20b9c95ce593f633dd63a2744" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Chang and Y. Chen: <a href="http://scholar.google.de/scholar?q=Pyramid%20Stereo%20Matching%20Network"> Pyramid Stereo Matching Network</a>. arXiv preprint arXiv:1803.08669 2018.<br></td>
   </tr>
   <tr>
    <td class="results">180</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ed1d954a391805d4b1122f769bd83a627f50303b">GANetREF_RVC</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/feihuzhang/GANet" target="blank">code</a></td>
     <td class="results"> 1.88 %</td>
     <td class="results"> 4.58 %</td>
     <td class="results"> 2.33 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.62 s</td>
     <td class="results">GPU @ &gt;3.5 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="ed1d954a391805d4b1122f769bd83a627f50303b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Zhang, V. Prisacariu, R. Yang and P. Torr: <a href="http://scholar.google.de/scholar?q=GA-Net:%20Guided%20Aggregation%20Net%20for%20End-to-end%20Stereo%20Matching"> GA-Net: Guided Aggregation Net for End-
to-end Stereo Matching</a>. Proceedings of the IEEE Conference on 
Computer Vision and Pattern Recognition 2019.<br></td>
   </tr>
   <tr>
    <td class="results">181</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=fe94cd121a6faf705d17d507f994587acee0e22a">TriStereoNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/cogsys-tuebingen/tristereonet" target="blank">code</a></td>
     <td class="results"> 1.86 %</td>
     <td class="results"> 4.77 %</td>
     <td class="results"> 2.35 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.5 s</td>
     <td class="results">Python,1080Ti</td>
     <td class="results"><input type="checkbox" value="fe94cd121a6faf705d17d507f994587acee0e22a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Shamsafar and A. Zell: <a href="http://scholar.google.de/scholar?q=TriStereoNet:%20A%20Trinocular%20Framework%20for%20Multi-baseline%20Disparity%20Estimation"> TriStereoNet: A Trinocular Framework for 
Multi-baseline Disparity Estimation</a>. arXiv preprint arXiv:2111.12502 2021.<br></td>
   </tr>
   <tr>
    <td class="results">182</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b53bafe70c0fb538c41fc99283471ddc0c95cb3d">MABNet_origin</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730341.pdf" target="blank">code</a></td>
     <td class="results"> 1.89 %</td>
     <td class="results"> 5.02 %</td>
     <td class="results"> 2.41 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.38 s</td>
     <td class="results">Nvidia rtx2080ti (Python)</td>
     <td class="results"><input type="checkbox" value="b53bafe70c0fb538c41fc99283471ddc0c95cb3d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Xing, Z. Qi, J. Dong, J. Cai and H. Liu: <a href="http://scholar.google.de/scholar?q=MABNet:%20A%20Lightweight%20Stereo%20Network%20Based%20on%20Multibranch%20Adjustable%20Bottleneck%20Module"> MABNet: A Lightweight Stereo Network 
Based on Multibranch Adjustable Bottleneck 
Module</a>. .<br></td>
   </tr>
   <tr>
    <td class="results">183</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5e8b9c360bf83f4907c357df7ecabd8a94ca61f0">gsnet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 1.93 %</td>
     <td class="results"> 4.95 %</td>
     <td class="results"> 2.43 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.2 s</td>
     <td class="results">GPU @ 3.0 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="5e8b9c360bf83f4907c357df7ecabd8a94ca61f0" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">184</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=3a0ef7ec8859e85f837b57bcb21f0d1f717ea1a1">MDTE4</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.06 %</td>
     <td class="results"> 4.32 %</td>
     <td class="results"> 2.43 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.03 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="3a0ef7ec8859e85f837b57bcb21f0d1f717ea1a1" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">185</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5ba7c6e459422e2c4ea1317fa090dc6b25e5dfc3">MDCTE3</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.06 %</td>
     <td class="results"> 4.32 %</td>
     <td class="results"> 2.43 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.06 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="5ba7c6e459422e2c4ea1317fa090dc6b25e5dfc3" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">186</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=7e945d1e3adb2c3a497fb0cd3a9bbe837d70b4f3">SAStereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.21 %</td>
     <td class="results"> 3.68 %</td>
     <td class="results"> 2.46 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.04 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="7e945d1e3adb2c3a497fb0cd3a9bbe837d70b4f3" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">187</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=6e894523d97c52776d47190c0a25752cd1a9fb25">AFDNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.21 %</td>
     <td class="results"> 3.78 %</td>
     <td class="results"> 2.47 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.31 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="6e894523d97c52776d47190c0a25752cd1a9fb25" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">188</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f092cbccc8baa83927d0e56930b68da3b0cab09b">ERSCNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.11 %</td>
     <td class="results"> 4.46 %</td>
     <td class="results"> 2.50 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.28 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="f092cbccc8baa83927d0e56930b68da3b0cab09b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Anonymous: <a href="http://scholar.google.de/scholar?q=ERSCNet"> ERSCNet</a>. Proceedings of the European 
Conference on Computer Vision (ECCV) 2020.<br></td>
   </tr>
   <tr>
    <td class="results">189</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=55d8bd4077b95b230de5a59ca7a6d38f0836e665">BGNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.07 %</td>
     <td class="results"> 4.74 %</td>
     <td class="results"> 2.51 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.02 s</td>
     <td class="results">GPU @ &gt;3.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="55d8bd4077b95b230de5a59ca7a6d38f0836e665" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">B. Xu, Y. Xu, X. Yang, W. Jia and Y. Guo: <a href="http://scholar.google.de/scholar?q=Bilateral%20Grid%20Learning%20for%20Stereo%20Matching%20Network"> Bilateral Grid Learning for Stereo 
Matching Network</a>. Proceedings of the IEEE/CVF Conference 
on Computer Vision and Pattern Recognition (CVPR) 2021.<br></td>
   </tr>
   <tr>
    <td class="results">190</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=6e451b8fe086afc772e1c11f8ad01c594385e219">UberATG-DRISF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 2.16 %</td>
     <td class="results"> 4.49 %</td>
     <td class="results"> 2.55 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.75 s</td>
     <td class="results">CPU+GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="6e451b8fe086afc772e1c11f8ad01c594385e219" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">W. Ma, S. Wang, R. Hu, Y. Xiong and R. Urtasun: <a href="http://scholar.google.de/scholar?q=Deep%20Rigid%20Instance%20Scene%20Flow"> Deep Rigid Instance Scene Flow</a>. CVPR 2019.<br></td>
   </tr>
   <tr>
    <td class="results">191</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=7e5ae6e5a47b9f09e34954d2f8d57a789456443b">AANet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/haofeixu/aanet" target="blank">code</a></td>
     <td class="results"> 1.99 %</td>
     <td class="results"> 5.39 %</td>
     <td class="results"> 2.55 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.062 s</td>
     <td class="results">NVIDIA V100 GPU</td>
     <td class="results"><input type="checkbox" value="7e5ae6e5a47b9f09e34954d2f8d57a789456443b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Xu and J. Zhang: <a href="http://scholar.google.de/scholar?q=AANet:%20Adaptive%20Aggregation%20Networkfor%20Efficient%20Stereo%20Matching"> AANet: Adaptive Aggregation Network
for Efficient Stereo Matching</a>. CVPR 2020.<br></td>
   </tr>
   <tr>
    <td class="results">192</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b212702fe61ec7dde49cef0e1506d6fdbf5617e6">GASN-FA</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.25 %</td>
     <td class="results"> 4.13 %</td>
     <td class="results"> 2.56 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.05 s</td>
     <td class="results">NVIDIA RTX 3090 (PyTorch)</td>
     <td class="results"><input type="checkbox" value="b212702fe61ec7dde49cef0e1506d6fdbf5617e6" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">193</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5cdecbf8c19700252006bd7f663a66cab5edaf63">PDSNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.29 %</td>
     <td class="results"> 4.05 %</td>
     <td class="results"> 2.58 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.5 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="5cdecbf8c19700252006bd7f663a66cab5edaf63" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">S. Tulyakov, A. Ivanov and F. Fleuret: <a href="http://scholar.google.de/scholar?q=Practical%20Deep%20Stereo%20(PDS):%20Toward%20applications-friendly%20deep%20stereo%20matching"> Practical Deep Stereo (PDS): Toward 
applications-friendly deep stereo matching</a>. Proceedings of the international conference 
on Neural Information Processing Systems (NIPS) 2018.<br></td>
   </tr>
   <tr>
    <td class="results">194</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=fde5e04e47f4c5b8c40ec3584eff4d6001b2a7fb">DeepPruner (fast)</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/uber-research/DeepPruner/" target="blank">code</a></td>
     <td class="results"> 2.32 %</td>
     <td class="results"> 3.91 %</td>
     <td class="results"> 2.59 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.06 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="fde5e04e47f4c5b8c40ec3584eff4d6001b2a7fb" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">S. Duggal, S. Wang, W. Ma, R. Hu and R. Urtasun: <a href="http://scholar.google.de/scholar?q=DeepPruner:%20Learning%20Efficient%20Stereo%20Matching%20via%20Differentiable%20PatchMatch"> DeepPruner: Learning Efficient Stereo Matching 
via Differentiable PatchMatch</a>. ICCV 2019.<br></td>
   </tr>
   <tr>
    <td class="results">195</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=fb66b3db2d2066965af87f7e4ca6bdc98bec2fb4">FADNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/HKBU-HPML/FADNet" target="blank">code</a></td>
     <td class="results"> 2.50 %</td>
     <td class="results"> 3.10 %</td>
     <td class="results"> 2.60 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.05 s</td>
     <td class="results">Tesla V100 (Python)</td>
     <td class="results"><input type="checkbox" value="fb66b3db2d2066965af87f7e4ca6bdc98bec2fb4" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Q. Wang, S. Shi, S. Zheng, K. Zhao and X. Chu: <a href="http://scholar.google.de/scholar?q=FADNet:%20A%20Fast%20and%20Accurate%20Network%20for%20Disparity%20Estimation"> FADNet: A Fast and Accurate Network 
for Disparity Estimation</a>. arXiv preprint arXiv:2003.10758 2020.<br></td>
   </tr>
   <tr>
    <td class="results">196</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f6cb50a47598e7ced242b38b5fc9bd6b25a6d67b">MMStereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.25 %</td>
     <td class="results"> 4.38 %</td>
     <td class="results"> 2.61 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.04 s</td>
     <td class="results">Nvidia Titan RTX (Python)</td>
     <td class="results"><input type="checkbox" value="f6cb50a47598e7ced242b38b5fc9bd6b25a6d67b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">K. Shankar, M. Tjersland, J. Ma, K. Stone and M. Bajracharya: <a href="http://scholar.google.de/scholar?q=A%20Learned%20Stereo%20Depth%20System%20forRobotic%20Manipulation%20in%20Homes"> A Learned Stereo Depth System for
Robotic Manipulation in Homes</a>. .<br></td>
   </tr>
   <tr>
    <td class="results">197</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=90899ecfea9a5eb74884056eb89ec8b930b81c35">SCV</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/rairyuu/SCVNet" target="blank">code</a></td>
     <td class="results"> 2.22 %</td>
     <td class="results"> 4.53 %</td>
     <td class="results"> 2.61 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.36 s</td>
     <td class="results">Nvidia GTX 1080 Ti</td>
     <td class="results"><input type="checkbox" value="90899ecfea9a5eb74884056eb89ec8b930b81c35" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">C. Lu, H. Uchiyama, D. Thomas, A. Shimada and R. Taniguchi: <a href="http://scholar.google.de/scholar?q=Sparse%20Cost%20Volume%20for%20Efficient%20Stereo%20Matching"> Sparse Cost Volume for Efficient 
Stereo Matching</a>. Remote Sensing 2018.<br></td>
   </tr>
   <tr>
    <td class="results">198</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=0ce1c49535dca74efcf042da1c3f96d0737d47d8">WaveletStereo:</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.24 %</td>
     <td class="results"> 4.62 %</td>
     <td class="results"> 2.63 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.27 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="0ce1c49535dca74efcf042da1c3f96d0737d47d8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Anonymous: <a href="http://scholar.google.de/scholar?q=WaveletStereo:%20Learning%20wavelet%20coefficients%20for%20stereo%20matching"> WaveletStereo: Learning wavelet coefficients 
for stereo matching</a>. arXiv: Computer Vision and Pattern 
Recognition 2019.<br></td>
   </tr>
   <tr>
    <td class="results">199</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=71a6f5598c25b0c08fb35ee415e4e9668be4a6a7">RLStereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/RayWuuuuu" target="blank">code</a></td>
     <td class="results"> 2.09 %</td>
     <td class="results"> 5.38 %</td>
     <td class="results"> 2.64 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.03 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="71a6f5598c25b0c08fb35ee415e4e9668be4a6a7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Anonymous: <a href="http://scholar.google.de/scholar?q=RLStereo:%20Real-time%20Stereo%20Matching%20based%20on%20Reinforcement%20Learning"> RLStereo: Real-time Stereo Matching 
based on Reinforcement Learning</a>. Proceedings of the IEEE/CVF 
International Conference on Computer Vision 2021.<br></td>
   </tr>
   <tr>
    <td class="results">200</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5108a172c833c45f3023f588fc1a3609f3ba85a6">AANet_RVC</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.23 %</td>
     <td class="results"> 4.89 %</td>
     <td class="results"> 2.67 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.1 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="5108a172c833c45f3023f588fc1a3609f3ba85a6" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Xu and J. Zhang: <a href="http://scholar.google.de/scholar?q=AANet:%20Adaptive%20Aggregation%20Network%20for%20Efficient%20Stereo%20Matching"> AANet: Adaptive Aggregation Network for 
Efficient Stereo Matching</a>. CVPR 2020.<br></td>
   </tr>
   <tr>
    <td class="results">201</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f791987e39ecb04c1eee821ae3a0cd53d5fd28c4">CRL</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/Artifineuro/crl" target="blank">code</a></td>
     <td class="results"> 2.48 %</td>
     <td class="results"> 3.59 %</td>
     <td class="results"> 2.67 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.47 s</td>
     <td class="results">Nvidia GTX 1080</td>
     <td class="results"><input type="checkbox" value="f791987e39ecb04c1eee821ae3a0cd53d5fd28c4" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Pang, W. Sun, J. Ren, C. Yang and Q. Yan: <a href="http://scholar.google.de/scholar?q=Cascade%20residual%20learning:%20A%20two-stage%20convolutional%20neural%20network%20for%20stereo%20matching"> Cascade residual learning: A two-stage 
convolutional neural network for stereo 
matching</a>. ICCV Workshop on Geometry Meets 
Deep Learning 2017.<br></td>
   </tr>
   <tr>
    <td class="results">202</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=39290e76173f581a8ca318bb1e9a12e16b8f3ca5">TSNnet_Teacher</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.24 %</td>
     <td class="results"> 4.99 %</td>
     <td class="results"> 2.70 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.01 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="39290e76173f581a8ca318bb1e9a12e16b8f3ca5" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">203</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=53b8257acf35d19410db728c95e5e666890c5e27">2D-MSNet / MSNet2D </a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/cogsys-tuebingen/mobilestereonet" target="blank">code</a></td>
     <td class="results"> 2.49 %</td>
     <td class="results"> 4.53 %</td>
     <td class="results"> 2.83 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.4s</td>
     <td class="results">Python,1080Ti</td>
     <td class="results"><input type="checkbox" value="53b8257acf35d19410db728c95e5e666890c5e27" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Shamsafar, S. Woerz, R. Rahim and A. Zell: <a href="http://scholar.google.de/scholar?q=MobileStereoNet:%20Towards%20Lightweight%20Deep%20Networks%20for%20Stereo%20Matching"> MobileStereoNet: Towards Lightweight Deep 
Networks for Stereo Matching</a>. Proceedings of the IEEE/CVF Winter 
Conference on Applications of Computer Vision 2022.<br></td>
   </tr>
   <tr>
    <td class="results">204</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=70b339586af7c573b33a4dad14ea4a7689dc9305">GC-NET</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.21 %</td>
     <td class="results"> 6.16 %</td>
     <td class="results"> 2.87 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.9 s</td>
     <td class="results">Nvidia GTX Titan X</td>
     <td class="results"><input type="checkbox" value="70b339586af7c573b33a4dad14ea4a7689dc9305" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach and A. Bry: <a href="http://scholar.google.de/scholar?q=End-to-End%20Learning%20of%20Geometry%20and%20Context%20for%20Deep%20Stereo%20Regression"> End-to-End Learning of Geometry and Context for 
Deep Stereo Regression</a>. Proceedings of the International Conference on 
Computer Vision (ICCV) 2017.<br></td>
   </tr>
   <tr>
    <td class="results">205</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b2024008a9731b2e5d4c0a65a21b92841736e5ef">PVStereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.29 %</td>
     <td class="results"> 6.50 %</td>
     <td class="results"> 2.99 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.10 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="b2024008a9731b2e5d4c0a65a21b92841736e5ef" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Wang, R. Fan, P. Cai and M. Liu: <a href="http://scholar.google.de/scholar?q=PVStereo:%20Pyramid%20voting%20module%20for%20end-to-end%20self-supervised%20stereo%20matching"> PVStereo: Pyramid voting module 
for end-to-end self-supervised stereo matching</a>. IEEE Robotics and Automation 
Letters 2021.<br></td>
   </tr>
   <tr>
    <td class="results">206</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=0a1da6a4cfb7c2f88835073bfb073238e601f00f">LRCR</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.55 %</td>
     <td class="results"> 5.42 %</td>
     <td class="results"> 3.03 %</td>
     <td class="results">100.00 %</td>
     <td class="results">49.2 s</td>
     <td class="results">Nvidia GTX Titan X</td>
     <td class="results"><input type="checkbox" value="0a1da6a4cfb7c2f88835073bfb073238e601f00f" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Jie, P. Wang, Y. Ling, B. Zhao, Y. Wei, J. Feng and W. Liu: <a href="http://scholar.google.de/scholar?q=Left-Right%20Comparative%20Recurrent%20Model%20for%20Stereo%20Matching"> Left-Right Comparative Recurrent Model for 
Stereo Matching</a>. IEEE Conference on Computer Vision and 
Pattern Recognition (CVPR) 2018.<br></td>
   </tr>
   <tr>
    <td class="results">207</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=81b064dbd91d4b87f4eed19832012c9e378ef11a">cas-stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.62 %</td>
     <td class="results"> 5.17 %</td>
     <td class="results"> 3.04 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.1 s</td>
     <td class="results">8 cores @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="81b064dbd91d4b87f4eed19832012c9e378ef11a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">208</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=525e1ae0f0f15a64e0bf06b8fd194f0783ec9416">TSNnet_student</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.35 %</td>
     <td class="results"> 6.70 %</td>
     <td class="results"> 3.07 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.01 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="525e1ae0f0f15a64e0bf06b8fd194f0783ec9416" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">209</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=982e0fb46a9622e3a135503c19492ef2190a127d">Fast DS-CS</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://projects.ayanc.org/fdscs/" target="blank">code</a></td>
     <td class="results"> 2.83 %</td>
     <td class="results"> 4.31 %</td>
     <td class="results"> 3.08 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.02 s</td>
     <td class="results">GPU @ 2.0 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="982e0fb46a9622e3a135503c19492ef2190a127d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">K. Yee and A. Chakrabarti: <a href="http://scholar.google.de/scholar?q=Fast%20Deep%20Stereo%20with%202D%20Convolutional%20Processing%20of%20Cost%20Signatures"> Fast Deep Stereo with 2D Convolutional 
Processing of Cost Signatures</a>. WACV 2020 (to appear).<br></td>
   </tr>
   <tr>
    <td class="results">210</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b17eabe15417dd3d8292707f0c813897ff4cc29e">AdaStereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.59 %</td>
     <td class="results"> 5.55 %</td>
     <td class="results"> 3.08 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.41 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="b17eabe15417dd3d8292707f0c813897ff4cc29e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">X. Song, G. Yang, X. Zhu, H. Zhou, Z. Wang and J. Shi: <a href="http://scholar.google.de/scholar?q=AdaStereo:%20A%20Simple%20and%20Efficient%20Approach%20for%20Adaptive%20Stereo%20Matching"> AdaStereo: A Simple and Efficient 
Approach for Adaptive Stereo Matching</a>. CVPR 2021.<br>X. Song, G. Yang, X. Zhu, H. Zhou, Y. Ma, Z. Wang and J. Shi: <a href="http://scholar.google.de/scholar?q=AdaStereo:%20An%20Efficient%20Domain-Adaptive%20Stereo%20Matching%20Approach"> AdaStereo: An Efficient Domain-Adaptive 
Stereo Matching Approach</a>. IJCV 2021.<br></td>
   </tr>
   <tr>
    <td class="results">211</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b3cdfa84718a58bc16798f07810b65b7be8be5e5">RecResNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/kbatsos/RecResNet" target="blank">code</a></td>
     <td class="results"> 2.46 %</td>
     <td class="results"> 6.30 %</td>
     <td class="results"> 3.10 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.3 s</td>
     <td class="results">GPU @ NVIDIA TITAN X (Tensorflow)</td>
     <td class="results"><input type="checkbox" value="b3cdfa84718a58bc16798f07810b65b7be8be5e5" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">K. Batsos and P. Mordohai: <a href="http://scholar.google.de/scholar?q=RecResNet:%20A%20Recurrent%20Residual%20CNN%20Architecture%20for%20Disparity%20Map%20Enhancement"> RecResNet: A Recurrent Residual CNN 
Architecture for Disparity Map Enhancement</a>.  In International Conference on 3D 
Vision (3DV)  2018.<br></td>
   </tr>
   <tr>
    <td class="results">212</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=95eaf3e73fa4858fc6bfe08bad4830d4e90e95af">NVStereoNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/NVIDIA-Jetson/redtail/tree/master/stereoDNN" target="blank">code</a></td>
     <td class="results"> 2.62 %</td>
     <td class="results"> 5.69 %</td>
     <td class="results"> 3.13 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.6 s</td>
     <td class="results">NVIDIA Titan Xp</td>
     <td class="results"><input type="checkbox" value="95eaf3e73fa4858fc6bfe08bad4830d4e90e95af" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">N. Smolyanskiy, A. Kamenev and S. Birchfield: <a href="http://scholar.google.de/scholar?q=On%20the%20Importance%20of%20Stereo%20for%20Accurate%20Depth%20Estimation:%20An%20Efficient%20Semi-Supervised%20Deep%20Neural%20Network%20Approach"> On the Importance of Stereo for Accurate 
Depth Estimation: An Efficient Semi-Supervised 
Deep Neural Network Approach</a>. arXiv preprint arXiv:1803.09719 2018.<br></td>
   </tr>
   <tr>
    <td class="results">213</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=365eacbf1effa761ed07aaa674a9b61c60fe9300">DRR</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.58 %</td>
     <td class="results"> 6.04 %</td>
     <td class="results"> 3.16 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.4 s</td>
     <td class="results">Nvidia GTX Titan X</td>
     <td class="results"><input type="checkbox" value="365eacbf1effa761ed07aaa674a9b61c60fe9300" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">S. Gidaris and N. Komodakis: <a href="http://scholar.google.de/scholar?q=Detect,%20Replace,%20Refine:%20Deep%20Structured%20Prediction%20For%20Pixel%20Wise%20Labeling"> Detect, Replace, Refine: Deep Structured Prediction For Pixel Wise Labeling</a>. arXiv preprint arXiv:1612.04770 2016.<br></td>
   </tr>
   <tr>
    <td class="results">214</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=77e2ffe05f35444bc1d61761468c49529f5fe99f">TSNnet_naive</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.64 %</td>
     <td class="results"> 6.47 %</td>
     <td class="results"> 3.28 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.01 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="77e2ffe05f35444bc1d61761468c49529f5fe99f" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">215</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=e97405a13ad3faf8eb7b876b614f67c483b26a88">DWARF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 3.20 %</td>
     <td class="results"> 3.94 %</td>
     <td class="results"> 3.33 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.14s - 1.43s</td>
     <td class="results">TitanXP - JetsonTX2</td>
     <td class="results"><input type="checkbox" value="e97405a13ad3faf8eb7b876b614f67c483b26a88" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Aleotti, M. Poggi, F. Tosi and S. Mattoccia: <a href="http://scholar.google.de/scholar?q=Learning%20end-to-end%20scene%20flow%20by%20distilling%20single%20tasks%20knowledge"> Learning end-to-end scene flow by 
distilling single tasks knowledge</a>. Thirty-Fourth AAAI Conference on 
Artificial Intelligence (AAAI-20) 2020.<br></td>
   </tr>
   <tr>
    <td class="results">216</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5873ed177c83b18b1833d55627e96fac57603429">SsSMnet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.70 %</td>
     <td class="results"> 6.92 %</td>
     <td class="results"> 3.40 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.8 s</td>
     <td class="results">P100</td>
     <td class="results"><input type="checkbox" value="5873ed177c83b18b1833d55627e96fac57603429" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Y. Zhong, Y. Dai and H. Li: <a href="http://scholar.google.de/scholar?q=Self-Supervised%20Learning%20for%20Stereo%20Matching%20with%20Self-Improving%20Ability"> Self-Supervised Learning for Stereo 
Matching with Self-Improving Ability</a>. arXiv:1709.00930 2017.<br></td>
   </tr>
   <tr>
    <td class="results">217</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=38c3491607b56cc1ac1417dde642f79900dfb4bb">L-ResMatch</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/amitshaked/resmatch" target="blank">code</a></td>
     <td class="results"> 2.72 %</td>
     <td class="results"> 6.95 %</td>
     <td class="results"> 3.42 %</td>
     <td class="results">100.00 %</td>
     <td class="results">48 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="38c3491607b56cc1ac1417dde642f79900dfb4bb" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Shaked and L. Wolf: <a href="http://scholar.google.de/scholar?q=Improved%20Stereo%20Matching%20with%20Constant%20Highway%20Networks%20and%20Reflective%20Loss"> Improved Stereo Matching with Constant Highway 
Networks and Reflective Loss</a>. arXiv preprint arxiv:1701.00165 2016.<br></td>
   </tr>
   <tr>
    <td class="results">218</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=776961485e1445f31a3c7949e76493e3250b7b38">Displets v2</a></td>
     <td class="results"></td>
     <td class="results"><a href="http://www.cvlibs.net/projects/displets/" target="blank">code</a></td>
     <td class="results"> 3.00 %</td>
     <td class="results"> 5.56 %</td>
     <td class="results"> 3.43 %</td>
     <td class="results">100.00 %</td>
     <td class="results">265 s</td>
     <td class="results">&gt;8 cores @ 3.0 Ghz (Matlab + C/C++)</td>
     <td class="results"><input type="checkbox" value="776961485e1445f31a3c7949e76493e3250b7b38" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Guney and A. Geiger: <a href="http://scholar.google.de/scholar?q=Displets:%20Resolving%20Stereo%20Ambiguities%20using%20Object%20Knowledge"> Displets: Resolving Stereo Ambiguities 
using Object Knowledge</a>. Conference on Computer Vision and 
Pattern Recognition (CVPR) 2015.<br></td>
   </tr>
   <tr>
    <td class="results">219</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=092c4e3c93b264c0f5d93f3425b13f65cb5d3da7">LBPS</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/VLOGroup/bp-layers" target="blank">code</a></td>
     <td class="results"> 2.85 %</td>
     <td class="results"> 6.35 %</td>
     <td class="results"> 3.44 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.39 s</td>
     <td class="results">GPU @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="092c4e3c93b264c0f5d93f3425b13f65cb5d3da7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">P. Knöbelreiter, C. Sormann, A. Shekhovtsov, F. Fraundorfer and T. Pock: <a href="http://scholar.google.de/scholar?q=Belief%20Propagation%20Reloaded:%20Learning%20BP-Layers%20for%20Labeling%20Problems"> Belief Propagation Reloaded: Learning 
BP-Layers for Labeling Problems</a>. IEEE Conference on Computer Vision 
and Pattern Recognition (CVPR) 2020.<br></td>
   </tr>
   <tr>
    <td class="results">220</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=447f9b3c0599f8cad6ff0603ca33fd2aa0852d1d">ACOSF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 2.79 %</td>
     <td class="results"> 7.56 %</td>
     <td class="results"> 3.58 %</td>
     <td class="results">100.00 %</td>
     <td class="results">5 min</td>
     <td class="results">1 core @ 3.0 Ghz (Matlab + C/C++)</td>
     <td class="results"><input type="checkbox" value="447f9b3c0599f8cad6ff0603ca33fd2aa0852d1d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">C. Li, H. Ma and Q. Liao: <a href="http://scholar.google.de/scholar?q=Two-Stage%20Adaptive%20Object%20Scene%20Flow%20Using%20Hybrid%20CNN-CRF%20Model"> Two-Stage Adaptive Object Scene Flow Using 
Hybrid CNN-CRF Model</a>. International Conference on Pattern 
Recognition (ICPR) 2020.<br></td>
   </tr>
   <tr>
    <td class="results">221</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=eb8aba2e21960d803fd6a0c6f22a646d4b6684e7">CNNF+SGM</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.78 %</td>
     <td class="results"> 7.69 %</td>
     <td class="results"> 3.60 %</td>
     <td class="results">100.00 %</td>
     <td class="results">71 s</td>
     <td class="results">TESLA K40C</td>
     <td class="results"><input type="checkbox" value="eb8aba2e21960d803fd6a0c6f22a646d4b6684e7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Zhang and B. Wah: <a href="http://scholar.google.de/scholar?q=Fundamental%20Principles%20on%20Learning%20New%20Features%20for%20Effective%20Dense%20Matching"> Fundamental Principles on Learning New 
Features for Effective Dense Matching</a>. IEEE Transactions on Image 
Processing 2018.<br></td>
   </tr>
   <tr>
    <td class="results">222</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b19e8118c0500d612179e1a078555682bb736b8f">PBCP</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.58 %</td>
     <td class="results"> 8.74 %</td>
     <td class="results"> 3.61 %</td>
     <td class="results">100.00 %</td>
     <td class="results">68 s</td>
     <td class="results">Nvidia GTX Titan X</td>
     <td class="results"><input type="checkbox" value="b19e8118c0500d612179e1a078555682bb736b8f" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Seki and M. Pollefeys: <a href="http://scholar.google.de/scholar?q=Patch%20Based%20Confidence%20Prediction%20for%20Dense%20Disparity%20Map"> Patch Based Confidence Prediction for 
Dense Disparity Map</a>. British Machine Vision Conference 
(BMVC) 2016.<br></td>
   </tr>
   <tr>
    <td class="results">223</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=3c76f3f0e552031f4364034bbed10ec5a8bbcd4a">SGM-Net</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 2.66 %</td>
     <td class="results"> 8.64 %</td>
     <td class="results"> 3.66 %</td>
     <td class="results">100.00 %</td>
     <td class="results">67 s</td>
     <td class="results">Titan X</td>
     <td class="results"><input type="checkbox" value="3c76f3f0e552031f4364034bbed10ec5a8bbcd4a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Seki and M. Pollefeys: <a href="http://scholar.google.de/scholar?q=SGM-Nets:%20Semi-Global%20Matching%20With%20Neural%20Networks"> SGM-Nets: Semi-Global Matching With Neural 
Networks</a>. CVPR 2017.<br></td>
   </tr>
   <tr>
    <td class="results">224</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ca16854fdce99a05d51007e8856a3357e75778d9">DSMNet-synthetic</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 3.11 %</td>
     <td class="results"> 6.72 %</td>
     <td class="results"> 3.71 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.6 s</td>
     <td class="results">4 cores @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="ca16854fdce99a05d51007e8856a3357e75778d9" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Zhang, X. Qi, R. Yang, V. Prisacariu, B. Wah and P. Torr: <a href="http://scholar.google.de/scholar?q=Domain-invariant%20Stereo%20Matching%20Networks"> Domain-invariant Stereo Matching 
Networks</a>. Europe Conference on Computer Vision 
(ECCV) 2020.<br></td>
   </tr>
   <tr>
    <td class="results">225</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=791e1ac8b1c77ae9b1a9b3e2c022a78527e157a9">HSM-Net_RVC</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/gengshan-y/high-res-stereo" target="blank">code</a></td>
     <td class="results"> 2.74 %</td>
     <td class="results"> 8.73 %</td>
     <td class="results"> 3.74 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.97 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="791e1ac8b1c77ae9b1a9b3e2c022a78527e157a9" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">G. Yang, J. Manela, M. Happold and D. Ramanan: <a href="http://scholar.google.de/scholar?q=Hierarchical%20deep%20stereo%20matching%20on%20high-resolution%20images"> Hierarchical deep stereo matching on 
high-resolution images</a>. Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition 2019.<br></td>
   </tr>
   <tr>
    <td class="results">226</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=582082f076511f5a571733e8e2d9fa667ec771b9">MABNet_tiny</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730341.pdf" target="blank">code</a></td>
     <td class="results"> 3.04 %</td>
     <td class="results"> 8.07 %</td>
     <td class="results"> 3.88 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.11 s</td>
     <td class="results">Nvidia rtx2080ti (Python)</td>
     <td class="results"><input type="checkbox" value="582082f076511f5a571733e8e2d9fa667ec771b9" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Xing, Z. Qi, J. Dong, J. Cai and H. Liu: <a href="http://scholar.google.de/scholar?q=MABNet:%20A%20Lightweight%20Stereo%20Network%20Based%20on%20Multibranch%20Adjustable%20Bottleneck%20Module"> MABNet: A Lightweight Stereo Network 
Based on Multibranch Adjustable Bottleneck 
Module</a>. .<br></td>
   </tr>
   <tr>
    <td class="results">227</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=92229724a5bc8ddd3a2b30406924010d6919934e">MC-CNN-acrt</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/jzbontar/mc-cnn" target="blank">code</a></td>
     <td class="results"> 2.89 %</td>
     <td class="results"> 8.88 %</td>
     <td class="results"> 3.89 %</td>
     <td class="results">100.00 %</td>
     <td class="results">67 s</td>
     <td class="results">Nvidia GTX Titan X (CUDA, Lua/Torch7)</td>
     <td class="results"><input type="checkbox" value="92229724a5bc8ddd3a2b30406924010d6919934e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Zbontar and Y. LeCun: <a href="http://scholar.google.de/scholar?q=Stereo%20Matching%20by%20Training%20a%20Convolutional%20Neural%20Network%20to%20Compare%20Image%20Patches"> Stereo Matching by Training a Convolutional 
Neural Network to Compare Image Patches</a>. Submitted to JMLR .<br></td>
   </tr>
   <tr>
    <td class="results">228</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=49595f2e37384684778f134d9eaf7282b7ea3a40">FD-Fusion</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/ferreram/FD-Fusion" target="blank">code</a></td>
     <td class="results"> 3.22 %</td>
     <td class="results"> 7.44 %</td>
     <td class="results"> 3.92 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.01 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="49595f2e37384684778f134d9eaf7282b7ea3a40" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">M. Ferrera, A. Boulch and J. Moras: <a href="http://scholar.google.de/scholar?q=Fast%20Stereo%20Disparity%20Maps%20Refinement%20By%20Fusion%20of%20Data-Based%20And%20Model-Based%20Estimations"> Fast Stereo Disparity Maps Refinement By Fusion of Data-Based And Model-Based Estimations</a>. International Conference on 3D Vision (3DV) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">229</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=9e6ba5a9d1cade3ce0394773f9825d15927a7576">Reversing-PSMNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/FilippoAleotti/Reversing" target="blank">code</a></td>
     <td class="results"> 3.13 %</td>
     <td class="results"> 8.70 %</td>
     <td class="results"> 4.06 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.41 s</td>
     <td class="results">1 core @ 1.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="9e6ba5a9d1cade3ce0394773f9825d15927a7576" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Aleotti, F. Tosi, L. Zhang, M. Poggi and S. Mattoccia: <a href="http://scholar.google.de/scholar?q=Reversing%20the%20cycle:%20self-supervised%20deep%20stereo%20through%20enhanced%20monocular%20distillation"> Reversing the cycle: self-supervised deep stereo through enhanced monocular distillation</a>. European Conference on Computer Vision (ECCV) 2020.<br></td>
   </tr>
   <tr>
    <td class="results">230</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=69355d11e1e810606056018c4abfa76c3d2d1c90">DGS</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 3.21 %</td>
     <td class="results"> 8.62 %</td>
     <td class="results"> 4.11 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.32 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="69355d11e1e810606056018c4abfa76c3d2d1c90" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">W. Chuah, R. Tennakoon, A. Bab-Hadiashar and D. Suter: <a href="http://scholar.google.de/scholar?q=Achieving%20Domain%20Robustness%20in%20Stereo%20Matching%20Networks%20by%20Removing%20Shortcut%20Learning"> Achieving Domain Robustness in Stereo 
Matching Networks by Removing Shortcut Learning</a>. arXiv preprint arXiv:2106.08486 2021.<br></td>
   </tr>
   <tr>
    <td class="results">231</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=827479c04ea1345d297e9030af99689acb64de17">PRSM</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div><div class="icon"><img src="./KITTI2015_files/icon_mv.png" alt="This method makes use of multiple (&gt;2) views."></div></td>
     <td class="results"><a href="https://github.com/vogechri/PRSM" target="blank">code</a></td>
     <td class="results"> 3.02 %</td>
     <td class="results">10.52 %</td>
     <td class="results"> 4.27 %</td>
     <td class="results">99.99 %</td>
     <td class="results">300 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="827479c04ea1345d297e9030af99689acb64de17" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">C. Vogel, K. Schindler and S. Roth: <a href="http://scholar.google.de/scholar?q=3D%20Scene%20Flow%20Estimation%20with%20aPiecewise%20Rigid%20Scene%20Model"> 3D Scene Flow Estimation with a
Piecewise Rigid Scene Model</a>. ijcv 2015.<br></td>
   </tr>
   <tr>
    <td class="results">232</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ead7a1b14c687ef985f49558bbf4cc786f6a5200">RS-IPA</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 3.13 %</td>
     <td class="results">10.05 %</td>
     <td class="results"> 4.28 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2 min</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="ead7a1b14c687ef985f49558bbf4cc786f6a5200" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">233</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b59d7f7a07254c236a4f5c87e865dbdb917f22b6">DispNetC</a></td>
     <td class="results"></td>
     <td class="results"><a href="http://lmb.informatik.uni-freiburg.de/resources/software.php" target="blank">code</a></td>
     <td class="results"> 4.32 %</td>
     <td class="results"> 4.41 %</td>
     <td class="results"> 4.34 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.06 s</td>
     <td class="results">Nvidia GTX Titan X (Caffe)</td>
     <td class="results"><input type="checkbox" value="b59d7f7a07254c236a4f5c87e865dbdb917f22b6" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">N. Mayer, E. Ilg, P. Häusser, P. Fischer, D. Cremers, A. Dosovitskiy and T. Brox: <a href="http://scholar.google.de/scholar?q=A%20Large%20Dataset%20to%20Train%20Convolutional%20Networks%20for%20Disparity,%20Optical%20Flow,%20and%20Scene%20Flow%20Estimation"> A Large Dataset to Train 
Convolutional Networks for 
Disparity, Optical Flow, and Scene Flow 
Estimation</a>. CVPR 2016.<br></td>
   </tr>
   <tr>
    <td class="results">234</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f2b8a26f094ca1c527c8bd5e25f6312041110f5c">SGM-Forest</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 3.11 %</td>
     <td class="results">10.74 %</td>
     <td class="results"> 4.38 %</td>
     <td class="results">99.92 %</td>
     <td class="results">6 seconds</td>
     <td class="results">1 core @ 3.0 Ghz (Python/C/C++)</td>
     <td class="results"><input type="checkbox" value="f2b8a26f094ca1c527c8bd5e25f6312041110f5c" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Schönberger, S. Sinha and M. Pollefeys: <a href="http://scholar.google.de/scholar?q=Learning%20to%20Fuse%20Proposals%20from%20Multiple%20Scanline%20Optimizations%20in%20Semi-Global%20Matching"> Learning to Fuse Proposals from Multiple Scanline Optimizations in Semi-Global Matching</a>. European Conference on Computer Vision (ECCV) 2018.<br></td>
   </tr>
   <tr>
    <td class="results">235</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=d56e8c2390c059f3db5ae977b021603450be9f70">SSF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 3.55 %</td>
     <td class="results"> 8.75 %</td>
     <td class="results"> 4.42 %</td>
     <td class="results">100.00 %</td>
     <td class="results">5 min</td>
     <td class="results">1 core @ 2.5 Ghz (Matlab + C/C++)</td>
     <td class="results"><input type="checkbox" value="d56e8c2390c059f3db5ae977b021603450be9f70" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Ren, D. Sun, J. Kautz and E. Sudderth: <a href="http://scholar.google.de/scholar?q=Cascaded%20Scene%20Flow%20Prediction%20using%20Semantic%20Segmentation"> Cascaded Scene Flow Prediction using 
Semantic Segmentation</a>. International Conference on 3D Vision 
(3DV) 2017.<br></td>
   </tr>
   <tr>
    <td class="results">236</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c7b689ecff5b44ee809b51a05fc19649c396f003">SMV</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 3.45 %</td>
     <td class="results"> 9.32 %</td>
     <td class="results"> 4.43 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.5 s</td>
     <td class="results">GPU @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="c7b689ecff5b44ee809b51a05fc19649c396f003" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">W. Yuan, Y. Zhang, B. Wu, S. Zhu, P. Tan, M. Wang and Q. Chen: <a href="http://scholar.google.de/scholar?q=Stereo%20Matching%20by%20Self-supervision%20of%20Multiscopic%20Vision"> Stereo Matching by Self-
supervision of Multiscopic Vision</a>. IEEE/RSJ International 
Conference on Intelligent Robots and
               Systems (IROS) 2021.<br></td>
   </tr>
   <tr>
    <td class="results">237</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=3b5055f00cbc2f40136418aca992b5abf4b0a49f">ISF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 4.12 %</td>
     <td class="results"> 6.17 %</td>
     <td class="results"> 4.46 %</td>
     <td class="results">100.00 %</td>
     <td class="results">10 min</td>
     <td class="results">1 core @ 3 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="3b5055f00cbc2f40136418aca992b5abf4b0a49f" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Behl, O. Jafari, S. Mustikovela, H. Alhaija, C. Rother and A. Geiger: <a href="http://scholar.google.de/scholar?q=Bounding%20Boxes,%20Segmentations%20and%20Object%20Coordinates:%20How%20Important%20is%20Recognition%20for%203D%20Scene%20Flow%20Estimation%20in%20Autonomous%20Driving%20Scenarios?"> Bounding Boxes, Segmentations and Object 
Coordinates: How Important is Recognition for 3D 
Scene Flow Estimation in Autonomous Driving 
Scenarios?</a>. International Conference on Computer 
Vision (ICCV) 2017.<br></td>
   </tr>
   <tr>
    <td class="results">238</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b54624a9eed52b4c8e6c76b411179dce4bd7d4d8">Content-CNN</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 3.73 %</td>
     <td class="results"> 8.58 %</td>
     <td class="results"> 4.54 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1 s</td>
     <td class="results">Nvidia GTX Titan X (Torch)</td>
     <td class="results"><input type="checkbox" value="b54624a9eed52b4c8e6c76b411179dce4bd7d4d8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">W. Luo, A. Schwing and R. Urtasun: <a href="http://scholar.google.de/scholar?q=Efficient%20Deep%20Learning%20for%20Stereo%20Matching"> Efficient Deep Learning for Stereo Matching</a>. CVPR 2016.<br></td>
   </tr>
   <tr>
    <td class="results">239</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5697f860279bab2542431396b4eb83a76b5da444">MADnet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo" target="blank">code</a></td>
     <td class="results"> 3.75 %</td>
     <td class="results"> 9.20 %</td>
     <td class="results"> 4.66 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.02 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="5697f860279bab2542431396b4eb83a76b5da444" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Tonioni, F. Tosi, M. Poggi, S. Mattoccia and L. Di Stefano: <a href="http://scholar.google.de/scholar?q=Real-Time%20self-adaptive%20deep%20stereo"> Real-Time self-adaptive deep stereo</a>. The IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">240</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=cb8f11536984856118fe8615284a1165563631bd">Self-SuperFlow-ft</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 3.81 %</td>
     <td class="results"> 8.92 %</td>
     <td class="results"> 4.66 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.13 s</td>
     <td class="results">GTX 1080 Ti</td>
     <td class="results"><input type="checkbox" value="cb8f11536984856118fe8615284a1165563631bd" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">K. Bendig, R. Schuster and D. Stricker: <a href="http://scholar.google.de/scholar?q=Self-SuperFlow:%20Self-supervised%20Scene%20Flow%20Prediction%20in%20Stereo%20Sequences"> Self-SuperFlow: Self-supervised Scene Flow Prediction in 

Stereo Sequences</a>. International Conference on Image Processing (ICIP) 2022.<br></td>
   </tr>
   <tr>
    <td class="results">241</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=39701da2ae8f05d8c9bf9c6541c41cec03cd3510">DTF_PWOC</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div><div class="icon"><img src="./KITTI2015_files/icon_mv.png" alt="This method makes use of multiple (&gt;2) views."></div></td>
     <td class="results"></td>
     <td class="results"> 3.91 %</td>
     <td class="results"> 8.57 %</td>
     <td class="results"> 4.68 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.38 s</td>
     <td class="results">RTX 2080 Ti</td>
     <td class="results"><input type="checkbox" value="39701da2ae8f05d8c9bf9c6541c41cec03cd3510" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Schuster, C. Unger and D. Stricker: <a href="http://scholar.google.de/scholar?q=A%20Deep%20Temporal%20Fusion%20Framework%20for%20Scene%20Flow%20Using%20a%20Learnable%20Motion%20Model%20and%20Occlusions"> A Deep Temporal Fusion Framework for Scene Flow Using a Learnable Motion Model and Occlusions</a>. IEEE Winter Conference on Applications of Computer Vision (WACV) 2021.<br></td>
   </tr>
   <tr>
    <td class="results">242</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=771030c09ef10657600a4fc35990053cace40163">PSMNet+ [syn2real]</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 3.17 %</td>
     <td class="results">12.47 %</td>
     <td class="results"> 4.72 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.41 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="771030c09ef10657600a4fc35990053cace40163" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">243</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=db5c414fa750b0040ed0c57d42ff79c4ae90cfe9">P3SNet+</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/aemlek/P3SNet" target="blank">code</a></td>
     <td class="results"> 4.15 %</td>
     <td class="results"> 7.59 %</td>
     <td class="results"> 4.72 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.01 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="db5c414fa750b0040ed0c57d42ff79c4ae90cfe9" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Emlek and M. Peker: <a href="http://scholar.google.de/scholar?q=P3SNet:%20Parallel%20Pyramid%20Pooling%20Stereo%20Network"> P3SNet: Parallel Pyramid Pooling Stereo 
Network</a>. IEEE Transactions on Intelligent 
Transportation Systems 2023.<br></td>
   </tr>
   <tr>
    <td class="results">244</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=297c81daec67449a4bbabce90bc495bb2058329c">GwcNet+ [syn2real]</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 3.23 %</td>
     <td class="results">12.79 %</td>
     <td class="results"> 4.82 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.41 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="297c81daec67449a4bbabce90bc495bb2058329c" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">245</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=d592c7ca07598e0248a933c32aa840bf26662848">VN</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 4.29 %</td>
     <td class="results"> 7.65 %</td>
     <td class="results"> 4.85 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.5 s</td>
     <td class="results">GPU @ 3.5 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="d592c7ca07598e0248a933c32aa840bf26662848" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">P. Knöbelreiter and T. Pock: <a href="http://scholar.google.de/scholar?q=Learned%20Collaborative%20Stereo%20Refinement"> Learned Collaborative Stereo Refinement</a>. German Conference on Pattern Recognition (GCPR) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">246</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=47e4a74b2e20f83ce1ff32cebe6069bbfd1451c9">Anonymous </a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 4.07 %</td>
     <td class="results"> 9.41 %</td>
     <td class="results"> 4.96 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.1 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="47e4a74b2e20f83ce1ff32cebe6069bbfd1451c9" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">247</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=1aa9259953ed30e98572898d6072c38870352061">MC-CNN-WS</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/tlkvstepan/mc-cnn-ws" target="blank">code</a></td>
     <td class="results"> 3.78 %</td>
     <td class="results">10.93 %</td>
     <td class="results"> 4.97 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.35 s</td>
     <td class="results">1 core 2.5 Ghz + K40 NVIDIA, Lua-Torch</td>
     <td class="results"><input type="checkbox" value="1aa9259953ed30e98572898d6072c38870352061" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">S. Tulyakov, A. Ivanov and F. Fleuret: <a href="http://scholar.google.de/scholar?q=Weakly%20supervised%20learning%20of%20deep%20metrics%20for%20stereo%20reconstruction"> Weakly supervised learning of deep 
metrics for stereo reconstruction</a>. ICCV 2017.<br></td>
   </tr>
   <tr>
    <td class="results">248</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=cfc9849d9217ab2cf1890f7ee2cacdc9fc83ef90">3DMST</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 3.36 %</td>
     <td class="results">13.03 %</td>
     <td class="results"> 4.97 %</td>
     <td class="results">100.00 %</td>
     <td class="results">93 s</td>
     <td class="results">1 core @ &gt;3.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="cfc9849d9217ab2cf1890f7ee2cacdc9fc83ef90" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">X. Lincheng Li and L. Zhang: <a href="http://scholar.google.de/scholar?q=3D%20Cost%20Aggregation%20with%20Multiple%20Minimum%20Spanning%20Trees%20for%20Stereo%20Matching"> 3D Cost Aggregation with Multiple Minimum 
Spanning Trees for Stereo Matching</a>. submitted to Applied Optics .<br></td>
   </tr>
   <tr>
    <td class="results">249</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b3327988edd87686de01baf78b151b0a94c32a76">CBMV_ROB</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/kbatsos/CBMV" target="blank">code</a></td>
     <td class="results"> 3.55 %</td>
     <td class="results">12.09 %</td>
     <td class="results"> 4.97 %</td>
     <td class="results">100.00 %</td>
     <td class="results">250 s</td>
     <td class="results">6 core @ 3.0 Ghz (Python + C/C++)</td>
     <td class="results"><input type="checkbox" value="b3327988edd87686de01baf78b151b0a94c32a76" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">K. Batsos, C. Cai and P. Mordohai: <a href="http://scholar.google.de/scholar?q=CBMV:%20A%20Coalesced%20Bidirectional%20Matching%20Volume%20for%20Disparity%20Estimation"> CBMV: A Coalesced Bidirectional Matching 
Volume for Disparity Estimation</a>. IEEE Conference on Computer Vision and 
Pattern Recognition (CVPR) 2018.<br></td>
   </tr>
   <tr>
    <td class="results">250</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c800ac23408dd404e5b192d19ae41d2e3e477cb7">OSF+TC</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div><div class="icon"><img src="./KITTI2015_files/icon_mv.png" alt="This method makes use of multiple (&gt;2) views."></div></td>
     <td class="results"></td>
     <td class="results"> 4.11 %</td>
     <td class="results"> 9.64 %</td>
     <td class="results"> 5.03 %</td>
     <td class="results">100.00 %</td>
     <td class="results">50 min</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="c800ac23408dd404e5b192d19ae41d2e3e477cb7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">M. Neoral and J. Šochman: <a href="http://scholar.google.de/scholar?q=Object%20Scene%20Flow%20with%20Temporal%20Consistency"> Object Scene Flow with Temporal 
Consistency</a>. 22nd Computer Vision Winter 
Workshop (CVWW) 2017.<br></td>
   </tr>
   <tr>
    <td class="results">251</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=a5498713b5b086dccf8e10b4e45ff4d1241ae7ab">P3SNet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/aemlek/P3SNet" target="blank">code</a></td>
     <td class="results"> 4.40 %</td>
     <td class="results"> 8.28 %</td>
     <td class="results"> 5.05 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.01 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="a5498713b5b086dccf8e10b4e45ff4d1241ae7ab" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Emlek and M. Peker: <a href="http://scholar.google.de/scholar?q=P3SNet:%20Parallel%20Pyramid%20Pooling%20Stereo%20Network"> P3SNet: Parallel Pyramid Pooling Stereo 
Network</a>. IEEE Transactions on Intelligent 
Transportation Systems 2023.<br></td>
   </tr>
   <tr>
    <td class="results">252</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=1fb7d5ec497f04c969e9a457cbb7fe9ffa5bc287">CBMV</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/kbatsos/CBMV" target="blank">code</a></td>
     <td class="results"> 4.17 %</td>
     <td class="results"> 9.53 %</td>
     <td class="results"> 5.06 %</td>
     <td class="results">100.00 %</td>
     <td class="results">250 s</td>
     <td class="results">6 cores @ 3.0 Ghz (Python,C/C++,CUDA Nvidia TitanX)</td>
     <td class="results"><input type="checkbox" value="1fb7d5ec497f04c969e9a457cbb7fe9ffa5bc287" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">K. Batsos, C. Cai and P. Mordohai: <a href="http://scholar.google.de/scholar?q=CBMV:%20A%20Coalesced%20Bidirectional%20Matching%20Volume%20for%20Disparity%20Estimation"> CBMV: A Coalesced Bidirectional Matching 
Volume for Disparity Estimation</a>. 2018.<br></td>
   </tr>
   <tr>
    <td class="results">253</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=32af59e87b36e8fe016355161ee74428abdfb8b9">PWOC-3D</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="https://github.com/dfki-av/pwoc-3d" target="blank">code</a></td>
     <td class="results"> 4.19 %</td>
     <td class="results"> 9.82 %</td>
     <td class="results"> 5.13 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.13 s</td>
     <td class="results">GTX 1080 Ti</td>
     <td class="results"><input type="checkbox" value="32af59e87b36e8fe016355161ee74428abdfb8b9" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Saxena, R. Schuster, O. Wasenmüller and D. Stricker: <a href="http://scholar.google.de/scholar?q=PWOC-3D:%20Deep%20Occlusion-Aware%20End-to-End%20Scene%20Flow%20Estimation"> PWOC-3D: Deep Occlusion-Aware End-to-End Scene Flow Estimation</a>. Intelligent Vehicles Symposium (IV) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">254</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b1fa648ac9313e66952c750581e69868f25b0345">StereoVAE</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 4.25 %</td>
     <td class="results">10.18 %</td>
     <td class="results"> 5.23 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.03 s</td>
     <td class="results">Jetson AGX Xavier GPU</td>
     <td class="results"><input type="checkbox" value="b1fa648ac9313e66952c750581e69868f25b0345" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Q. Chang, X. Li, X. Xu, X. Liu, Y. Li and J. Miyazaki: <a href="http://scholar.google.de/scholar?q=StereoVAE:%20A%20lightweight%20stereo%20matching%20system%20using%20embedded%20GPUs"> StereoVAE: A lightweight stereo matching 
system using embedded GPUs</a>. International Conference on Robotics 
and Automation 2023.<br></td>
   </tr>
   <tr>
    <td class="results">255</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=d42d35c84a146059cd8642181ac5a4ad3f4f0064">OSF 2018</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="http://www.cvlibs.net/projects/objectsceneflow" target="blank">code</a></td>
     <td class="results"> 4.11 %</td>
     <td class="results">11.12 %</td>
     <td class="results"> 5.28 %</td>
     <td class="results">100.00 %</td>
     <td class="results">390 s</td>
     <td class="results">1 core @ 2.5 Ghz (Matlab + C/C++)</td>
     <td class="results"><input type="checkbox" value="d42d35c84a146059cd8642181ac5a4ad3f4f0064" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">M. Menze, C. Heipke and A. Geiger: <a href="http://scholar.google.de/scholar?q=Object%20Scene%20Flow"> Object Scene Flow</a>. ISPRS Journal of Photogrammetry and Remote Sensing (JPRS) 2018.<br></td>
   </tr>
   <tr>
    <td class="results">256</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=8b1d8bc017befc0a470089234e5fc45bc46151b7">SPS-St</a></td>
     <td class="results"></td>
     <td class="results"><a href="http://ttic.uchicago.edu/~dmcallester/SPS/index.html" target="blank">code</a></td>
     <td class="results"> 3.84 %</td>
     <td class="results">12.67 %</td>
     <td class="results"> 5.31 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2 s</td>
     <td class="results">1 core @ 3.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="8b1d8bc017befc0a470089234e5fc45bc46151b7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">K. Yamaguchi, D. McAllester and R. Urtasun: <a href="http://scholar.google.de/scholar?q=Efficient%20Joint%20Segmentation,%20Occlusion%20Labeling,%20Stereo%20and%20Flow%20Estimation"> Efficient Joint Segmentation, Occlusion Labeling, 
Stereo and Flow Estimation</a>. ECCV 2014.<br></td>
   </tr>
   <tr>
    <td class="results">257</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=1f1aba77bef87f04f190ba5e30417a4d9a533a88">MDP</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_st.png" alt="This method uses stereo information."></div></td>
     <td class="results"></td>
     <td class="results"> 4.19 %</td>
     <td class="results">11.25 %</td>
     <td class="results"> 5.36 %</td>
     <td class="results">100.00 %</td>
     <td class="results">11.4 s</td>
     <td class="results">4 cores @ 3.5 Ghz (Matlab + C/C++)</td>
     <td class="results"><input type="checkbox" value="1f1aba77bef87f04f190ba5e30417a4d9a533a88" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Li, D. Chen, Y. Liu and Z. Yuan: <a href="http://scholar.google.de/scholar?q=Coordinating%20Multiple%20Disparity%20Proposals%20for%20Stereo%20Computation"> Coordinating Multiple Disparity Proposals for Stereo Computation</a>. IEEE Conference on Computer Vision and Pattern Recognition 2016.<br></td>
   </tr>
   <tr>
    <td class="results">258</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=9f26f2229812040b9b3760dca43155d44bc09d67">SFF++</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div><div class="icon"><img src="./KITTI2015_files/icon_mv.png" alt="This method makes use of multiple (&gt;2) views."></div></td>
     <td class="results"></td>
     <td class="results"> 4.27 %</td>
     <td class="results">12.38 %</td>
     <td class="results"> 5.62 %</td>
     <td class="results">100.00 %</td>
     <td class="results">78 s</td>
     <td class="results">4 cores @ 3.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="9f26f2229812040b9b3760dca43155d44bc09d67" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Schuster, O. Wasenmüller, C. Unger, G. Kuschk and D. Stricker: <a href="http://scholar.google.de/scholar?q=SceneFlowFields++:%20Multi-frame%20Matching,%20Visibility%20Prediction,%20and%20Robust%20Interpolation%20for%20Scene%20Flow%20Estimation"> SceneFlowFields++: Multi-frame Matching, Visibility Prediction, and Robust Interpolation for Scene Flow Estimation</a>. International Journal of Computer Vision (IJCV) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">259</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=92d4a3f6c54c182e7fc3668be327e0c850439f44">OSF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="http://www.cvlibs.net/projects/objectsceneflow" target="blank">code</a></td>
     <td class="results"> 4.54 %</td>
     <td class="results">12.03 %</td>
     <td class="results"> 5.79 %</td>
     <td class="results">100.00 %</td>
     <td class="results">50 min</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="92d4a3f6c54c182e7fc3668be327e0c850439f44" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">M. Menze and A. Geiger: <a href="http://scholar.google.de/scholar?q=Object%20Scene%20Flow%20for%20Autonomous%20Vehicles"> Object Scene Flow for Autonomous Vehicles</a>. Conference on Computer Vision and Pattern Recognition (CVPR) 2015.<br></td>
   </tr>
   <tr>
    <td class="results">260</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f74a69235c2a9a58651af8bdc7dd743ce2fe27cc">pSGM</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 4.84 %</td>
     <td class="results">11.64 %</td>
     <td class="results"> 5.97 %</td>
     <td class="results">100.00 %</td>
     <td class="results">7.77 s</td>
     <td class="results">4 cores @ 3.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="f74a69235c2a9a58651af8bdc7dd743ce2fe27cc" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Y. Lee, M. Park, Y. Hwang, Y. Shin and C. Kyung: <a href="http://scholar.google.de/scholar?q=Memory-Efficient%20Parametric%20Semiglobal%20Matching"> Memory-Efficient Parametric Semiglobal 
Matching</a>. IEEE Signal Processing Letters 2018.<br></td>
   </tr>
   <tr>
    <td class="results">261</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=624a8f825d98a069fc29ccd3ae42b386ca523b92">CSF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 4.57 %</td>
     <td class="results">13.04 %</td>
     <td class="results"> 5.98 %</td>
     <td class="results">99.99 %</td>
     <td class="results">80 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="624a8f825d98a069fc29ccd3ae42b386ca523b92" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Z. Lv, C. Beall, P. Alcantarilla, F. Li, Z. Kira and F. Dellaert: <a href="http://scholar.google.de/scholar?q=A%20Continuous%20Optimization%20Approach%20for%20Efficient%20and%20Accurate%20Scene%20Flow"> A Continuous Optimization Approach for 
Efficient and Accurate Scene Flow</a>. European Conf. on Computer Vision 
(ECCV) 2016.<br></td>
   </tr>
   <tr>
    <td class="results">262</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=815d286b729587483fbf495531b61d6734368a67">MBM</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 4.69 %</td>
     <td class="results">13.05 %</td>
     <td class="results"> 6.08 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.13 s</td>
     <td class="results">1 core @ 3.0 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="815d286b729587483fbf495531b61d6734368a67" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">N. Einecke and J. Eggert: <a href="http://scholar.google.de/scholar?q=A%20Multi-Block-Matching%20Approach%20for%20Stereo"> A Multi-Block-Matching Approach for Stereo</a>. IV 2015.<br></td>
   </tr>
   <tr>
    <td class="results">263</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=9f19fbff8d23e901f7b8135b3bc0867bce2fb4fc">CRD-Fusion</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/fanxiule/CRD_Fusion" target="blank">code</a></td>
     <td class="results"> 4.59 %</td>
     <td class="results">13.68 %</td>
     <td class="results"> 6.11 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.02 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="9f19fbff8d23e901f7b8135b3bc0867bce2fb4fc" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">X. Fan, S. Jeon and B. Fidan: <a href="http://scholar.google.de/scholar?q=Occlusion-Aware%20Self-Supervised%20Stereo%20Matching%20with%20Confidence%20Guided%20Raw%20Disparity%20Fusion"> Occlusion-Aware Self-Supervised Stereo 
Matching with Confidence Guided Raw Disparity 
Fusion</a>. Conference on Robots and Vision 2022.<br></td>
   </tr>
   <tr>
    <td class="results">264</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=84da07a0a8b3a9a09dc4907cee512e615a446f29">LDC-S</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 5.78 %</td>
     <td class="results"> 7.92 %</td>
     <td class="results"> 6.14 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.04 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="84da07a0a8b3a9a09dc4907cee512e615a446f29" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">265</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=37ee2403f719dabe5ef49c795c209e91f9beae15">AAFS+</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 5.02 %</td>
     <td class="results">11.75 %</td>
     <td class="results"> 6.14 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.01 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="37ee2403f719dabe5ef49c795c209e91f9beae15" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">266</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=976abf77346237796b4de87397a7fc662d91267d">PR-Sceneflow</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="https://github.com/vogechri/PRSM" target="blank">code</a></td>
     <td class="results"> 4.74 %</td>
     <td class="results">13.74 %</td>
     <td class="results"> 6.24 %</td>
     <td class="results">100.00 %</td>
     <td class="results">150 s</td>
     <td class="results">4 core @ 3.0 Ghz (Matlab + C/C++)</td>
     <td class="results"><input type="checkbox" value="976abf77346237796b4de87397a7fc662d91267d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">C. Vogel, K. Schindler and S. Roth: <a href="http://scholar.google.de/scholar?q=Piecewise%20Rigid%20Scene%20Flow"> Piecewise Rigid Scene Flow</a>. ICCV 2013.<br></td>
   </tr>
   <tr>
    <td class="results">267</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=288eb75d114d7797013d6378edc95b79a730157a">LDCNetLG</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 5.65 %</td>
     <td class="results"> 9.46 %</td>
     <td class="results"> 6.28 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.04 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="288eb75d114d7797013d6378edc95b79a730157a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">ERROR: Wrong syntax in BIBTEX file.<br></td>
   </tr>
   <tr>
    <td class="results">268</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=87bbf791dbf17325e6bad5b10458889f1847a89e">DispSegNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 4.20 %</td>
     <td class="results">16.97 %</td>
     <td class="results"> 6.33 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.9 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="87bbf791dbf17325e6bad5b10458889f1847a89e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Zhang, K. Skinner, R. Vasudevan and M. Johnson-Roberson: <a href="http://scholar.google.de/scholar?q=DispSegNet:%20Leveraging%20Semantics%20for%20End-to-End%20Learning%20of%20Disparity%20Estimation%20From%20Stereo%20Imagery"> DispSegNet: Leveraging Semantics for End-
to-End Learning of Disparity Estimation From 
Stereo Imagery</a>. IEEE Robotics and Automation Letters 2019.<br></td>
   </tr>
   <tr>
    <td class="results">269</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=06806ac0b70cf3c10d9470f02994c7dd5a908a3c">DeepCostAggr</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/andrey-kuzmin/deep-cost-aggr" target="blank">code</a></td>
     <td class="results"> 5.34 %</td>
     <td class="results">11.35 %</td>
     <td class="results"> 6.34 %</td>
     <td class="results">99.98 %</td>
     <td class="results">0.03 s</td>
     <td class="results">GPU @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="06806ac0b70cf3c10d9470f02994c7dd5a908a3c" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Kuzmin, D. Mikushin and V. Lempitsky: <a href="http://scholar.google.de/scholar?q=End-to-end%20Learning%20of%20Cost-Volume%20Aggregation%20for%20Real-time%20Dense%20Stereo"> End-to-end Learning of Cost-Volume Aggregation 
for 
Real-time Dense Stereo</a>. 2017 IEEE 27th International Workshop on 
Machine Learning for Signal Processing (MLSP) 2017.<br></td>
   </tr>
   <tr>
    <td class="results">270</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=474922eb673df9273b4f6db7e87ba12b21f604c0">SGM_RVC</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 5.06 %</td>
     <td class="results">13.00 %</td>
     <td class="results"> 6.38 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.11 s</td>
     <td class="results">Nvidia GTX 980</td>
     <td class="results"><input type="checkbox" value="474922eb673df9273b4f6db7e87ba12b21f604c0" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Hirschm\"uller: <a href="http://scholar.google.de/scholar?q=Stereo%20Processing%20by%20Semi-Global%20Matching%20and%20Mutual%20Information"> Stereo Processing by Semi-Global 
Matching and Mutual Information</a>. IEEE Transactions on Pattern 
Analysis and Machine Intelligence 2008.<br></td>
   </tr>
   <tr>
    <td class="results">271</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f18e35501e4ef28c1666167f980896fc0d8df320">LDC-L</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 5.65 %</td>
     <td class="results">10.19 %</td>
     <td class="results"> 6.41 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.04 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="f18e35501e4ef28c1666167f980896fc0d8df320" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">272</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=fd827fe457c9ff3bf497b93e908752c36d476e55">UHP</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 5.00 %</td>
     <td class="results">13.70 %</td>
     <td class="results"> 6.45 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.02 s</td>
     <td class="results">GPU @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="fd827fe457c9ff3bf497b93e908752c36d476e55" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">273</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=259897c8dfe6a9479dd51b16e53e80c4ad6d3b8a">SceneFFields</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 5.12 %</td>
     <td class="results">13.83 %</td>
     <td class="results"> 6.57 %</td>
     <td class="results">100.00 %</td>
     <td class="results">65 s</td>
     <td class="results">4 cores @ 3.7 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="259897c8dfe6a9479dd51b16e53e80c4ad6d3b8a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Schuster, O. Wasenmüller, G. Kuschk, C. Bailer and D. Stricker: <a href="http://scholar.google.de/scholar?q=SceneFlowFields:%20Dense%20Interpolation%20of%20Sparse%20Scene%20Flow%20Correspondences"> SceneFlowFields: Dense Interpolation of Sparse Scene Flow Correspondences</a>. IEEE Winter Conference on Applications of Computer Vision (WACV) 2018.<br></td>
   </tr>
   <tr>
    <td class="results">274</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=60d67e86d73bdc8db6e0e092ab9329a072dfe0a3">SPS+FF++</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="http://av.dfki.de/" target="blank">code</a></td>
     <td class="results"> 5.47 %</td>
     <td class="results">12.19 %</td>
     <td class="results"> 6.59 %</td>
     <td class="results">100.00 %</td>
     <td class="results">36 s</td>
     <td class="results">1 core @ 3.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="60d67e86d73bdc8db6e0e092ab9329a072dfe0a3" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Schuster, O. Wasenmüller and D. Stricker: <a href="http://scholar.google.de/scholar?q=Dense%20Scene%20Flow%20from%20Stereo%20Disparity%20and%20Optical%20Flow"> Dense Scene Flow from Stereo Disparity and Optical Flow</a>. ACM Computer Science in Cars Symposium (CSCS) 2018.<br></td>
   </tr>
   <tr>
    <td class="results">275</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=51ce75cff0507e3f09c75476f6a9538381100790">Flow2Stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 5.01 %</td>
     <td class="results">14.62 %</td>
     <td class="results"> 6.61 %</td>
     <td class="results">99.97 %</td>
     <td class="results">0.05 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="51ce75cff0507e3f09c75476f6a9538381100790" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">P. Liu, I. King, M. Lyu and J. Xu: <a href="http://scholar.google.de/scholar?q=Flow2Stereo:%20Effective%20Self-Supervised%20Learning%20of%20Optical%20Flow%20and%20Stereo%20Matching"> Flow2Stereo: Effective Self-Supervised 
Learning of Optical Flow and Stereo Matching</a>. CVPR 2020.<br></td>
   </tr>
   <tr>
    <td class="results">276</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=9f23d86c92b872c4012e3bae452090b8c5aa4168">FSF+MS</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div><div class="icon"><img src="./KITTI2015_files/icon_ms.png" alt="This method makes use of the epipolar geometry."></div><div class="icon"><img src="./KITTI2015_files/icon_mv.png" alt="This method makes use of multiple (&gt;2) views."></div></td>
     <td class="results"></td>
     <td class="results"> 5.72 %</td>
     <td class="results">11.84 %</td>
     <td class="results"> 6.74 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2.7 s</td>
     <td class="results">4 cores @ 3.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="9f23d86c92b872c4012e3bae452090b8c5aa4168" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">T. Taniai, S. Sinha and Y. Sato: <a href="http://scholar.google.de/scholar?q=Fast%20Multi-frame%20Stereo%20Scene%20Flow%20with%20Motion%20Segmentation"> Fast Multi-frame Stereo Scene Flow 
with Motion Segmentation</a>. IEEE Conference on Computer Vision 
and Pattern Recognition (CVPR 2017) 2017.<br></td>
   </tr>
   <tr>
    <td class="results">277</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=609241292f4ad31c035b7cf4c9ced8bc8325a4d8">AABM</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 4.88 %</td>
     <td class="results">16.07 %</td>
     <td class="results"> 6.74 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.08 s</td>
     <td class="results">1 core @ 3.0 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="609241292f4ad31c035b7cf4c9ced8bc8325a4d8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">N. Einecke and J. Eggert: <a href="http://scholar.google.de/scholar?q=Stereo%20Image%20Warping%20for%20Improved%20Depth%20Estimation%20of%20Road%20Surfaces"> Stereo Image Warping for Improved Depth Estimation of Road Surfaces</a>. IV 2013.<br></td>
   </tr>
   <tr>
    <td class="results">278</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=9962fc0fdec9bbad537a376b01019a063baba25d">test_ours</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 5.62 %</td>
     <td class="results">12.81 %</td>
     <td class="results"> 6.82 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.1 s</td>
     <td class="results">1 core @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="9962fc0fdec9bbad537a376b01019a063baba25d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">ERROR: Wrong syntax in BIBTEX file.<br></td>
   </tr>
   <tr>
    <td class="results">279</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=7cb0857378f600cbb4010b51f2596c1c77a8f6f1">SGM+C+NL</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="http://people.seas.harvard.edu/~dqsun/" target="blank">code</a></td>
     <td class="results"> 5.15 %</td>
     <td class="results">15.29 %</td>
     <td class="results"> 6.84 %</td>
     <td class="results">100.00 %</td>
     <td class="results">4.5 min</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="7cb0857378f600cbb4010b51f2596c1c77a8f6f1" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Hirschmüller: <a href="http://scholar.google.de/scholar?q=Stereo%20Processing%20by%20Semiglobal%20Matching%20and%20Mutual%20Information"> Stereo Processing by Semiglobal Matching and Mutual Information</a>. PAMI 2008.<br>D. Sun, S. Roth and M. Black: <a href="http://scholar.google.de/scholar?q=A%20Quantitative%20Analysis%20of%20Current%20Practices%20in%20Optical%20Flow%20Estimation%20and%20the%20Principles%20Behind%20Them"> A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them</a>. IJCV 2013.<br></td>
   </tr>
   <tr>
    <td class="results">280</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c3cea1243278b2d1ef116d7c5fca16d14a0484a7">SGM+LDOF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="http://lmb.informatik.uni-freiburg.de/resources/binaries/" target="blank">code</a></td>
     <td class="results"> 5.15 %</td>
     <td class="results">15.29 %</td>
     <td class="results"> 6.84 %</td>
     <td class="results">100.00 %</td>
     <td class="results">86 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="c3cea1243278b2d1ef116d7c5fca16d14a0484a7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Hirschmüller: <a href="http://scholar.google.de/scholar?q=Stereo%20Processing%20by%20Semiglobal%20Matching%20and%20Mutual%20Information"> Stereo Processing by Semiglobal Matching and Mutual Information</a>. PAMI 2008.<br>T. Brox and J. Malik: <a href="http://scholar.google.de/scholar?q=Large%20Displacement%20Optical%20Flow:%20Descriptor%20Matching%20in%20Variational%20Motion%20Estimation"> Large Displacement Optical Flow: Descriptor Matching in Variational Motion Estimation</a>. PAMI 2011.<br></td>
   </tr>
   <tr>
    <td class="results">281</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=e53532d300932808a428b30e829a30966c96a6e3">SGM+SF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 5.15 %</td>
     <td class="results">15.29 %</td>
     <td class="results"> 6.84 %</td>
     <td class="results">100.00 %</td>
     <td class="results">45 min</td>
     <td class="results">16 core @ 3.2 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="e53532d300932808a428b30e829a30966c96a6e3" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Hirschmüller: <a href="http://scholar.google.de/scholar?q=Stereo%20Processing%20by%20Semiglobal%20Matching%20and%20Mutual%20Information"> Stereo Processing by Semiglobal Matching 
and Mutual Information</a>. PAMI 2008.<br>M. Hornacek, A. Fitzgibbon and C. Rother: <a href="http://scholar.google.de/scholar?q=SphereFlow:%206%20DoF%20Scene%20Flow%20from%20RGB-D%20Pairs"> SphereFlow: 6 
DoF Scene Flow from RGB-D Pairs</a>. CVPR 2014.<br></td>
   </tr>
   <tr>
    <td class="results">282</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=2aa10777bae89ca0ee3164220d43650c419d73d2">SNCC</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 5.36 %</td>
     <td class="results">16.05 %</td>
     <td class="results"> 7.14 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.08 s</td>
     <td class="results">1 core @ 3.0 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="2aa10777bae89ca0ee3164220d43650c419d73d2" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">N. Einecke and J. Eggert: <a href="http://scholar.google.de/scholar?q=A%20Two-Stage%20Correlation%20Method%20for%20Stereoscopic%20Depth%20Estimation"> A Two-Stage Correlation Method for Stereoscopic Depth Estimation</a>. DICTA 2010.<br></td>
   </tr>
   <tr>
    <td class="results">283</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=78dc427d034c849ebb9794ebb3fa8d5b204b8238">Permutation Stereo</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 5.53 %</td>
     <td class="results">15.47 %</td>
     <td class="results"> 7.18 %</td>
     <td class="results">99.93 %</td>
     <td class="results">30 s</td>
     <td class="results">GPU @ 2.5 Ghz (Matlab)</td>
     <td class="results"><input type="checkbox" value="78dc427d034c849ebb9794ebb3fa8d5b204b8238" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">P. Brousseau and S. Roy: <a href="http://scholar.google.de/scholar?q=A%20Permutation%20Model%20for%20the%20Self-Supervised%20Stereo%20Matching%20Problem"> A Permutation Model for the Self-
Supervised Stereo Matching Problem</a>. 2022 19th Conference on Robots and 
Vision (CRV) 2022.<br></td>
   </tr>
   <tr>
    <td class="results">284</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=278dcc43fb04eb4cc7a921c0a87202c6c1e85bfa">PASMnet</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/LongguangWang/PAM" target="blank">code</a></td>
     <td class="results"> 5.41 %</td>
     <td class="results">16.36 %</td>
     <td class="results"> 7.23 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.5 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="278dcc43fb04eb4cc7a921c0a87202c6c1e85bfa" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">L. Wang, Y. Guo, Y. Wang, Z. Liang, Z. Lin, J. Yang and W. An: <a href="http://scholar.google.de/scholar?q=Parallax%20Attention%20for%20Unsupervised%20Stereo%20Correspondence%20Learning"> Parallax Attention for 
Unsupervised 
Stereo Correspondence Learning</a>. IEEE Transactions on Pattern 
Analysis and Machine Intelligence(T-PAMI) 2020.<br></td>
   </tr>
   <tr>
    <td class="results">285</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5495296d364a94c7b72ed4096604d45d39332397">AAFS</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 6.27 %</td>
     <td class="results">13.95 %</td>
     <td class="results"> 7.54 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.01 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="5495296d364a94c7b72ed4096604d45d39332397" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Chang, P. Chang and Y. Chen: <a href="http://scholar.google.de/scholar?q=Attention-Aware%20Feature%20Aggregation%20for%20Real-time%20Stereo%20Matching%20on%20Edge%20Devices"> Attention-Aware Feature Aggregation for 
Real-time Stereo Matching on Edge Devices</a>. Proceedings of the Asian Conference 
on Computer Vision 2020.<br></td>
   </tr>
   <tr>
    <td class="results">286</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c56e510884edb37c0facbb1764894c1c3c9ed900">Z2ZNCC</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 6.55 %</td>
     <td class="results">13.19 %</td>
     <td class="results"> 7.65 %</td>
     <td class="results">99.93 %</td>
     <td class="results">0.035s</td>
     <td class="results">Jetson TX2 GPU @ 1.0 Ghz (CUDA)</td>
     <td class="results"><input type="checkbox" value="c56e510884edb37c0facbb1764894c1c3c9ed900" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Q. Chang, A. Zha, W. Wang, X. Liu, M. Onishi, L. Lei, M. Er and T. Maruyama: <a href="http://scholar.google.de/scholar?q=Efficient%20stereo%20matching%20on%20embedded%20GPUs%20with%20zero-means%20cross%20correlation"> Efficient stereo matching on embedded 
GPUs with zero-means cross correlation</a>. Journal of Systems Architecture 2022.<br></td>
   </tr>
   <tr>
    <td class="results">287</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=0ec35d1edceab578b0bf55a9fe7be7f0a0a97ff7">ReS2tAC</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_st.png" alt="This method uses stereo information."></div></td>
     <td class="results"></td>
     <td class="results"> 6.27 %</td>
     <td class="results">16.07 %</td>
     <td class="results"> 7.90 %</td>
     <td class="results">86.03 %</td>
     <td class="results">0.06 s</td>
     <td class="results">Jetson AGX GPU @ 1.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="0ec35d1edceab578b0bf55a9fe7be7f0a0a97ff7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">B. Ruf, J. Mohrs, M. Weinmann, S. Hinz and J. Beyerer: <a href="http://scholar.google.de/scholar?q=ReS2tAC%20-%20UAV-Borne%20Real-Time%20SGM%20Stereo%20Optimized%20for%20Embedded%20ARM%20and%20CUDA%20Devices"> ReS2tAC - UAV-Borne Real-Time 
SGM Stereo Optimized for Embedded ARM and 
CUDA Devices</a>. Sensors 2021.<br></td>
   </tr>
   <tr>
    <td class="results">288</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=a7fdffbcfd6a3f85e2ac5b4180241c2a99978fbf">Self-SuperFlow</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 5.78 %</td>
     <td class="results">19.76 %</td>
     <td class="results"> 8.11 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.13 s</td>
     <td class="results">GTX 1080 Ti</td>
     <td class="results"><input type="checkbox" value="a7fdffbcfd6a3f85e2ac5b4180241c2a99978fbf" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">K. Bendig, R. Schuster and D. Stricker: <a href="http://scholar.google.de/scholar?q=Self-SuperFlow:%20Self-supervised%20Scene%20Flow%20Prediction%20in%20Stereo%20Sequences"> Self-SuperFlow: Self-supervised Scene Flow Prediction in 
Stereo Sequences</a>. International Conference on Image Processing (ICIP) 2022.<br></td>
   </tr>
   <tr>
    <td class="results">289</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=80b0b89fe893b8408cbaa0a28e7300bc72b12fb8">CSCT+SGM+MF</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 6.91 %</td>
     <td class="results">14.87 %</td>
     <td class="results"> 8.24 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.0064  s</td>
     <td class="results">Nvidia GTX Titan X @ 1.0 Ghz (CUDA)</td>
     <td class="results"><input type="checkbox" value="80b0b89fe893b8408cbaa0a28e7300bc72b12fb8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">D. Hernandez-Juarez, A. Chacon, A. Espinosa, D. Vazquez, J. Moure and A. Lopez: <a href="http://scholar.google.de/scholar?q=Embedded%20real-time%20stereo%20estimation%20via%20Semi-Global%20Matching%20on%20the%20GPU"> Embedded real-time stereo estimation via Semi-Global Matching on the GPU</a>. Procedia Computer Science 2016.<br></td>
   </tr>
   <tr>
    <td class="results">290</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5c50c19ca9e1f117b2a203ed6e56665e6946bab4">3DG-DVO</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 7.62 %</td>
     <td class="results">11.44 %</td>
     <td class="results"> 8.26 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.04 s</td>
     <td class="results">GPU @ 1.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="5c50c19ca9e1f117b2a203ed6e56665e6946bab4" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">291</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=485eb6125f88830fa16d0ae4c118f0faa8348c31">MBMGPU</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 6.61 %</td>
     <td class="results">16.70 %</td>
     <td class="results"> 8.29 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.0019 s</td>
     <td class="results">GPU @ 1.0 Ghz (CUDA)</td>
     <td class="results"><input type="checkbox" value="485eb6125f88830fa16d0ae4c118f0faa8348c31" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Q. Chang and T. Maruyama: <a href="http://scholar.google.de/scholar?q=Real-Time%20Stereo%20Vision%20System:%20A%20Multi-Block%20Matching%20on%20GPU"> Real-Time Stereo Vision System: 
A Multi-Block Matching on GPU</a>. IEEE Access 2018.<br></td>
   </tr>
   <tr>
    <td class="results">292</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=58b9207738885c37f3d048fdb73f3ee4c575e4c8">MeshStereo</a></td>
     <td class="results"></td>
     <td class="results"><a href="http://chizhang.me/" target="blank">code</a></td>
     <td class="results"> 5.82 %</td>
     <td class="results">21.21 %</td>
     <td class="results"> 8.38 %</td>
     <td class="results">100.00 %</td>
     <td class="results">87 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="58b9207738885c37f3d048fdb73f3ee4c575e4c8" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">C. Zhang, Z. Li, Y. Cheng, R. Cai, H. Chao and Y. Rui: <a href="http://scholar.google.de/scholar?q=MeshStereo:%20A%20Global%20Stereo%20Model%20With%20Mesh%20Alignment%20Regularization%20for%20View%20Interpolation"> MeshStereo: A Global Stereo Model With 
Mesh Alignment Regularization for View 
Interpolation</a>. The IEEE International Conference on 
Computer Vision (ICCV) 2015.<br></td>
   </tr>
   <tr>
    <td class="results">293</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=214f3cdbcc1b9593e6fb50ea9eeeeccc30e66536">PCOF + ACTF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 6.31 %</td>
     <td class="results">19.24 %</td>
     <td class="results"> 8.46 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.08 s</td>
     <td class="results">GPU @ 2.0 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="214f3cdbcc1b9593e6fb50ea9eeeeccc30e66536" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">M. Derome, A. Plyer, M. Sanfourche and G. Le Besnerais: <a href="http://scholar.google.de/scholar?q=A%20Prediction-Correction%20Approach%20for%20Real-Time%20Optical%20Flow%20Computation%20Using%20Stereo"> A Prediction-Correction Approach for Real-Time Optical Flow Computation Using Stereo</a>. German Conference on Pattern Recognition 2016.<br></td>
   </tr>
   <tr>
    <td class="results">294</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=a773f10a25d0ba372b4d247507ccaca37239f0e5">PCOF-LDOF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 6.31 %</td>
     <td class="results">19.24 %</td>
     <td class="results"> 8.46 %</td>
     <td class="results">100.00 %</td>
     <td class="results">50 s</td>
     <td class="results">1 core @ 3.0 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="a773f10a25d0ba372b4d247507ccaca37239f0e5" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">M. Derome, A. Plyer, M. Sanfourche and G. Le Besnerais: <a href="http://scholar.google.de/scholar?q=A%20Prediction-Correction%20Approach%20for%20Real-Time%20Optical%20Flow%20Computation%20Using%20Stereo"> A Prediction-Correction Approach for Real-Time Optical Flow Computation Using Stereo</a>. German Conference on Pattern Recognition 2016.<br></td>
   </tr>
   <tr>
    <td class="results">295</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=46767c6ebc19ed52ed7052c07bd70d17438f515b">OASM-Net</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 6.89 %</td>
     <td class="results">19.42 %</td>
     <td class="results"> 8.98 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.73 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="46767c6ebc19ed52ed7052c07bd70d17438f515b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Li and Z. Yuan: <a href="http://scholar.google.de/scholar?q=Occlusion%20Aware%20Stereo%20Matching%20via%20Cooperative%20Unsupervised%20Learning"> Occlusion Aware Stereo Matching via Cooperative Unsupervised Learning</a>. Proceedings of the Asian Conference on Computer Vision, ACCV 2018.<br></td>
   </tr>
   <tr>
    <td class="results">296</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=fd6fa4c64a181d07d010429a2ac14fdf8c4fc1e1">ELAS_RVC</a></td>
     <td class="results"></td>
     <td class="results"><a href="http://www.cvlibs.net/software/libelas" target="blank">code</a></td>
     <td class="results"> 7.38 %</td>
     <td class="results">21.15 %</td>
     <td class="results"> 9.67 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.19 s</td>
     <td class="results">4 cores @ &gt;3.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="fd6fa4c64a181d07d010429a2ac14fdf8c4fc1e1" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Geiger, M. Roser and R. Urtasun: <a href="http://scholar.google.de/scholar?q=Efficient%20Large-Scale%20Stereo%20Matching"> Efficient Large-Scale Stereo Matching</a>. ACCV 2010.<br></td>
   </tr>
   <tr>
    <td class="results">297</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=94048cbe5693045adcea3d7b3560654ba3079e5d">ELAS</a></td>
     <td class="results"></td>
     <td class="results"><a href="http://www.cvlibs.net/software/libelas" target="blank">code</a></td>
     <td class="results"> 7.86 %</td>
     <td class="results">19.04 %</td>
     <td class="results"> 9.72 %</td>
     <td class="results">92.35 %</td>
     <td class="results">0.3 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="94048cbe5693045adcea3d7b3560654ba3079e5d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Geiger, M. Roser and R. Urtasun: <a href="http://scholar.google.de/scholar?q=Efficient%20Large-Scale%20Stereo%20Matching"> Efficient Large-Scale Stereo Matching</a>. ACCV 2010.<br></td>
   </tr>
   <tr>
    <td class="results">298</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=de13fd001062cdfdd6cd65c018b37dc9886fcea7">REAF</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://www.researchgate.net/publication/281200485_REAF_aggregation_SourceCode" target="blank">code</a></td>
     <td class="results"> 8.43 %</td>
     <td class="results">18.51 %</td>
     <td class="results">10.11 %</td>
     <td class="results">100.00 %</td>
     <td class="results">1.1 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="de13fd001062cdfdd6cd65c018b37dc9886fcea7" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">C. Cigla: <a href="http://scholar.google.de/scholar?q=Recursive%20Edge-Aware%20Filters%20for%20Stereo%20Matching"> Recursive Edge-Aware Filters for Stereo Matching</a>. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops 2015.<br></td>
   </tr>
   <tr>
    <td class="results">299</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=c7d20658ab9ca3ed0e81e054fa6d2cfd0512a242">self-raft3d</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results"> 8.15 %</td>
     <td class="results">20.86 %</td>
     <td class="results">10.27 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.1 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="c7d20658ab9ca3ed0e81e054fa6d2cfd0512a242" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11"></td>
   </tr>
   <tr>
    <td class="results">300</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=d12c78ffb1c0bee81fa87f67d78867782a37bf49">iGF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_mv.png" alt="This method makes use of multiple (&gt;2) views."></div></td>
     <td class="results"></td>
     <td class="results"> 8.64 %</td>
     <td class="results">21.85 %</td>
     <td class="results">10.84 %</td>
     <td class="results">100.00 %</td>
     <td class="results">220 s</td>
     <td class="results">1 core @ 3.0 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="d12c78ffb1c0bee81fa87f67d78867782a37bf49" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Hamzah, H. Ibrahim and A. Hassan: <a href="http://scholar.google.de/scholar?q=Stereo%20matching%20algorithm%20based%20on%20per%20pixel%20difference%20adjustment,%20iterative%20guided%20filter%20and%20graph%20segmentation"> Stereo matching algorithm based on per pixel difference adjustment, iterative guided filter and graph segmentation</a>. Journal of Visual Communication and Image Representation 2016.<br></td>
   </tr>
   <tr>
    <td class="results">301</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=67877bd6fcc43163421fa0108c7df83bbc69fea3">OCV-SGBM</a></td>
     <td class="results"></td>
     <td class="results"><a href="http://opencv.org/" target="blank">code</a></td>
     <td class="results"> 8.92 %</td>
     <td class="results">20.59 %</td>
     <td class="results">10.86 %</td>
     <td class="results">90.41 %</td>
     <td class="results">1.1 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="67877bd6fcc43163421fa0108c7df83bbc69fea3" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">H. Hirschmueller: <a href="http://scholar.google.de/scholar?q=Stereo%20processing%20by%20semiglobal%20matchingand%20mutual%20information"> Stereo processing by semiglobal matching
and mutual information</a>. PAMI 2008.<br></td>
   </tr>
   <tr>
    <td class="results">302</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=3b040bc75ee71c05abc5fd2b3c1563b3c7bddc64">TW-SMNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results">11.92 %</td>
     <td class="results">12.16 %</td>
     <td class="results">11.96 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.7 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="3b040bc75ee71c05abc5fd2b3c1563b3c7bddc64" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">M. El-Khamy, H. Ren, X. Du and J. Lee: <a href="http://scholar.google.de/scholar?q=TW-SMNet:%20Deep%20Multitask%20Learning%20of%20Tele-Wide%20Stereo%20Matching"> TW-SMNet: Deep Multitask Learning of Tele-Wide Stereo Matching</a>. arXiv:1906.04463 2019.<br></td>
   </tr>
   <tr>
    <td class="results">303</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=4e003d65b0a9b36e9e58184b1300be97045dcfde">SDM</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results"> 9.41 %</td>
     <td class="results">24.75 %</td>
     <td class="results">11.96 %</td>
     <td class="results">62.56 %</td>
     <td class="results">1 min</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="4e003d65b0a9b36e9e58184b1300be97045dcfde" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Kostkova: <a href="http://scholar.google.de/scholar?q=Stratified%20dense%20matching%20for%20stereopsisin%20complex%20scenes"> Stratified dense matching for stereopsis
in complex scenes</a>. BMVC 2003.<br></td>
   </tr>
   <tr>
    <td class="results">304</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=09564aaff23d6adf6aaf6d6a502494899372e250">SGM&amp;FlowFie+</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results">11.93 %</td>
     <td class="results">20.57 %</td>
     <td class="results">13.37 %</td>
     <td class="results">81.24 %</td>
     <td class="results">29 s</td>
     <td class="results">1 core @ 3.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="09564aaff23d6adf6aaf6d6a502494899372e250" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Schuster, C. Bailer, O. Wasenmüller and D. Stricker: <a href="http://scholar.google.de/scholar?q=Combining%20Stereo%20Disparity%20and%20Optical%20Flow%20for%20Basic%20Scene%20Flow"> Combining Stereo Disparity and Optical Flow for Basic Scene Flow</a>. Commercial Vehicle Technology Symposium (CVTS) 2018.<br></td>
   </tr>
   <tr>
    <td class="results">305</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=167c3e957b19770252a773b1c778cb3167833848">GCSF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="http://cmp.felk.cvut.cz/~cechj/GCSF/" target="blank">code</a></td>
     <td class="results">11.64 %</td>
     <td class="results">27.11 %</td>
     <td class="results">14.21 %</td>
     <td class="results">100.00 %</td>
     <td class="results">2.4 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="167c3e957b19770252a773b1c778cb3167833848" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Cech, J. Sanchez-Riera and R. Horaud: <a href="http://scholar.google.de/scholar?q=Scene%20Flow%20Estimation%20by%20growing%20Correspondence%20Seeds"> Scene Flow Estimation by growing Correspondence Seeds</a>. CVPR 2011.<br></td>
   </tr>
   <tr>
    <td class="results">306</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=ffe90e3e6ae5e99ebed7e77e809f74c3d5fe295e">MT-TW-SMNet</a></td>
     <td class="results"></td>
     <td class="results"></td>
     <td class="results">15.47 %</td>
     <td class="results">16.25 %</td>
     <td class="results">15.60 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.4s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="ffe90e3e6ae5e99ebed7e77e809f74c3d5fe295e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">M. El-Khamy, X. Du, H. Ren and J. Lee: <a href="http://scholar.google.de/scholar?q=Multi-Task%20Learning%20of%20Depth%20from%20Tele%20and%20Wide%20Stereo%20Image%20Pairs"> Multi-Task Learning of Depth from Tele and Wide Stereo Image Pairs</a>. Proceedings of the IEEE Conference on Image Processing 2019.<br></td>
   </tr>
   <tr>
    <td class="results">307</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f12372e50ae7eb8cbadb1589e8ed2260c183d6fb">Mono-SF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results">14.21 %</td>
     <td class="results">26.94 %</td>
     <td class="results">16.32 %</td>
     <td class="results">100.00 %</td>
     <td class="results">41 s</td>
     <td class="results">1 core @ 3.5 Ghz (Matlab + C/C++)</td>
     <td class="results"><input type="checkbox" value="f12372e50ae7eb8cbadb1589e8ed2260c183d6fb" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Brickwedde, S. Abraham and R. Mester: <a href="http://scholar.google.de/scholar?q=Mono-SF:%20Multi-View%20Geometry%20meets%20Single-View%20Depth%20for%20Monocular%20Scene%20Flow%20Estimation%20of%20Dynamic%20Traffic%20Scenes"> Mono-SF: Multi-View Geometry meets Single-View Depth for Monocular Scene Flow Estimation of Dynamic Traffic Scenes</a>. Proc. of International Conference on Computer Vision (ICCV) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">308</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=f633453878ad1c95d6ad5ff0a3849a2ab4495fd0">CostFilter</a></td>
     <td class="results"></td>
     <td class="results"><a href="http://www.ims.tuwien.ac.at/research/costFilter/" target="blank">code</a></td>
     <td class="results">17.53 %</td>
     <td class="results">22.88 %</td>
     <td class="results">18.42 %</td>
     <td class="results">100.00 %</td>
     <td class="results">4 min</td>
     <td class="results">1 core @ 2.5 Ghz (Matlab)</td>
     <td class="results"><input type="checkbox" value="f633453878ad1c95d6ad5ff0a3849a2ab4495fd0" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">C. Rhemann, A. Hosni, M. Bleyer, C. Rother and M. Gelautz: <a href="http://scholar.google.de/scholar?q=Fast%20Cost-Volume%20Filtering%20for%20VisualCorrespondence%20and%20Beyond"> Fast Cost-Volume Filtering for Visual
Correspondence and Beyond</a>. CVPR 2011.<br></td>
   </tr>
   <tr>
    <td class="results">309</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=e8ab8f0f07645240230fb29fd8421b7b0e03fe4b">MonoComb</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results">17.89 %</td>
     <td class="results">21.16 %</td>
     <td class="results">18.44 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.58 s</td>
     <td class="results">RTX 2080 Ti</td>
     <td class="results"><input type="checkbox" value="e8ab8f0f07645240230fb29fd8421b7b0e03fe4b" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Schuster, C. Unger and D. Stricker: <a href="http://scholar.google.de/scholar?q=MonoComb:%20A%20Sparse-to-Dense%20Combination%20Approach%20for%20Monocular%20Scene%20Flow"> MonoComb: A Sparse-to-Dense Combination Approach for Monocular Scene Flow</a>. ACM Computer Science in Cars Symposium (CSCS) 2020.<br></td>
   </tr>
   <tr>
    <td class="results">310</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=61f3fa293b97c3010525fc4c7df95698f4c66a46">DWBSF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results">19.61 %</td>
     <td class="results">22.69 %</td>
     <td class="results">20.12 %</td>
     <td class="results">100.00 %</td>
     <td class="results">7 min</td>
     <td class="results">4 cores @ 3.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="61f3fa293b97c3010525fc4c7df95698f4c66a46" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">C. Richardt, H. Kim, L. Valgaerts and C. Theobalt: <a href="http://scholar.google.de/scholar?q=Dense%20Wide-Baseline%20Scene%20Flow%20From%20Two%20Handheld%20Video%20Cameras"> Dense Wide-Baseline Scene Flow 
From Two Handheld Video Cameras</a>. 3DV 2016.<br></td>
   </tr>
   <tr>
    <td class="results">311</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=7f2c8421d4430acf5483e861b73777d644804180">RAFT-MSF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results">18.10 %</td>
     <td class="results">36.82 %</td>
     <td class="results">21.21 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.18 s</td>
     <td class="results">GPU @ 2.5 Ghz (Python)</td>
     <td class="results"><input type="checkbox" value="7f2c8421d4430acf5483e861b73777d644804180" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">ERROR: Wrong syntax in BIBTEX file.<br></td>
   </tr>
   <tr>
    <td class="results">312</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=4c82533b073c52b148f285e9b34185f9394e6983">monoResMatch</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/fabiotosi92/monoResMatch-Tensorflow" target="blank">code</a></td>
     <td class="results">22.10 %</td>
     <td class="results">19.81 %</td>
     <td class="results">21.72 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.16 s</td>
     <td class="results">Titan X GPU</td>
     <td class="results"><input type="checkbox" value="4c82533b073c52b148f285e9b34185f9394e6983" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Tosi, F. Aleotti, M. Poggi and S. Mattoccia: <a href="http://scholar.google.de/scholar?q=Learning%20monocular%20depth%20estimation%20infusing%20traditional%20stereo%20knowledge"> Learning monocular depth estimation 
infusing traditional stereo knowledge</a>. The IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR) 2019.<br></td>
   </tr>
   <tr>
    <td class="results">313</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=7e544e1d0319efe7b114bf6fb35eca5b4d5e2745">Self-Mono-SF-ft</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="https://github.com/visinf/self-mono-sf" target="blank">code</a></td>
     <td class="results">20.72 %</td>
     <td class="results">29.41 %</td>
     <td class="results">22.16 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.09 s</td>
     <td class="results">NVIDIA GTX 1080 Ti</td>
     <td class="results"><input type="checkbox" value="7e544e1d0319efe7b114bf6fb35eca5b4d5e2745" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Hur and S. Roth: <a href="http://scholar.google.de/scholar?q=Self-Supervised%20Monocular%20Scene%20Flow%20Estimation"> Self-Supervised Monocular Scene 
Flow Estimation</a>. CVPR 2020.<br></td>
   </tr>
   <tr>
    <td class="results">314</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=3db367f85ebee8cf541399b86feaeec979d94364">Multi-Mono-SF-ft</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div><div class="icon"><img src="./KITTI2015_files/icon_mv.png" alt="This method makes use of multiple (&gt;2) views."></div></td>
     <td class="results"><a href="https://github.com/visinf/multi-mono-sf" target="blank">code</a></td>
     <td class="results">21.60 %</td>
     <td class="results">28.22 %</td>
     <td class="results">22.71 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.06 s</td>
     <td class="results">NVIDIA GTX 1080 Ti</td>
     <td class="results"><input type="checkbox" value="3db367f85ebee8cf541399b86feaeec979d94364" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Hur and S. Roth: <a href="http://scholar.google.de/scholar?q=Self-Supervised%20Multi-Frame%20Monocular%20Scene%20Flow"> Self-Supervised Multi-Frame 
Monocular Scene Flow</a>. CVPR 2021.<br></td>
   </tr>
   <tr>
    <td class="results">315</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=3efe8a529600756876eb7629682a391c6d15433a">OCV-BM</a></td>
     <td class="results"></td>
     <td class="results"><a href="http://opencv.org/" target="blank">code</a></td>
     <td class="results">24.29 %</td>
     <td class="results">30.13 %</td>
     <td class="results">25.27 %</td>
     <td class="results">58.54 %</td>
     <td class="results">0.1 s</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="3efe8a529600756876eb7629682a391c6d15433a" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">G. Bradski: <a href="http://scholar.google.de/scholar?q=The%20OpenCV%20Library"> The OpenCV Library</a>. Dr. Dobb's Journal of Software Tools 2000.<br></td>
   </tr>
   <tr>
    <td class="results">316</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=5921c882e2966e1aa4976911fe6c7c0a41e7e11e">VSF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="http://devernay.free.fr/vision/varsceneflow/" target="blank">code</a></td>
     <td class="results">27.31 %</td>
     <td class="results">21.72 %</td>
     <td class="results">26.38 %</td>
     <td class="results">100.00 %</td>
     <td class="results">125 min</td>
     <td class="results">1 core @ 2.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="5921c882e2966e1aa4976911fe6c7c0a41e7e11e" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">F. Huguet and F. Devernay: <a href="http://scholar.google.de/scholar?q=A%20Variational%20Method%20for%20Scene%20Flow%20Estimation%20from%20Stereo%20Sequences"> A Variational Method for Scene Flow Estimation from Stereo Sequences</a>. ICCV 2007.<br></td>
   </tr>
   <tr>
    <td class="results">317</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=4923b8fc555364876a7aa275638246a8890b6d87">SED</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://bitbucket.org/dexmont/edge_mapping" target="blank">code</a></td>
     <td class="results">25.01 %</td>
     <td class="results">40.43 %</td>
     <td class="results">27.58 %</td>
     <td class="results"> 4.02 %</td>
     <td class="results">0.68 s</td>
     <td class="results">1 core @ 2.0 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="4923b8fc555364876a7aa275638246a8890b6d87" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">D. Pe\~{n}a and A. Sutherland: <a href="http://scholar.google.de/scholar?q=Disparity%20Estimation%20by%20Simultaneous%20Edge%20Drawing"> Disparity Estimation by Simultaneous Edge Drawing</a>. Computer Vision -- ACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 2017.<br></td>
   </tr>
   <tr>
    <td class="results">318</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=563674fd35f666292acce5092c836cf596e0610d">Multi-Mono-SF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div><div class="icon"><img src="./KITTI2015_files/icon_mv.png" alt="This method makes use of multiple (&gt;2) views."></div></td>
     <td class="results"><a href="https://github.com/visinf/multi-mono-sf" target="blank">code</a></td>
     <td class="results">27.48 %</td>
     <td class="results">47.30 %</td>
     <td class="results">30.78 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.06 s</td>
     <td class="results">NVIDIA GTX 1080 Ti</td>
     <td class="results"><input type="checkbox" value="563674fd35f666292acce5092c836cf596e0610d" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Hur and S. Roth: <a href="http://scholar.google.de/scholar?q=Self-Supervised%20Multi-Frame%20Monocular%20Scene%20Flow"> Self-Supervised Multi-Frame 
Monocular Scene Flow</a>. CVPR 2021.<br></td>
   </tr>
   <tr>
    <td class="results">319</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=83d880672f04f45c0a9259c0b84e9d8ce37bb4e5">mts1</a></td>
     <td class="results"></td>
     <td class="results"><a href="https://github.com/rbrandt1/MaxTreeS" target="blank">code</a></td>
     <td class="results">28.03 %</td>
     <td class="results">46.55 %</td>
     <td class="results">31.11 %</td>
     <td class="results"> 2.52 %</td>
     <td class="results">0.18 s</td>
     <td class="results">4 cores @ 3.5 Ghz (C/C++)</td>
     <td class="results"><input type="checkbox" value="83d880672f04f45c0a9259c0b84e9d8ce37bb4e5" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">R. Brandt, N. Strisciuglio, N. Petkov and M. Wilkinson: <a href="http://scholar.google.de/scholar?q=Efficient%20binocular%20stereo%20correspondence%20matching%20with%201-D%20Max-Trees"> Efficient binocular stereo 
correspondence matching with 1-D Max-Trees</a>. Pattern Recognition Letters 2020.<br></td>
   </tr>
   <tr>
    <td class="results">320</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=a66132278ccc340d6d0e925b1c7651f2fcfeb6de">Self-Mono-SF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"><a href="https://github.com/visinf/self-mono-sf" target="blank">code</a></td>
     <td class="results">31.22 %</td>
     <td class="results">48.04 %</td>
     <td class="results">34.02 %</td>
     <td class="results">100.00 %</td>
     <td class="results">0.09 s</td>
     <td class="results">NVIDIA GTX 1080 Ti</td>
     <td class="results"><input type="checkbox" value="a66132278ccc340d6d0e925b1c7651f2fcfeb6de" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">J. Hur and S. Roth: <a href="http://scholar.google.de/scholar?q=Self-Supervised%20Monocular%20Scene%20Flow%20Estimation"> Self-Supervised Monocular Scene 
Flow Estimation</a>. CVPR 2020.<br></td>
   </tr>
   <tr>
    <td class="results">321</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=9424ea627b4787fd7cca9e45cd60d85f85dd4f28">MST</a></td>
     <td class="results"></td>
     <td class="results"><a href="http://www.cs.cityu.edu.hk/~qiyang/publications/cvpr-12/" target="blank">code</a></td>
     <td class="results">45.83 %</td>
     <td class="results">38.22 %</td>
     <td class="results">44.57 %</td>
     <td class="results">100.00 %</td>
     <td class="results">7 s</td>
     <td class="results">1 core @ 2.5 Ghz (Matlab + C/C++)</td>
     <td class="results"><input type="checkbox" value="9424ea627b4787fd7cca9e45cd60d85f85dd4f28" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">Q. Yang: <a href="http://scholar.google.de/scholar?q=A%20Non-Local%20Cost%20Aggregation%20Method%20for%20Stereo%20Matching"> A Non-Local Cost Aggregation Method 
for Stereo Matching</a>. CVPR 2012.<br></td>
   </tr>
   <tr>
    <td class="results">322</td>
     <td class="results"><a href="https://www.cvlibs.net/datasets/kitti/eval_scene_flow_detail.php?benchmark=stereo&amp;result=b5f622c9c3d028e8d0db79ece9e27f9d6d5d0848">Stereo-RSSF</a></td>
     <td class="results"><div class="icon"><img src="./KITTI2015_files/icon_fl.png" alt="This method uses optical flow information."></div></td>
     <td class="results"></td>
     <td class="results">56.60 %</td>
     <td class="results">73.05 %</td>
     <td class="results">59.34 %</td>
     <td class="results"> 9.26 %</td>
     <td class="results">2.5 s</td>
     <td class="results">8 core @ 2.5 Ghz (Matlab)</td>
     <td class="results"><input type="checkbox" value="b5f622c9c3d028e8d0db79ece9e27f9d6d5d0848" name="selected[]"></td>
   </tr>
   <tr>
    <td class="results_sub" colspan="11">A. Erfan salehi and R. hoseuni: <a href="http://scholar.google.de/scholar?q=Stereo-RSSF:%20Stereo%20Robust%20Sparse%20Scene-Flow%20Estimation"> Stereo-RSSF: Stereo Robust Sparse Scene-Flow Estimation</a>. 2023.<br></td>
   </tr>
 </tbody></table></form>

<center><a target="_blank" href="https://www.cvlibs.net/datasets/kitti/table_scene_flow.php?benchmark=stereo&amp;eval_gt=all&amp;eval_area=all&amp;mode=1"> Table as LaTeX</a> | <a target="_blank" href="https://www.cvlibs.net/datasets/kitti/table_scene_flow.php?benchmark=stereo&amp;eval_gt=all&amp;eval_area=all&amp;mode=2"> Only published Methods</a></center><br><br><br><br>

</div></div>
<div class="section">
<h2 class="title">Related Datasets</h2>
<div class="entry"><p>

</p><ul>
<li><a href="http://hci.iwr.uni-heidelberg.de/Static/challenge2012/" target="_blank">HCI/Bosch Robust Vision Challenge</a>: Optical flow and stereo vision challenge on high resolution imagery recorded at a high frame rate under diverse weather conditions (e.g., sunny, cloudy, rainy). The Robert Bosch AG provides a prize for the best performing method.</li>
<li><a href="http://www.mi.auckland.ac.nz/index.php?option=com_content&amp;view=article&amp;id=44&amp;Itemid=67" target="_blank">Image Sequence Analysis Test Site (EISATS)</a>: Synthetic image sequences with ground truth information provided by UoA and Daimler AG. Some of the images come with 3D range sensor information.</li>
<li><a href="http://vision.middlebury.edu/stereo/" target="_blank">Middlebury Stereo Evaluation</a>: The classic stereo evaluation benchmark, featuring four test images in version 2 of the benchmark, with very accurate ground truth from a structured light system. 38 image pairs are provided in total.</li>
<li><a href="http://www.6d-vision.com/ground-truth-stixel-dataset" target="_blank">Daimler Stereo Dataset</a>: Stereo bad weather highway scenes with partial ground truth for freespace</li>
<li><a href="http://make3d.cs.cornell.edu/data.html" target="_blank">Make3D Range Image Data</a>: Images with small-resolution ground truth used to learn and evaluate depth from single monocular images.</li>
<li><a href="http://www.robots.ox.ac.uk/~lubor/" target="_blank">Lubor Ladicky's Stereo Dataset</a>: Stereo Images with manually labeled ground truth based on polygonal areas.</li>
<li><a href="http://vision.middlebury.edu/flow/" target="_blank">Middlebury Optical Flow Evaluation</a>: The classic optical flow evaluation benchmark, featuring eight test images, with very accurate ground truth from a shape from UV light pattern system. 24 image pairs are provided in total.</li>
</ul>
<p></p></div></div>

<div class="section">
<h2 class="title">Citation</h2>
<div class="entry"><p>
When using this dataset in your research, we will be happy if you cite us:<br>
@ARTICLE{<a href="https://www.cvlibs.net/publications/Menze2018JPRS.pdf" target="_blank">Menze2018JPRS</a>,<br>
&nbsp; author = {<a href="http://www.ipi.uni-hannover.de/tmm.html" target="blank">Moritz Menze</a> and <a href="http://www.ipi.uni-hannover.de/chei.html" target="blank">Christian Heipke</a> and <a href="https://www.cvlibs.net/" target="blank">Andreas Geiger</a>},<br>
&nbsp; title = {Object Scene Flow},<br>&nbsp; journal = {ISPRS Journal of Photogrammetry and Remote Sensing (JPRS)},<br>
&nbsp; year = {2018}<br>
}
<br>
@INPROCEEDINGS{<a href="https://www.cvlibs.net/publications/Menze2015ISA.pdf" target="_blank">Menze2015ISA</a>,<br>
&nbsp; author = {<a href="http://www.ipi.uni-hannover.de/tmm.html" target="blank">Moritz Menze</a> and <a href="http://www.ipi.uni-hannover.de/chei.html" target="blank">Christian Heipke</a> and <a href="https://www.cvlibs.net/" target="blank">Andreas Geiger</a>},<br>
&nbsp; title = {Joint 3D Estimation of Vehicles and Scene Flow},<br>&nbsp; booktitle = {ISPRS Workshop on Image Sequence Analysis (ISA)},<br>
&nbsp; year = {2015}<br>
}
</p></div></div>

  <div class="clearfix"></div>
  <div class="tracker">
  <div id="eXTReMe"><br><br><a href="http://extremetracking.com/open?login=votec">
  <img src="./KITTI2015_files/i.gif" style="border: 0;" height="38" width="41" id="EXim" alt="eXTReMe Tracker"></a>
  <script type="text/javascript"><!--
  var EXlogin='votec' // Login
  var EXvsrv='s10' // VServer
  EXs=screen;EXw=EXs.width;navigator.appName!="Netscape"?
  EXb=EXs.colorDepth:EXb=EXs.pixelDepth;
  navigator.javaEnabled()==1?EXjv="y":EXjv="n";
  EXd=document;EXw?"":EXw="na";EXb?"":EXb="na";
  EXd.write("<img src=http://e1.extreme-dm.com",
  "/"+EXvsrv+".g?login="+EXlogin+"&amp;",
  "jv="+EXjv+"&amp;j=y&amp;srw="+EXw+"&amp;srb="+EXb+"&amp;",
  "l="+escape(EXd.referrer)+" height=1 width=1>");//-->
  </script><img src="./KITTI2015_files/s10.g" height="1" width="1"><noscript><div id="neXTReMe"><img height="1" width="1" alt=""
  src="http://e1.extreme-dm.com/s10.g?login=votec&amp;j=n&amp;jv=n" />
  </div></noscript></div>
  </div>

</div>
<div class="clearfix">&nbsp;</div>
</div>
</div>

<div id="footer" class="container">
	<p>© 2023 | <a href="https://www.cvlibs.net/aboutme.html" target="_blank">Andreas Geiger</a> | <a href="https://www.cvlibs.net/" target="_blank">cvlibs.net</a> | <a href="http://www.freecsstemplates.org/" target="_blank">csstemplates</a></p>
</div>





</body></html>